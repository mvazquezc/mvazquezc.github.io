<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Beginner’s Guide to RAG: What I Wish Someone Told Me | Linuxera</title><meta name=keywords content="artificialintelligence,ai,llm,llms,rag"><meta name=description content="A Beginner’s Guide to RAG: What I Wish Someone Told Me In this post, I&rsquo;ll try to provide a beginners guide to RAG, focusing on what I wish someone told me before trying to build a RAG solution.
Attention
While I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct. If you notice anything missing or inaccurate, please leave a comment!"><meta name=author content="Mario"><link rel=canonical href=https://linuxera.org/rag-beginners-guide/><link crossorigin=anonymous href=/assets/css/stylesheet.8cc7ef3cdd44c5188f9267864f378d8dd8892d583e0fd07b5e5321e496f1e4d1.css integrity="sha256-jMfvPN1ExRiPkmeGTzeNjdiJLVg+D9B7XlMh5Jbx5NE=" rel="preload stylesheet" as=style><link rel=icon href=https://linuxera.org/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://linuxera.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://linuxera.org/favicon-32x32.png><link rel=apple-touch-icon href=https://linuxera.org/apple-touch-icon.png><link rel=mask-icon href=https://linuxera.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script defer data-domain=linuxera.org src=https://stats.linuxera.org/js/script.file-downloads.hash.outbound-links.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script><meta property="og:title" content="A Beginner’s Guide to RAG: What I Wish Someone Told Me"><meta property="og:description" content="A Beginner’s Guide to RAG: What I Wish Someone Told Me In this post, I&rsquo;ll try to provide a beginners guide to RAG, focusing on what I wish someone told me before trying to build a RAG solution.
Attention
While I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct. If you notice anything missing or inaccurate, please leave a comment!"><meta property="og:type" content="article"><meta property="og:url" content="https://linuxera.org/rag-beginners-guide/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-27T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-27T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Beginner’s Guide to RAG: What I Wish Someone Told Me"><meta name=twitter:description content="A Beginner’s Guide to RAG: What I Wish Someone Told Me In this post, I&rsquo;ll try to provide a beginners guide to RAG, focusing on what I wish someone told me before trying to build a RAG solution.
Attention
While I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct. If you notice anything missing or inaccurate, please leave a comment!"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://linuxera.org/posts/"},{"@type":"ListItem","position":2,"name":"A Beginner’s Guide to RAG: What I Wish Someone Told Me","item":"https://linuxera.org/rag-beginners-guide/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Beginner’s Guide to RAG: What I Wish Someone Told Me","name":"A Beginner’s Guide to RAG: What I Wish Someone Told Me","description":"A Beginner’s Guide to RAG: What I Wish Someone Told Me In this post, I\u0026rsquo;ll try to provide a beginners guide to RAG, focusing on what I wish someone told me before trying to build a RAG solution.\nAttention\nWhile I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct. If you notice anything missing or inaccurate, please leave a comment!","keywords":["artificialintelligence","ai","llm","llms","rag"],"articleBody":"A Beginner’s Guide to RAG: What I Wish Someone Told Me In this post, I’ll try to provide a beginners guide to RAG, focusing on what I wish someone told me before trying to build a RAG solution.\nAttention\nWhile I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct. If you notice anything missing or inaccurate, please leave a comment!\nWhat is Retrieval-Augmented Generation (RAG) RAG (Retrieval-Augmented Generation) is an AI technique that combines a language model with a search system to retrieve relevant documents and use them as context for generating more accurate, informed, and up-to-date responses. It enhances output by grounding it in external knowledge.\nTo put it simple, we provide the LLM (Large Language Model) with a set of documents it has to use when answering. These documents, are processed and stored in a vector database.\nWhy RAG? There are several use cases that benefit from the RAG pattern. Based on my findings, the most common involve leveraging existing documents to answer user questions.\nFor example:\nLet’s say your company has lots of internal documents — onboarding manuals, policy guides, troubleshooting playbooks — scattered across different platforms. You want to let employees ask questions like:\n“How do I request parental leave?” “How do I set up a VPN on a company-issued laptop?” “What’s the on-call escalation policy for SREs?” Public LLMs don’t have access to any of that, they don’t know your company’s internal knowledge. But a RAG system can retrieve the relevant documents at inference time and inject them into the model’s context. This grounds the answers in your organization’s actual data.\nRAG is especially useful for internal search, support bots, and documentation assistants - particularly when your knowledge base is large and constantly changing.\nEmbeddings To make documents usable for a Retrieval-Augmented Generation (RAG) system, they must be converted into vector embeddings (numerical representations that capture semantic meaning). These embeddings are generated using a specialized embedding model, and then stored in a vector database, where they can later be searched and retrieved based on similarity to a user’s query.\nThe typical flow looks like this:\nDocument -\u003e Embedding Model (tokenizes internally) -\u003e Vector Database\nContext Window We described Context Window in more detail in a previous blog post, but here’s a quick refresher.\nThe Context Window defines the maximum number of tokens (words, subwords, or characters depending on the tokenizer) the model can process in a single input sequence (including both the prompt and any output generated during a single pass).\nFor example, if a model has a context window of 2048 tokens, and the prompt uses 1000 tokens, the model has 1048 tokens left for generating a response.\nWhy is the Context Window important for RAG? In Retrieval-Augmented Generation (RAG), once relevant documents are retrieved from the vector database, they are injected into the context window along with the user’s query. This is how the model gains access to external knowledge at inference time.\nIf the combined length of the query and retrieved content exceeds the context window, the model won’t be able to see the full input. This can result in:\nTruncated documents Missing critical context Inaccurate or hallucinated responses In summary, the context window acts as the model’s working memory. Keeping the inputs within its limits is essential for accurate, high-quality answers in RAG systems.\nKey Considerations for RAG Document Quality over Quantity “Garbage in, garbage out” applies especially to RAG systems. Even the most advanced LLM can’t compensate for low-quality source material. If you wouldn’t hand a document to a new hire to explain a topic, don’t hand it to your model either.\nTo improve the quality of your documents, consider the following strategies:\nCollaborate with content owners to update and improve documentation. Use LLMs to summarize, clean up, or extract key points — there are specialized models for creating FAQs, concise summaries, etc. Also, prepare documents with retrievability in mind. Remove elements that are hard to parse or irrelevant for text-based models:\nDiagrams, images, screenshots Long blocks of low-readability content (e.g., raw tracebacks or logs) In summary, don’t try to go from zero to hero. Start with high-quality documents you trust, and gradually expand your ingestion pipeline as you validate the results.\nUnstructed vs Structured In RAG, it’s critical to understand the type of data you’re working with:\nUnstructured data: Free-form text like articles, web pages, manuals, wikis, and documentation. No strict schema. Structured data: Information with a predefined schema, such as database rows, spreadsheets, or API responses. If you’re working with structured data, classic RAG may not be the best fit. Instead, consider using Agentic RAG, where the LLM is given access to tools (like APIs or SQL queries) that retrieve structured data on-demand and inject it into the context window.\nAgentic RAG also works well when you need to blend structured and unstructured sources in a single system.\nRAG Searches Retrieval-Augmented Generation (RAG) systems rely on effective document retrieval techniques to surface relevant context for the LLM. There are three main search strategies you should be familiar with: semantic search, keyword search, and hybrid approaches.\nSemantic Search Semantic search focuses on understanding the meaning behind a query rather than matching specific words. It uses vector embeddings to represent both the query and documents in a high-dimensional space, then measures their similarity.\nThis is especially useful when the user’s query uses different phrasing than the source material — for example, searching “How to get a refund?” could match documents about “return policies.”\nCommon similarity metrics include:\nCosine Similarity: Measures the angle between two vectors — good for checking direction/semantic similarity. Dot Product: Takes into account both similarity and vector magnitude — useful when embedding magnitude carries importance. (Be cautious: unnormalized dot product can favor longer text regardless of relevance.) Euclidean Distance: Measures literal distance — often used in clustering or when you want to group semantically close documents. Semantic Search Algorithms Cosine Similarity Compare two texts to see if they talk about the same thing, even if one is longer.\nDoc A: “The capital of France is Paris.” Doc B: “Paris is the capital city of France, known for the Eiffel Tower.” Despite different lengths and wordings, their cosine similarity will be high because they are semantically aligned.\nDot Product Prioritize documents that not only match the query direction but also carry stronger signals (e.g., longer content, weighted terms).\nDoc A: “200 words about EU regulations on emissions.” Doc B: “2000 words with comprehensive policy analysis, including emissions and carbon tax.” Both match query direction, but doc B has a higher magnitude. Possibly due to length, or emphasis.\nEuclidean Distance Grouping documents with similar embeddings into clusters, regardless of angle. Closer means more similar overall vector positions.\nYou embed 1000 support tickets, and you want to cluster them into 5 issue types. Euclidean will help you do K-Means Clustering to find natural groupings. Keyword / Full-text Search Keyword-based search is more literal. It tries to find exact words or phrases within documents — like traditional search engines or grep. It’s ideal when the user is looking for a specific term, phrase, or entity.\nA commonly used algorithm here is:\nBM25 (Best Match 25): A probabilistic model that ranks documents based on the occurrence and frequency of query terms, adjusted by document length. While it doesn’t capture semantic meaning, keyword search is fast, simple, and precise — especially for technical content with consistent terminology.\nHybrid Search Hybrid search combines both semantic and keyword search to maximize recall and precision. A typical approach might:\nUse keyword search (e.g: BM25) to quickly narrow down a large document set. Use semantic search to rank or rerank the candidates based on relevance and meaning. This approach gives you the best of both worlds: the precision of keywords with the flexibility of semantics.\nThe order of operations in hybrid search isn’t fixed — it depends on your use case:\nKeyword First → Semantic Rerank: Good when you have a lot of documents and the question uses specific terms. It’s fast and works well. Semantic First → Keyword Filter or Boost: Good when people might phrase things differently. Helps find relevant results even if the wording doesn’t match exactly. Improving RAG Searches A RAG system is only as good as the documents it retrieves. Even if your documents are high-quality, poor retrieval can lead to weak answers. These strategies help improve document retrieval and relevance.\nReranking When your system retrieves documents — whether using semantic, keyword, or hybrid search — you typically get a list of candidates with scores. Reranking helps reorder these results to better match your specific use case.\nHere are common reranking strategies:\nLLM-based Reranking: Use a language model to evaluate which results best answer the user’s question. Cross-Encoder Reranking: Use a model that takes both the query and document together to assign a new relevance score. Custom Rerankers: Create your own logic to boost or demote certain results based on metadata (e.g., date, author, document type, etc.). Refine user’s questions Sometimes the original question doesn’t provide enough context, especially in ambiguous cases. For example:\nUser query: Where can I buy a mouse? LLM: Do they mean a computer peripheral or a pet? To avoid confusion, you can refine the query using an LLM by injecting context. If you know your documents cover internal hardware policies, your refinement prompt might look like:\n“You’re a helpful assistant specialized in helping users buy computer hardware following company policy.” Make sure to include the user’s original question in the final prompt. This helps keep the user’s intent while clarifying the meaning for better retrieval.\nUse the right algorithm for semantic search Different similarity algorithms can give different retrieval behaviors. Choose based on what matters most in your case.\nYou can also combine them, for example:\nUse Cosine Similarity to get meaning-aligned docs. Use Dot Product to emphasize importance or weight. Improving RAG Answers Once your RAG system retrieves the right documents, the next step is generating accurate and trustworthy answers. Here are some key practices to improve answer quality.\nAlways Include Source Links Make sure the system outputs links or references to the documents used to generate an answer. This improves transparency and helps users verify the information.\nLimit the Number of Documents Used Allow your system to restrict the number of retrieved documents passed into the context window — for example, the top 3 most relevant ones.\nFewer, more relevant documents often lead to better answers than many loosely related ones.\nManage the Context Window Wisely The context window is finite — and balance is key. When injecting documents into the prompt:\nIf the documents are small and fit entirely into the context window, include the full document:\nBut don’t overload the context —\u003e this can confuse the model (overstuffing). And don’t underload —\u003e the model may hallucinate or miss key details (undersupplying). Example:\nWant a summary of Chapter 1 of a book? ➜ Send only Chapter 1. Want to know how many times a character appears? ➜ Send the whole book (if it fits). Embeddings Strategies In a RAG system, how you generate embeddings has a big impact on both retrieval performance and answer quality. The two most common approaches are full-length and chunk-level embeddings — and each has trade-offs.\nFull-Length Embeddings Full-length embeddings are vector representations of entire documents or large text blocks. Their goal is to capture the overall semantic meaning of the content.\nEfficient retrieval: Fewer vectors to store and search (e.g., 1 vector per document). Lower granularity: May miss fine details buried deep in the text. This approach is useful when you’re trying to quickly identify which document is most relevant — like finding the right manual or report.\nChunk-Level Embeddings Chunk-level embeddings break documents into smaller parts — sentences, paragraphs, or sections — and generate vectors for each chunk. These embeddings focus on local details.\nHigher precision: Great for surfacing exact sections relevant to a query. More expensive: You’ll need to store and search many more vectors. While more vectors improve granularity, they increase index size, memory use, and retrieval latency — especially for large bodies of text. This approach shines when specific answers are scattered throughout large documents — like answering “What ports does this device support?” from a 100-page manual.\nCombine the Two A smart strategy is to combine both approaches:\nStart with full-length embeddings to quickly identify relevant documents. This narrows down your search space (e.g., 1,000 vectors instead of 10,000). Then, search within those documents using chunk-level embeddings to locate the most relevant sections. This hybrid method may require using different embedding models optimized for different levels of granularity — but the payoff is better retrieval and more accurate answers.\nRAG vs Fine-Tuning RAG injects relevant external documents into the model’s context at inference time to improve responses without changing the model itself. It’s flexible and keeps up with changing information.\nFine-tuning modifies the model’s internal weights by training it on task-specific data. This can improve performance but updates require retraining and are less responsive to real-time changes compared to RAG.\nIn summary, RAG adds knowledge dynamically at inference time; fine-tuning bakes knowledge into the model permanently.\nRAFT (RAG + Fine-Tuning) RAFT (Retrieval-Augmented Generation + Fine-Tuning) brings together the strengths of both approaches to maximize performance and flexibility.\nFine-tune your LLM on high-quality, stable knowledge (e.g., internal documentation, product specs, historical data). Use RAG to inject dynamic or frequently updated information (e.g., current policies, real-time data) at query time. This hybrid strategy is especially effective when your system needs to reason over both historical and real-time knowledge.\nVague Recollection vs Working Memory To understand how RAFT works, it helps to think of your LLM like a person:\nFine-tuned knowledge is like vague recollection — facts learned and internalized over time. Stored in the model’s weights. RAG-injected knowledge is like working memory — recent or immediate context added temporarily. Stored in the context window. Example:\nSummarizing a book you read months ago? That’s vague recollection (fine-tuned knowledge). Summarizing a chapter you just read? That’s working memory (RAG). By combining both, RAFT enables your LLM to be knowledgeable, adaptable, and context-aware.\nUseful Resources Major’s Hayden blog on RAG Embedding models leaderboard Effective context window size for models (NVIDIA Ruler) Repository with advanced RAG techniques LlamaIndex Introduction to RAG LLamaIndex Production RAG r/RAG SML RAG Arena What’s Next? Go and try building a RAG system by using existing frameworks like LLamaIndex or LangChain.\nIn the next days I’ll be publishing a test app I made while trying to learn about RAG internals.\n","wordCount":"2442","inLanguage":"en","datePublished":"2025-05-27T00:00:00Z","dateModified":"2025-05-27T00:00:00Z","author":{"@type":"Person","name":"Mario"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://linuxera.org/rag-beginners-guide/"},"publisher":{"@type":"Organization","name":"Linuxera","logo":{"@type":"ImageObject","url":"https://linuxera.org/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://linuxera.org/ accesskey=h title="Linuxera (Alt + H)">Linuxera</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://linuxera.org/ title="🏠 Home"><span>🏠 Home</span></a></li><li><a href=https://linuxera.org/archives/ title="🗄️ Archive"><span>🗄️ Archive</span></a></li><li><a href=https://linuxera.org/search/ title="🔎 Search"><span>🔎 Search</span></a></li><li><a href=https://linuxera.org/tags/ title="🏷️ Tags"><span>🏷️ Tags</span></a></li><li><a href=https://linuxera.org/presentations/ title="🎴 Presentations"><span>🎴 Presentations</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://linuxera.org/>Home</a>&nbsp;»&nbsp;<a href=https://linuxera.org/posts/>Posts</a></div><h1 class=post-title>A Beginner’s Guide to RAG: What I Wish Someone Told Me</h1><div class=post-meta><span title='2025-05-27 00:00:00 +0000 UTC'>Published on May 27, 2025</span>&nbsp;·&nbsp;<span title='2025-05-27 00:00:00 +0000 UTC'>Last updated on May 27, 2025</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Mario</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#a-beginners-guide-to-rag-what-i-wish-someone-told-me aria-label="A Beginner’s Guide to RAG: What I Wish Someone Told Me">A Beginner’s Guide to RAG: What I Wish Someone Told Me</a><ul><li><a href=#what-is-retrieval-augmented-generation-rag aria-label="What is Retrieval-Augmented Generation (RAG)">What is Retrieval-Augmented Generation (RAG)</a></li><li><a href=#why-rag aria-label="Why RAG?">Why RAG?</a></li><li><a href=#embeddings aria-label=Embeddings>Embeddings</a></li><li><a href=#context-window aria-label="Context Window">Context Window</a><ul><li><a href=#why-is-the-context-window-important-for-rag aria-label="Why is the Context Window important for RAG?">Why is the Context Window important for RAG?</a></li></ul></li><li><a href=#key-considerations-for-rag aria-label="Key Considerations for RAG">Key Considerations for RAG</a><ul><li><a href=#document-quality-over-quantity aria-label="Document Quality over Quantity">Document Quality over Quantity</a></li><li><a href=#unstructed-vs-structured aria-label="Unstructed vs Structured">Unstructed vs Structured</a></li></ul></li><li><a href=#rag-searches aria-label="RAG Searches">RAG Searches</a><ul><li><a href=#semantic-search aria-label="Semantic Search">Semantic Search</a><ul><li><a href=#semantic-search-algorithms aria-label="Semantic Search Algorithms">Semantic Search Algorithms</a><ul><li><a href=#cosine-similarity aria-label="Cosine Similarity">Cosine Similarity</a></li><li><a href=#dot-product aria-label="Dot Product">Dot Product</a></li><li><a href=#euclidean-distance aria-label="Euclidean Distance">Euclidean Distance</a></li></ul></li></ul></li><li><a href=#keyword--full-text-search aria-label="Keyword / Full-text Search">Keyword / Full-text Search</a></li><li><a href=#hybrid-search aria-label="Hybrid Search">Hybrid Search</a></li></ul></li><li><a href=#improving-rag-searches aria-label="Improving RAG Searches">Improving RAG Searches</a><ul><li><a href=#reranking aria-label=Reranking>Reranking</a></li><li><a href=#refine-users-questions aria-label="Refine user&amp;rsquo;s questions">Refine user&rsquo;s questions</a></li><li><a href=#use-the-right-algorithm-for-semantic-search aria-label="Use the right algorithm for semantic search">Use the right algorithm for semantic search</a></li></ul></li><li><a href=#improving-rag-answers aria-label="Improving RAG Answers">Improving RAG Answers</a><ul><li><a href=#always-include-source-links aria-label="Always Include Source Links">Always Include Source Links</a></li><li><a href=#limit-the-number-of-documents-used aria-label="Limit the Number of Documents Used">Limit the Number of Documents Used</a></li><li><a href=#manage-the-context-window-wisely aria-label="Manage the Context Window Wisely">Manage the Context Window Wisely</a></li></ul></li><li><a href=#embeddings-strategies aria-label="Embeddings Strategies">Embeddings Strategies</a><ul><li><a href=#full-length-embeddings aria-label="Full-Length Embeddings">Full-Length Embeddings</a></li><li><a href=#chunk-level-embeddings aria-label="Chunk-Level Embeddings">Chunk-Level Embeddings</a></li><li><a href=#combine-the-two aria-label="Combine the Two">Combine the Two</a></li></ul></li><li><a href=#rag-vs-fine-tuning aria-label="RAG vs Fine-Tuning">RAG vs Fine-Tuning</a></li><li><a href=#raft-rag--fine-tuning aria-label="RAFT (RAG + Fine-Tuning)">RAFT (RAG + Fine-Tuning)</a><ul><li><a href=#vague-recollection-vs-working-memory aria-label="Vague Recollection vs Working Memory">Vague Recollection vs Working Memory</a></li></ul></li><li><a href=#useful-resources aria-label="Useful Resources">Useful Resources</a></li><li><a href=#whats-next aria-label="What&amp;rsquo;s Next?">What&rsquo;s Next?</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=a-beginners-guide-to-rag-what-i-wish-someone-told-me>A Beginner’s Guide to RAG: What I Wish Someone Told Me<a hidden class=anchor aria-hidden=true href=#a-beginners-guide-to-rag-what-i-wish-someone-told-me>#</a></h1><p>In this post, I&rsquo;ll try to provide a beginners guide to RAG, focusing on what I wish someone told me before trying to build a RAG solution.</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>While I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct. If you notice anything missing or inaccurate, please leave a comment!</p></div><h2 id=what-is-retrieval-augmented-generation-rag>What is Retrieval-Augmented Generation (RAG)<a hidden class=anchor aria-hidden=true href=#what-is-retrieval-augmented-generation-rag>#</a></h2><p>RAG (Retrieval-Augmented Generation) is an AI technique that combines a language model with a search system to retrieve relevant documents and use them as context for generating more accurate, informed, and up-to-date responses. It enhances output by grounding it in external knowledge.</p><p>To put it simple, we provide the <a href=https://linuxera.org/introduction-to-llm-concepts/>LLM (Large Language Model)</a> with a set of documents it has to use when answering. These documents, are processed and stored in a vector database.</p><h2 id=why-rag>Why RAG?<a hidden class=anchor aria-hidden=true href=#why-rag>#</a></h2><p>There are several use cases that benefit from the RAG pattern. Based on my findings, the most common involve leveraging existing documents to answer user questions.</p><p>For example:</p><p>Let’s say your company has lots of internal documents — onboarding manuals, policy guides, troubleshooting playbooks — scattered across different platforms. You want to let employees ask questions like:</p><ul><li>&ldquo;How do I request parental leave?&rdquo;</li><li>&ldquo;How do I set up a VPN on a company-issued laptop?&rdquo;</li><li>&ldquo;What’s the on-call escalation policy for SREs?&rdquo;</li></ul><p>Public LLMs don&rsquo;t have access to any of that, they don&rsquo;t know your company’s internal knowledge. But a RAG system can retrieve the relevant documents at inference time and inject them into the model’s context. This grounds the answers in your organization’s actual data.</p><p>RAG is especially useful for internal search, support bots, and documentation assistants - particularly when your knowledge base is large and constantly changing.</p><h2 id=embeddings>Embeddings<a hidden class=anchor aria-hidden=true href=#embeddings>#</a></h2><p>To make documents usable for a Retrieval-Augmented Generation (RAG) system, they must be converted into vector embeddings (numerical representations that capture semantic meaning). These embeddings are generated using a specialized embedding model, and then stored in a vector database, where they can later be searched and retrieved based on similarity to a user’s query.</p><p>The typical flow looks like this:</p><p>Document -> Embedding Model (<a href=https://linuxera.org/introduction-to-llm-concepts/#what-are-tokens-in-the-context-of-llms>tokenizes internally</a>) -> Vector Database</p><h2 id=context-window>Context Window<a hidden class=anchor aria-hidden=true href=#context-window>#</a></h2><p>We described Context Window in more detail in a <a href=https://linuxera.org/introduction-to-llm-concepts/#what-is-the-context-windowcontext-lengthmodel-max-length>previous blog post</a>, but here’s a quick refresher.</p><p>The Context Window defines the maximum number of tokens (words, subwords, or characters depending on the tokenizer) the model can process in a single input sequence (including both the prompt and any output generated during a single pass).</p><p>For example, if a model has a context window of 2048 tokens, and the prompt uses 1000 tokens, the model has 1048 tokens left for generating a response.</p><h3 id=why-is-the-context-window-important-for-rag>Why is the Context Window important for RAG?<a hidden class=anchor aria-hidden=true href=#why-is-the-context-window-important-for-rag>#</a></h3><p>In Retrieval-Augmented Generation (RAG), once relevant documents are retrieved from the vector database, they are injected into the context window along with the user’s query. This is how the model gains access to external knowledge at inference time.</p><p>If the combined length of the query and retrieved content exceeds the context window, the model won’t be able to see the full input. This can result in:</p><ul><li>Truncated documents</li><li>Missing critical context</li><li>Inaccurate or hallucinated responses</li></ul><p>In summary, the context window acts as the model&rsquo;s working memory. Keeping the inputs within its limits is essential for accurate, high-quality answers in RAG systems.</p><h2 id=key-considerations-for-rag>Key Considerations for RAG<a hidden class=anchor aria-hidden=true href=#key-considerations-for-rag>#</a></h2><h3 id=document-quality-over-quantity>Document Quality over Quantity<a hidden class=anchor aria-hidden=true href=#document-quality-over-quantity>#</a></h3><p>&ldquo;Garbage in, garbage out&rdquo; applies especially to RAG systems. Even the most advanced LLM can&rsquo;t compensate for low-quality source material. If you wouldn&rsquo;t hand a document to a new hire to explain a topic, don’t hand it to your model either.</p><p>To improve the quality of your documents, consider the following strategies:</p><ul><li>Collaborate with content owners to update and improve documentation.</li><li>Use LLMs to summarize, clean up, or extract key points — there are specialized models for creating FAQs, concise summaries, etc.</li></ul><p>Also, prepare documents with retrievability in mind. Remove elements that are hard to parse or irrelevant for text-based models:</p><ul><li>Diagrams, images, screenshots</li><li>Long blocks of low-readability content (e.g., raw tracebacks or logs)</li></ul><p>In summary, don’t try to go from zero to hero. Start with high-quality documents you trust, and gradually expand your ingestion pipeline as you validate the results.</p><h3 id=unstructed-vs-structured>Unstructed vs Structured<a hidden class=anchor aria-hidden=true href=#unstructed-vs-structured>#</a></h3><p>In RAG, it&rsquo;s critical to understand the type of data you’re working with:</p><ul><li>Unstructured data: Free-form text like articles, web pages, manuals, wikis, and documentation. No strict schema.</li><li>Structured data: Information with a predefined schema, such as database rows, spreadsheets, or API responses.</li></ul><p>If you’re working with structured data, classic RAG may not be the best fit. Instead, consider using Agentic RAG, where the LLM is given access to tools (like APIs or SQL queries) that retrieve structured data on-demand and inject it into the context window.</p><p>Agentic RAG also works well when you need to blend structured and unstructured sources in a single system.</p><h2 id=rag-searches>RAG Searches<a hidden class=anchor aria-hidden=true href=#rag-searches>#</a></h2><p>Retrieval-Augmented Generation (RAG) systems rely on effective document retrieval techniques to surface relevant context for the LLM. There are three main search strategies you should be familiar with: semantic search, keyword search, and hybrid approaches.</p><h3 id=semantic-search>Semantic Search<a hidden class=anchor aria-hidden=true href=#semantic-search>#</a></h3><p>Semantic search focuses on understanding the meaning behind a query rather than matching specific words. It uses vector embeddings to represent both the query and documents in a high-dimensional space, then measures their similarity.</p><p>This is especially useful when the user’s query uses different phrasing than the source material — for example, searching &ldquo;How to get a refund?&rdquo; could match documents about &ldquo;return policies.&rdquo;</p><p>Common similarity metrics include:</p><ul><li>Cosine Similarity: Measures the angle between two vectors — good for checking direction/semantic similarity.</li><li>Dot Product: Takes into account both similarity and vector magnitude — useful when embedding magnitude carries importance. (Be cautious: unnormalized dot product can favor longer text regardless of relevance.)</li><li>Euclidean Distance: Measures literal distance — often used in clustering or when you want to group semantically close documents.</li></ul><h4 id=semantic-search-algorithms>Semantic Search Algorithms<a hidden class=anchor aria-hidden=true href=#semantic-search-algorithms>#</a></h4><h5 id=cosine-similarity>Cosine Similarity<a hidden class=anchor aria-hidden=true href=#cosine-similarity>#</a></h5><p>Compare two texts to see if they talk about the same thing, even if one is longer.</p><ul><li>Doc A: &ldquo;The capital of France is Paris.&rdquo;</li><li>Doc B: &ldquo;Paris is the capital city of France, known for the Eiffel Tower.”</li></ul><p>Despite different lengths and wordings, their cosine similarity will be high because they are semantically aligned.</p><h5 id=dot-product>Dot Product<a hidden class=anchor aria-hidden=true href=#dot-product>#</a></h5><p>Prioritize documents that not only match the query direction but also carry stronger signals (e.g., longer content, weighted terms).</p><ul><li>Doc A: &ldquo;200 words about EU regulations on emissions.&rdquo;</li><li>Doc B: &ldquo;2000 words with comprehensive policy analysis, including emissions and carbon tax.&rdquo;</li></ul><p>Both match query direction, but doc B has a higher magnitude. Possibly due to length, or emphasis.</p><h5 id=euclidean-distance>Euclidean Distance<a hidden class=anchor aria-hidden=true href=#euclidean-distance>#</a></h5><p>Grouping documents with similar embeddings into clusters, regardless of angle. Closer means more similar overall vector positions.</p><ul><li>You embed 1000 support tickets, and you want to cluster them into 5 issue types. Euclidean will help you do K-Means Clustering to find natural groupings.</li></ul><h3 id=keyword--full-text-search>Keyword / Full-text Search<a hidden class=anchor aria-hidden=true href=#keyword--full-text-search>#</a></h3><p>Keyword-based search is more literal. It tries to find exact words or phrases within documents — like traditional search engines or grep. It’s ideal when the user is looking for a specific term, phrase, or entity.</p><p>A commonly used algorithm here is:</p><ul><li>BM25 (Best Match 25): A probabilistic model that ranks documents based on the occurrence and frequency of query terms, adjusted by document length.</li></ul><p>While it doesn’t capture semantic meaning, keyword search is fast, simple, and precise — especially for technical content with consistent terminology.</p><h3 id=hybrid-search>Hybrid Search<a hidden class=anchor aria-hidden=true href=#hybrid-search>#</a></h3><p>Hybrid search combines both semantic and keyword search to maximize recall and precision. A typical approach might:</p><ol><li>Use keyword search (e.g: BM25) to quickly narrow down a large document set.</li><li>Use semantic search to rank or rerank the candidates based on relevance and meaning.</li></ol><p>This approach gives you the best of both worlds: the precision of keywords with the flexibility of semantics.</p><p>The order of operations in hybrid search isn’t fixed — it depends on your use case:</p><ul><li>Keyword First → Semantic Rerank: Good when you have a lot of documents and the question uses specific terms. It’s fast and works well.</li><li>Semantic First → Keyword Filter or Boost: Good when people might phrase things differently. Helps find relevant results even if the wording doesn’t match exactly.</li></ul><h2 id=improving-rag-searches>Improving RAG Searches<a hidden class=anchor aria-hidden=true href=#improving-rag-searches>#</a></h2><p>A RAG system is only as good as the documents it retrieves. Even if your documents are high-quality, poor retrieval can lead to weak answers. These strategies help improve document retrieval and relevance.</p><h3 id=reranking>Reranking<a hidden class=anchor aria-hidden=true href=#reranking>#</a></h3><p>When your system retrieves documents — whether using semantic, keyword, or hybrid search — you typically get a list of candidates with scores. Reranking helps reorder these results to better match your specific use case.</p><p>Here are common reranking strategies:</p><ul><li>LLM-based Reranking: Use a language model to evaluate which results best answer the user’s question.</li><li>Cross-Encoder Reranking: Use a model that takes both the query and document together to assign a new relevance score.</li><li>Custom Rerankers: Create your own logic to boost or demote certain results based on metadata (e.g., date, author, document type, etc.).</li></ul><h3 id=refine-users-questions>Refine user&rsquo;s questions<a hidden class=anchor aria-hidden=true href=#refine-users-questions>#</a></h3><p>Sometimes the original question doesn’t provide enough context, especially in ambiguous cases. For example:</p><ul><li>User query: Where can I buy a mouse?</li><li>LLM: Do they mean a computer peripheral or a pet?</li></ul><p>To avoid confusion, you can refine the query using an LLM by injecting context. If you know your documents cover internal hardware policies, your refinement prompt might look like:</p><ul><li>&ldquo;You&rsquo;re a helpful assistant specialized in helping users buy computer hardware following company policy.&rdquo;</li></ul><p>Make sure to include the user’s original question in the final prompt. This helps keep the user’s intent while clarifying the meaning for better retrieval.</p><h3 id=use-the-right-algorithm-for-semantic-search>Use the right algorithm for semantic search<a hidden class=anchor aria-hidden=true href=#use-the-right-algorithm-for-semantic-search>#</a></h3><p>Different similarity algorithms can give different retrieval behaviors. Choose based on what matters most in your case.</p><p>You can also combine them, for example:</p><ol><li>Use <a href=#cosine-similarity>Cosine Similarity</a> to get meaning-aligned docs.</li><li>Use <a href=#dot-product>Dot Product</a> to emphasize importance or weight.</li></ol><h2 id=improving-rag-answers>Improving RAG Answers<a hidden class=anchor aria-hidden=true href=#improving-rag-answers>#</a></h2><p>Once your RAG system retrieves the right documents, the next step is generating accurate and trustworthy answers. Here are some key practices to improve answer quality.</p><h3 id=always-include-source-links>Always Include Source Links<a hidden class=anchor aria-hidden=true href=#always-include-source-links>#</a></h3><p>Make sure the system outputs links or references to the documents used to generate an answer. This improves transparency and helps users verify the information.</p><h3 id=limit-the-number-of-documents-used>Limit the Number of Documents Used<a hidden class=anchor aria-hidden=true href=#limit-the-number-of-documents-used>#</a></h3><p>Allow your system to restrict the number of retrieved documents passed into the context window — for example, the top 3 most relevant ones.</p><p>Fewer, more relevant documents often lead to better answers than many loosely related ones.</p><h3 id=manage-the-context-window-wisely>Manage the Context Window Wisely<a hidden class=anchor aria-hidden=true href=#manage-the-context-window-wisely>#</a></h3><p>The context window is finite — and balance is key. When injecting documents into the prompt:</p><p>If the documents are small and fit entirely into the context window, include the full document:</p><ul><li>But don’t overload the context —> this can confuse the model (overstuffing).</li><li>And don’t underload —> the model may hallucinate or miss key details (undersupplying).</li></ul><p>Example:</p><ul><li>Want a summary of Chapter 1 of a book? ➜ Send only Chapter 1.</li><li>Want to know how many times a character appears? ➜ Send the whole book (if it fits).</li></ul><h2 id=embeddings-strategies>Embeddings Strategies<a hidden class=anchor aria-hidden=true href=#embeddings-strategies>#</a></h2><p>In a RAG system, how you generate embeddings has a big impact on both retrieval performance and answer quality. The two most common approaches are full-length and chunk-level embeddings — and each has trade-offs.</p><h3 id=full-length-embeddings>Full-Length Embeddings<a hidden class=anchor aria-hidden=true href=#full-length-embeddings>#</a></h3><p>Full-length embeddings are vector representations of entire documents or large text blocks. Their goal is to capture the overall semantic meaning of the content.</p><ul><li>Efficient retrieval: Fewer vectors to store and search (e.g., 1 vector per document).</li><li>Lower granularity: May miss fine details buried deep in the text.</li></ul><p>This approach is useful when you&rsquo;re trying to quickly identify which document is most relevant — like finding the right manual or report.</p><h3 id=chunk-level-embeddings>Chunk-Level Embeddings<a hidden class=anchor aria-hidden=true href=#chunk-level-embeddings>#</a></h3><p>Chunk-level embeddings break documents into smaller parts — sentences, paragraphs, or sections — and generate vectors for each chunk. These embeddings focus on local details.</p><ul><li>Higher precision: Great for surfacing exact sections relevant to a query.</li><li>More expensive: You’ll need to store and search many more vectors. While more vectors improve granularity, they increase index size, memory use, and retrieval latency — especially for large bodies of text.</li></ul><p>This approach shines when specific answers are scattered throughout large documents — like answering “What ports does this device support?” from a 100-page manual.</p><h3 id=combine-the-two>Combine the Two<a hidden class=anchor aria-hidden=true href=#combine-the-two>#</a></h3><p>A smart strategy is to combine both approaches:</p><ol><li>Start with full-length embeddings to quickly identify relevant documents. This narrows down your search space (e.g., 1,000 vectors instead of 10,000).</li><li>Then, search within those documents using chunk-level embeddings to locate the most relevant sections.</li></ol><p>This hybrid method may require using different embedding models optimized for different levels of granularity — but the payoff is better retrieval and more accurate answers.</p><h2 id=rag-vs-fine-tuning>RAG vs Fine-Tuning<a hidden class=anchor aria-hidden=true href=#rag-vs-fine-tuning>#</a></h2><p>RAG injects relevant external documents into the model’s context at inference time to improve responses without changing the model itself. It’s flexible and keeps up with changing information.</p><p>Fine-tuning modifies the model&rsquo;s internal weights by training it on task-specific data. This can improve performance but updates require retraining and are less responsive to real-time changes compared to RAG.</p><p>In summary, RAG adds knowledge dynamically at inference time; fine-tuning bakes knowledge into the model permanently.</p><h2 id=raft-rag--fine-tuning>RAFT (RAG + Fine-Tuning)<a hidden class=anchor aria-hidden=true href=#raft-rag--fine-tuning>#</a></h2><p>RAFT (Retrieval-Augmented Generation + Fine-Tuning) brings together the strengths of both approaches to maximize performance and flexibility.</p><ul><li>Fine-tune your LLM on high-quality, stable knowledge (e.g., internal documentation, product specs, historical data).</li><li>Use RAG to inject dynamic or frequently updated information (e.g., current policies, real-time data) at query time.</li></ul><p>This hybrid strategy is especially effective when your system needs to reason over both historical and real-time knowledge.</p><h3 id=vague-recollection-vs-working-memory>Vague Recollection vs Working Memory<a hidden class=anchor aria-hidden=true href=#vague-recollection-vs-working-memory>#</a></h3><p>To understand how RAFT works, it helps to think of your LLM like a person:</p><ul><li>Fine-tuned knowledge is like vague recollection — facts learned and internalized over time. Stored in the model’s weights.</li><li>RAG-injected knowledge is like working memory — recent or immediate context added temporarily. Stored in the context window.</li></ul><p>Example:</p><ul><li>Summarizing a book you read months ago? That&rsquo;s vague recollection (fine-tuned knowledge).</li><li>Summarizing a chapter you just read? That’s working memory (RAG).</li></ul><p>By combining both, RAFT enables your LLM to be knowledgeable, adaptable, and context-aware.</p><h2 id=useful-resources>Useful Resources<a hidden class=anchor aria-hidden=true href=#useful-resources>#</a></h2><ul><li><a href=https://major.io/p/dont-tell-me-rag-is-easy/>Major&rsquo;s Hayden blog on RAG</a></li><li><a href=https://huggingface.co/spaces/mteb/leaderboard>Embedding models leaderboard</a></li><li><a href=https://github.com/NVIDIA/RULER>Effective context window size for models (NVIDIA Ruler)</a></li><li><a href=https://github.com/NirDiamant/RAG_Techniques>Repository with advanced RAG techniques</a></li><li><a href=https://docs.llamaindex.ai/en/stable/understanding/rag/>LlamaIndex Introduction to RAG</a></li><li><a href=https://docs.llamaindex.ai/en/stable/optimizing/production_rag/>LLamaIndex Production RAG</a></li><li><a href=https://www.reddit.com/r/Rag/>r/RAG</a></li><li><a href=https://huggingface.co/spaces/aizip-dev/SLM-RAG-Arena>SML RAG Arena</a></li></ul><h2 id=whats-next>What&rsquo;s Next?<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h2><p>Go and try building a RAG system by using existing frameworks like <a href=https://www.llamaindex.ai/>LLamaIndex</a> or <a href=https://www.langchain.com/>LangChain</a>.</p><p>In the next days I&rsquo;ll be publishing a test app I made while trying to learn about RAG internals.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://linuxera.org/tags/artificialintelligence/>artificialintelligence</a></li><li><a href=https://linuxera.org/tags/ai/>ai</a></li><li><a href=https://linuxera.org/tags/llm/>llm</a></li><li><a href=https://linuxera.org/tags/llms/>llms</a></li><li><a href=https://linuxera.org/tags/rag/>rag</a></li></ul><nav class=paginav><a class=next href=https://linuxera.org/introduction-to-llm-concepts/><span class=title>Next »</span><br><span>Introduction to LLM concepts</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share A Beginner’s Guide to RAG: What I Wish Someone Told Me on twitter" href="https://twitter.com/intent/tweet/?text=A%20Beginner%e2%80%99s%20Guide%20to%20RAG%3a%20What%20I%20Wish%20Someone%20Told%20Me&url=https%3a%2f%2flinuxera.org%2frag-beginners-guide%2f&hashtags=artificialintelligence%2cai%2cllm%2cllms%2crag"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Beginner’s Guide to RAG: What I Wish Someone Told Me on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flinuxera.org%2frag-beginners-guide%2f&title=A%20Beginner%e2%80%99s%20Guide%20to%20RAG%3a%20What%20I%20Wish%20Someone%20Told%20Me&summary=A%20Beginner%e2%80%99s%20Guide%20to%20RAG%3a%20What%20I%20Wish%20Someone%20Told%20Me&source=https%3a%2f%2flinuxera.org%2frag-beginners-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></div></footer><div class=share-buttons><p>If this post has been helpful to you, consider <u><a target=_blank href=https://ko-fi.com/mvazce>supporting the work.</a></u></p></div><script src=https://utteranc.es/client.js repo=mvazquezc/mvazquezc.github.io issue-term=pathname label=blog-comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://linuxera.org/>Linuxera</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Turning the Knobs of LLM Text Generation | Linuxera</title><meta name=keywords content="context engineering,llm,AI,artificial intelligence,top-k,top-p,temperature"><meta name=description content="Turning the Knobs of LLM Text Generation Ever wonder how much control you actually have over the text an LLM produces? In this post, we will look at three simple but powerful knobs you can tweak to push a model toward more deterministic output or toward something more creative.
We are talking about top_k, top_p and temperature. But before describing them, we need to understand the two main behaviors we can get from an LLM when it is sampling tokens:"><meta name=author content="Mario"><link rel=canonical href=https://linuxera.org/turning-the-knobs-of-llm-text-generation/><link crossorigin=anonymous href=/assets/css/stylesheet.8cc7ef3cdd44c5188f9267864f378d8dd8892d583e0fd07b5e5321e496f1e4d1.css integrity="sha256-jMfvPN1ExRiPkmeGTzeNjdiJLVg+D9B7XlMh5Jbx5NE=" rel="preload stylesheet" as=style><link rel=icon href=https://linuxera.org/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://linuxera.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://linuxera.org/favicon-32x32.png><link rel=apple-touch-icon href=https://linuxera.org/apple-touch-icon.png><link rel=mask-icon href=https://linuxera.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script defer data-domain=linuxera.org src=https://stats.linuxera.org/js/script.file-downloads.hash.outbound-links.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script><meta property="og:title" content="Turning the Knobs of LLM Text Generation"><meta property="og:description" content="Turning the Knobs of LLM Text Generation Ever wonder how much control you actually have over the text an LLM produces? In this post, we will look at three simple but powerful knobs you can tweak to push a model toward more deterministic output or toward something more creative.
We are talking about top_k, top_p and temperature. But before describing them, we need to understand the two main behaviors we can get from an LLM when it is sampling tokens:"><meta property="og:type" content="article"><meta property="og:url" content="https://linuxera.org/turning-the-knobs-of-llm-text-generation/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-26T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-26T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Turning the Knobs of LLM Text Generation"><meta name=twitter:description content="Turning the Knobs of LLM Text Generation Ever wonder how much control you actually have over the text an LLM produces? In this post, we will look at three simple but powerful knobs you can tweak to push a model toward more deterministic output or toward something more creative.
We are talking about top_k, top_p and temperature. But before describing them, we need to understand the two main behaviors we can get from an LLM when it is sampling tokens:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://linuxera.org/posts/"},{"@type":"ListItem","position":2,"name":"Turning the Knobs of LLM Text Generation","item":"https://linuxera.org/turning-the-knobs-of-llm-text-generation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Turning the Knobs of LLM Text Generation","name":"Turning the Knobs of LLM Text Generation","description":"Turning the Knobs of LLM Text Generation Ever wonder how much control you actually have over the text an LLM produces? In this post, we will look at three simple but powerful knobs you can tweak to push a model toward more deterministic output or toward something more creative.\nWe are talking about top_k, top_p and temperature. But before describing them, we need to understand the two main behaviors we can get from an LLM when it is sampling tokens:","keywords":["context engineering","llm","AI","artificial intelligence","top-k","top-p","temperature"],"articleBody":"Turning the Knobs of LLM Text Generation Ever wonder how much control you actually have over the text an LLM produces? In this post, we will look at three simple but powerful knobs you can tweak to push a model toward more deterministic output or toward something more creative.\nWe are talking about top_k, top_p and temperature. But before describing them, we need to understand the two main behaviors we can get from an LLM when it is sampling tokens:\nGreedy Sampling: At each step, the model picks the token with the highest probability given the preceding context.\nPros: Produces output that is typically coherent and aligned with the most common patterns in the training data. Is deterministic. Same prompt, same output every time. Cons: Often dull or repetitive because it never takes alternative paths. Short-sighted: the ‚Äúbest next token‚Äù doesn‚Äôt always lead to the best overall sequence. Random (Stochastic) Sampling: The model draws the next token from the entire probability distribution. Higher probability tokens are more likely, but even low probability tokens can be sampled.\nPros: Output is more diverse and creative. Helps avoid repetitive loops that greedy sampling can fall into. Cons: Can reduce coherence or quality. Non-deterministic. Running the same prompt multiple times produces different results. The influence of top_k, top_p, and temperature lies in how they control the amount of randomness the model is allowed to use during sampling. By adjusting them, you can push the model closer to greedy, deterministic behavior, make it sample from a broader range of possibilities, or land anywhere in between.\nTop-K This parameter limits the model to the top K most probable tokens at each generation step. Everything outside that shortlist is ignored.\nExample Let‚Äôs say we have the prompt The future of AI is, and we configure top_k = 5. This means the model will pick only tokens among the five tokens with the highest probability.\nThat list might look like this:\nToken: ' in' (ID: 11) | Logprob: -2.1674 | %: 11.45% Token: ' not' (ID: 45) | Logprob: -3.1830 | %: 4.15% Token: ' now' (ID: 122) | Logprob: -3.4174 | %: 3.28% Token: ' here' (ID: 259) | Logprob: -3.4330 | %: 3.23% Token: ' a' (ID: 10) | Logprob: -3.4955 | %: 3.03% With top_k = 5, the model must pick from these five tokens and ignore everything else. How it chooses among them depends on your temperature setting.\nIf temperature = 0, the sampling becomes greedy and the model will always select the token with the highest probability,¬†in in the example above.\nTop-P (Nucleus Sampling) This parameter filters out tokens once the cumulative probability threshold is reached. Unlike top_k, which always keeps a fixed number of tokens, top_p includes as many tokens as necessary until their combined probability reaches the chosen value.\nExample top_k = 5 -\u003e Always selects the 5 highest-probability tokens. top_p = 0.3 -\u003e Collects tokens (from highest to lowest probability) until their cumulative probability reaches 30%. Given a list of token probabilities, we accumulate them in descending order:\nToken: ' the' | Prob: 8.86% | Cumul: 8.86% Token: ' a' | Prob: 7.61% | Cumul: 16.48% Token: ' not' | Prob: 3.81% | Cumul: 20.29% Token: ' in' | Prob: 3.11% | Cumul: 23.40% Token: ' being' | Prob: 2.16% | Cumul: 25.56% Token: ' one' | Prob: 1.36% | Cumul: 26.93% Token: ' home' | Prob: 1.36% | Cumul: 28.29% Token: ' now' | Prob: 1.29% | Cumul: 29.58% Token: ' getting' | Prob: 1.24% | Cumul: 30.82% Top-p threshold reached (30.82% / 30.0%). Remaining tokens are ignored. With top_p = 0.3, the model would sample from this set of tokens, regardless of how many there are. In this example, it happened to include nine tokens, 4 more tokens than the fixed limit you‚Äôd get from top_k = 5. These could have been different if tokens had a high probability, in that case we would have gotten fewer tokens than with top_k = 5.\nAgain, if temperature = 0, the sampling becomes greedy and the model will always select the token with the highest probability,¬†the in the example above.\nTemperature This parameter adjusts how ‚Äúspread out‚Äù the probability distribution is during sampling by scaling the model‚Äôs logits before selecting the next token.\nLow temperature (closer to 0): Sharpens the distribution, making high-probability tokens even more likely. Output becomes predictable and stable. High temperature (closer to 1): Flattens the distribution, increasing the chances of lower-probability tokens. Output becomes more varied and creative, but also more chaotic if pushed too far. temperature = 0 removes randomness entirely and forces greedy behavior (always picking the token with the highest probability).\nHow can I adjust those parameters? Good question. How you can tweak top_k, top_p, and temperature depends on how you‚Äôre using LLMs.\nIf you‚Äôre interacting through managed web interfaces (like ChatGPT‚Äôs website), you typically cannot adjust these parameters. If you‚Äôre using API access provided by the service, you can usually control them when sending requests. Always check the official documentation of your provider for the exact way to set these parameters in API calls.\nWhat‚Äôs next? If you want to experiment with tuning these parameters on open models, check out this Jupyter Notebook, which uses vLLM python bindings to tune LLM text generation.\nJupyter Notebook ","wordCount":"885","inLanguage":"en","datePublished":"2025-11-26T00:00:00Z","dateModified":"2025-11-26T00:00:00Z","author":{"@type":"Person","name":"Mario"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://linuxera.org/turning-the-knobs-of-llm-text-generation/"},"publisher":{"@type":"Organization","name":"Linuxera","logo":{"@type":"ImageObject","url":"https://linuxera.org/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://linuxera.org/ accesskey=h title="Linuxera (Alt + H)">Linuxera</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://linuxera.org/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://linuxera.org/archives/ title="üóÑÔ∏è Archive"><span>üóÑÔ∏è Archive</span></a></li><li><a href=https://linuxera.org/search/ title="üîé Search"><span>üîé Search</span></a></li><li><a href=https://linuxera.org/tags/ title="üè∑Ô∏è Tags"><span>üè∑Ô∏è Tags</span></a></li><li><a href=https://linuxera.org/presentations/ title="üé¥ Presentations"><span>üé¥ Presentations</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://linuxera.org/>Home</a>&nbsp;¬ª&nbsp;<a href=https://linuxera.org/posts/>Posts</a></div><h1 class=post-title>Turning the Knobs of LLM Text Generation</h1><div class=post-meta><span title='2025-11-26 00:00:00 +0000 UTC'>Published on November 26, 2025</span>&nbsp;¬∑&nbsp;<span title='2025-11-26 00:00:00 +0000 UTC'>Last updated on November 26, 2025</span>&nbsp;¬∑&nbsp;5 min&nbsp;¬∑&nbsp;Mario</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#turning-the-knobs-of-llm-text-generation aria-label="Turning the Knobs of LLM Text Generation">Turning the Knobs of LLM Text Generation</a><ul><li><a href=#top-k aria-label=Top-K>Top-K</a><ul><li><a href=#example aria-label=Example>Example</a></li></ul></li><li><a href=#top-p-nucleus-sampling aria-label="Top-P (Nucleus Sampling)">Top-P (Nucleus Sampling)</a><ul><li><a href=#example-1 aria-label=Example>Example</a></li></ul></li><li><a href=#temperature aria-label=Temperature>Temperature</a></li><li><a href=#how-can-i-adjust-those-parameters aria-label="How can I adjust those parameters?">How can I adjust those parameters?</a></li><li><a href=#whats-next aria-label="What&amp;rsquo;s next?">What&rsquo;s next?</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=turning-the-knobs-of-llm-text-generation>Turning the Knobs of LLM Text Generation<a hidden class=anchor aria-hidden=true href=#turning-the-knobs-of-llm-text-generation>#</a></h1><p>Ever wonder how much control you actually have over the text an LLM produces? In this post, we will look at three simple but powerful knobs you can tweak to push a model toward more deterministic output or toward something more creative.</p><p>We are talking about <code>top_k</code>, <code>top_p</code> and <code>temperature</code>. But before describing them, we need to understand the two main behaviors we can get from an LLM when it is sampling tokens:</p><ul><li><p><strong>Greedy Sampling</strong>: At each step, the model picks the token with the highest probability given the preceding context.</p><ul><li>Pros:<ul><li>Produces output that is typically coherent and aligned with the most common patterns in the training data.</li><li>Is deterministic. Same prompt, same output every time.</li></ul></li><li>Cons:<ul><li>Often dull or repetitive because it never takes alternative paths.</li><li>Short-sighted: the &ldquo;best next token&rdquo; doesn‚Äôt always lead to the best overall sequence.</li></ul></li></ul></li><li><p><strong>Random (Stochastic) Sampling</strong>: The model draws the next token from the entire probability distribution. Higher probability tokens are more likely, but even low probability tokens can be sampled.</p><ul><li>Pros:<ul><li>Output is more diverse and creative.</li><li>Helps avoid repetitive loops that greedy sampling can fall into.</li></ul></li><li>Cons:<ul><li>Can reduce coherence or quality.</li><li>Non-deterministic. Running the same prompt multiple times produces different results.</li></ul></li></ul></li></ul><p>The influence of <code>top_k</code>, <code>top_p</code>, and <code>temperature</code> lies in how they control the amount of randomness the model is <em>allowed</em> to use during sampling. By adjusting them, you can push the model closer to greedy, deterministic behavior, make it sample from a broader range of possibilities, or land anywhere in between.</p><h2 id=top-k>Top-K<a hidden class=anchor aria-hidden=true href=#top-k>#</a></h2><p>This parameter limits the model to the top K most probable tokens at each generation step. Everything outside that shortlist is ignored.</p><h3 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h3><p>Let&rsquo;s say we have the prompt <code>The future of AI is</code>, and we configure <code>top_k = 5</code>. This means the model will pick only tokens among the five tokens with the highest probability.</p><p>That list might look like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Token: &#39; in&#39; (ID: 11) | Logprob: -2.1674 | %: 11.45%
</span></span><span class=line><span class=cl>Token: &#39; not&#39; (ID: 45) | Logprob: -3.1830 | %: 4.15%
</span></span><span class=line><span class=cl>Token: &#39; now&#39; (ID: 122) | Logprob: -3.4174 | %: 3.28%
</span></span><span class=line><span class=cl>Token: &#39; here&#39; (ID: 259) | Logprob: -3.4330 | %: 3.23%
</span></span><span class=line><span class=cl>Token: &#39; a&#39; (ID: 10) | Logprob: -3.4955 | %: 3.03%
</span></span></code></pre></div><p>With <code>top_k = 5</code>, the model must pick from these five tokens and ignore everything else. How it chooses among them depends on your <code>temperature</code> setting.</p><p>If <code>temperature = 0</code>, the sampling becomes greedy and the model will always select the token with the highest probability,¬†<code> in</code> in the example above.</p><h2 id=top-p-nucleus-sampling>Top-P (Nucleus Sampling)<a hidden class=anchor aria-hidden=true href=#top-p-nucleus-sampling>#</a></h2><p>This parameter filters out tokens once the cumulative probability threshold is reached. Unlike <code>top_k</code>, which always keeps a fixed number of tokens, <code>top_p</code> includes as many tokens as necessary until their combined probability reaches the chosen value.</p><h3 id=example-1>Example<a hidden class=anchor aria-hidden=true href=#example-1>#</a></h3><ul><li><code>top_k = 5</code> -> Always selects the 5 highest-probability tokens.</li><li><code>top_p = 0.3</code> -> Collects tokens (from highest to lowest probability) until their cumulative probability reaches 30%.</li></ul><p>Given a list of token probabilities, we accumulate them in descending order:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Token: &#39; the&#39;       | Prob:  8.86% | Cumul:  8.86%
</span></span><span class=line><span class=cl>Token: &#39; a&#39;         | Prob:  7.61% | Cumul: 16.48%
</span></span><span class=line><span class=cl>Token: &#39; not&#39;       | Prob:  3.81% | Cumul: 20.29%
</span></span><span class=line><span class=cl>Token: &#39; in&#39;        | Prob:  3.11% | Cumul: 23.40%
</span></span><span class=line><span class=cl>Token: &#39; being&#39;     | Prob:  2.16% | Cumul: 25.56%
</span></span><span class=line><span class=cl>Token: &#39; one&#39;       | Prob:  1.36% | Cumul: 26.93%
</span></span><span class=line><span class=cl>Token: &#39; home&#39;      | Prob:  1.36% | Cumul: 28.29%
</span></span><span class=line><span class=cl>Token: &#39; now&#39;       | Prob:  1.29% | Cumul: 29.58%
</span></span><span class=line><span class=cl>Token: &#39; getting&#39;   | Prob:  1.24% | Cumul: 30.82%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Top-p threshold reached (30.82% / 30.0%). Remaining tokens are ignored.
</span></span></code></pre></div><p>With <code>top_p = 0.3</code>, the model would sample from this set of tokens, regardless of how many there are. In this example, it happened to include nine tokens, 4 more tokens than the fixed limit you‚Äôd get from <code>top_k = 5</code>. These could have been different if tokens had a high probability, in that case we would have gotten fewer tokens than with <code>top_k = 5</code>.</p><p>Again, if <code>temperature = 0</code>, the sampling becomes greedy and the model will always select the token with the highest probability,¬†<code> the</code> in the example above.</p><h2 id=temperature>Temperature<a hidden class=anchor aria-hidden=true href=#temperature>#</a></h2><p>This parameter adjusts how &ldquo;spread out&rdquo; the probability distribution is during sampling by scaling the model‚Äôs logits before selecting the next token.</p><ul><li>Low temperature (closer to 0): Sharpens the distribution, making high-probability tokens even more likely. Output becomes predictable and stable.</li><li>High temperature (closer to 1): Flattens the distribution, increasing the chances of lower-probability tokens. Output becomes more varied and creative, but also more chaotic if pushed too far.</li></ul><p><code>temperature = 0</code> removes randomness entirely and forces greedy behavior (always picking the token with the highest probability).</p><h2 id=how-can-i-adjust-those-parameters>How can I adjust those parameters?<a hidden class=anchor aria-hidden=true href=#how-can-i-adjust-those-parameters>#</a></h2><p>Good question. How you can tweak <code>top_k</code>, <code>top_p</code>, and <code>temperature</code> depends on how you‚Äôre using LLMs.</p><ul><li>If you‚Äôre interacting through managed web interfaces (like ChatGPT‚Äôs website), you typically cannot adjust these parameters.</li><li>If you‚Äôre using API access provided by the service, you can usually control them when sending requests.</li></ul><p>Always check the official documentation of your provider for the exact way to set these parameters in API calls.</p><h2 id=whats-next>What&rsquo;s next?<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h2><p>If you want to experiment with tuning these parameters on open models, check out this Jupyter Notebook, which uses <a href=https://docs.vllm.ai/>vLLM</a> python bindings to tune LLM text generation.</p><ul><li><a href=https://github.com/mvazquezc/ai-helpers/blob/main/notebooks/TopK,TopP,Temperature.ipynb>Jupyter Notebook</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://linuxera.org/tags/context-engineering/>context engineering</a></li><li><a href=https://linuxera.org/tags/llm/>llm</a></li><li><a href=https://linuxera.org/tags/ai/>ai</a></li><li><a href=https://linuxera.org/tags/artificial-intelligence/>artificial intelligence</a></li><li><a href=https://linuxera.org/tags/top-k/>top-k</a></li><li><a href=https://linuxera.org/tags/top-p/>top-p</a></li><li><a href=https://linuxera.org/tags/temperature/>temperature</a></li></ul><nav class=paginav><a class=next href=https://linuxera.org/get-data-from-encrypted-etcd-backup/><span class=title>Next ¬ª</span><br><span>Get data from an etcd backup (encrypted or not!)</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Turning the Knobs of LLM Text Generation on twitter" href="https://twitter.com/intent/tweet/?text=Turning%20the%20Knobs%20of%20LLM%20Text%20Generation&url=https%3a%2f%2flinuxera.org%2fturning-the-knobs-of-llm-text-generation%2f&hashtags=contextengineering%2cllm%2cAI%2cartificialintelligence%2ctop-k%2ctop-p%2ctemperature"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Turning the Knobs of LLM Text Generation on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flinuxera.org%2fturning-the-knobs-of-llm-text-generation%2f&title=Turning%20the%20Knobs%20of%20LLM%20Text%20Generation&summary=Turning%20the%20Knobs%20of%20LLM%20Text%20Generation&source=https%3a%2f%2flinuxera.org%2fturning-the-knobs-of-llm-text-generation%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></div></footer><div class=share-buttons><p>If this post has been helpful to you, consider <u><a target=_blank href=https://ko-fi.com/mvazce>supporting the work.</a></u></p></div><script src=https://utteranc.es/client.js repo=mvazquezc/mvazquezc.github.io issue-term=pathname label=blog-comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://linuxera.org/>Linuxera</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
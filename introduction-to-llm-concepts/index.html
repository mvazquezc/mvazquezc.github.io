<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introduction to LLM concepts | Linuxera</title><meta name=keywords content="artificialintelligence,ai,llm,llms,openai"><meta name=description content="Introduction to LLM concepts In this post, I&rsquo;ll cover various LLM concepts and the questions I asked myself while diving deep into the world of LLMs. I expect this post to be updated as I continue to learn more things around LLMs.
Attention
This post is the result of my exploratory work on LLMs. While I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct."><meta name=author content="Mario"><link rel=canonical href=https://linuxera.org/introduction-to-llm-concepts/><link crossorigin=anonymous href=/assets/css/stylesheet.8cc7ef3cdd44c5188f9267864f378d8dd8892d583e0fd07b5e5321e496f1e4d1.css integrity="sha256-jMfvPN1ExRiPkmeGTzeNjdiJLVg+D9B7XlMh5Jbx5NE=" rel="preload stylesheet" as=style><link rel=icon href=https://linuxera.org/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://linuxera.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://linuxera.org/favicon-32x32.png><link rel=apple-touch-icon href=https://linuxera.org/apple-touch-icon.png><link rel=mask-icon href=https://linuxera.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script defer data-domain=linuxera.org src=https://stats.linuxera.org/js/script.file-downloads.hash.outbound-links.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script><meta property="og:title" content="Introduction to LLM concepts"><meta property="og:description" content="Introduction to LLM concepts In this post, I&rsquo;ll cover various LLM concepts and the questions I asked myself while diving deep into the world of LLMs. I expect this post to be updated as I continue to learn more things around LLMs.
Attention
This post is the result of my exploratory work on LLMs. While I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct."><meta property="og:type" content="article"><meta property="og:url" content="https://linuxera.org/introduction-to-llm-concepts/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-25T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-25T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Introduction to LLM concepts"><meta name=twitter:description content="Introduction to LLM concepts In this post, I&rsquo;ll cover various LLM concepts and the questions I asked myself while diving deep into the world of LLMs. I expect this post to be updated as I continue to learn more things around LLMs.
Attention
This post is the result of my exploratory work on LLMs. While I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://linuxera.org/posts/"},{"@type":"ListItem","position":2,"name":"Introduction to LLM concepts","item":"https://linuxera.org/introduction-to-llm-concepts/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introduction to LLM concepts","name":"Introduction to LLM concepts","description":"Introduction to LLM concepts In this post, I\u0026rsquo;ll cover various LLM concepts and the questions I asked myself while diving deep into the world of LLMs. I expect this post to be updated as I continue to learn more things around LLMs.\nAttention\nThis post is the result of my exploratory work on LLMs. While I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct.","keywords":["artificialintelligence","ai","llm","llms","openai"],"articleBody":"Introduction to LLM concepts In this post, I’ll cover various LLM concepts and the questions I asked myself while diving deep into the world of LLMs. I expect this post to be updated as I continue to learn more things around LLMs.\nAttention\nThis post is the result of my exploratory work on LLMs. While I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct. If you notice anything missing or inaccurate, please leave a comment!\nGeneral LLM Concepts and Questions This section describes general concepts and questions around Large Language Models.\nWhat’s a Large Language Model (LLM)? A Large Language Model (LLM) is an AI system trained on vast amounts of text data to understand and generate human-like language. Using deep learning, specifically transformer architectures, LLMs can answer questions, summarize text, generate content, and more. They predict the most likely next word based on context.\nWhat’s the difference between a Base and an Instruct model? A base model predicts the next word, an instruct model has been fine-tuned to predict the next word in a conversation between a user and a helpful assistant and specifically to follow instructions. There are other types like chat models that are fine-tuned to follow a conversation in a chat-like mode with a given format, etc.\nYou could make a base model follow instructions but that requires configuring the model via prompts to do so.\nUsually, base models are used for fine-tune your model based on the base one.\nWe can make a base model behave like an instruct model by formatting our prompts in a consistent way that the model can understand. That’s done via chat templates. If you want to learn more about chat templates, read this. You can see the chat template for an instruct model in the tokenizer_config.json (example).\nAspect Base Models Instruct Models Training Objective General language modeling. Fine-tuned for instruction following. Focus Broad and generic text understanding. Task-specific, user-centric interactions. Performance Literal, unrefined responses. Context-aware, tailored responses. Safety and Alignment Lower alignment with human values. Higher alignment due to RLHF. Examples of Usage Pre-training for fine-tuning tasks. User-facing applications like chatbots. RLHF -\u003e Reinforcement Learning from Human Feedback\nBase Models: Useful as a starting point for further fine-tuning for specific tasks. Instruct Models: Ideal for deployment in interactive environments where users provide natural-language instructions. You may want to use base when you train the model yourself or you want to do purely text/code continuation.\nWhat are Parameters in a LLM Model? In a Large Language Model (LLM), parameters refer to the numerical values within the model that determine how it processes and generates text. These parameters are the weights and biases of the neural network, which are adjusted during the training process.\nTeacher vs Critic vs Judge Model Teacher Model A teacher model in many cases is a large, high-performing model whose outputs are used to train a smaller, more efficient student model. This process (called knowledge distillation) typically involves having the teacher model generate “soft labels” or guidance on either the same dataset or additional unlabeled data, which helps the student learn more nuanced patterns. A teacher model can also be used to generate synthetic data from a curated dataset (i.e: one carefully selected or labeled by humans).\nCritic Model A critic model is designed to evaluate or critique the output of another model. Its role is typically focused on assessing the quality, relevance, accuracy, or coherence of generated outputs and identifying areas of improvement.\nCritic models are often used during training to evaluate outputs from the teacher model.\nJudge Model A judge model is designed to evaluate and rank the performance of multiple outputs. It acts more as an arbiter than a feedback provider, making decisions about which output is “better” according to predefined criteria. Depending on the setup, its role can overlap with or complement that of a critic model.\nJudge models can sometimes be trained on human preference data, which helps them simulate human judgement more closely.\nLLM as a Judge Using LLMs as judges is a powerful solution to asses outputs in a human way, without requiring costly human time. The idea is simple: ask an LLM to do the grading for you.\nYou can read more about this in this hugging face cookbook.\nCritic and Judge models collaboration In advanced LLM setups, critic and judge models often work together:\nCritic improves candidate outputs by iterating over feedback. Judge evaluates the improved outputs to decide the final response. This dynamic can be crucial in applications where fine-tuned outputs, such as accurate medical advice, high-quality creative writing, or nuanced technical analysis, are required.\nRAG vs Fine Tuning The difference between RAG and fine-tuning is that RAG augments a natural language processing (NLP) model by connecting it to an organization’s proprietary database, while fine-tuning optimizes deep learning models for domain-specific tasks. RAG and fine-tuning have the same intended outcome: enhancing a model’s performance to maximize value for the enterprise that uses it.\nRAG uses an organization’s internal data to augment prompt engineering, while fine-tuning retrains a model on a focused set of external data to improve performance.\nWhen we talk about fine-tuning most of the time we refer to parameter-efficient fine-tuning (PEFT) which focuses on tuning only the relevant parameters to make the mode more effective in a certain domain, this helps keeping training costs low.\nAspect RAG (Retrieval-Augmented Generation) Fine-Tuning Knowledge Source External, dynamically retrieved. Internal, baked into the model’s weights. Adaptation Method Combines retrieval and generation dynamically. Updates model weights with new training data. Scalability Scales easily with growing knowledge bases. Requires retraining for new domains. Flexibility Can adapt to changing information in real-time. Static after training; requires retraining for updates. Cost and Time Requires infrastructure for retrieval; no retraining cost. Computationally expensive and time-consuming. Performance Dependent on the quality of the retrieval system. Highly accurate for specific tasks if trained well. Use Cases Open-domain, dynamic tasks needing current data. Fixed-domain, static tasks requiring precision. Evaluating Models In order to evaluate a LLM performance there are different benchmarks that can be used, there are several benchmarks available, two of the most common ones are:\nMMLU: Massive Multitask Language Understanding, tests a model’s proficiency across a wide range of subjects, ensuring broad knowledge coverage. MATH: Evaluates mathematical problem-solving abilities, ensuring that models can handle numerical and logical challenges What is quantization? Quantization is a compression technique in order to reduce model’s size. It maps high precision values to lower precision values for the model’s weights and activations. This compression has an impact on the model accuracy, in some cases comparable results can be achieve with lower precision.\nThe way it works is by representing a range of full precision float32 numbers (FP32) in half precision float16 numbers (FP16), and sometimes even integer numbers like int 4 bits (INT4). Usually quantization is done from FP32 to INT8.\nFor example, if we have a 2B parameter model with half precision FP16 where each parameter takes 16 bits (2bytes), it means the model will weight ~4GB:\n(2000000000*2)/1024/1024/1024 = 3.7\nMore info available here.\nWhat is a distilled model? A distilled model is a smaller, more efficient version of a larger model, trained using a technique called knowledge distillation. The goal is to retain as much of the original model’s performance as possible while reducing its size and computational requirements.\nHow Knowledge Distillation Works A large model (teacher) generates outputs on a dataset.\nA smaller model (student) is trained to mimic the outputs of the teacher model, often learning from both:\nThe original dataset. The teacher model’s predictions, probabilities, or reasoning steps. The student model achieves similar accuracy with fewer parameters, making it faster and cheaper to use.\nThis approach is common in LLMs to create lighter, optimized versions for deployment in real-world applications with limited computing power.\nWhat are tokens in the context of LLMs? We can think of a token as if it was a word, but a token is not always a whole word. For example, English has around ~600k words, while an LLM might have a vocabulary of around ~32k tokens (i.e: llama2).\nTokenization works on sub-words units that can be combined. For example, the tokens “play” and “ful” can be combined to form “playful”. Or we can append “ed” to form “played”.\nYou can see a tokenizer in action in this hugging face tokenizer playground.\nLLMs use special tokens specific to the models to open and close the structured components of its generation. The most important special token is the End of sequence token (EOS). For example SmolLM2 from hugging face uses the following EOS token \u003c|im_end|\u003e. You can see the special tokens for models in the tokenizer_config.json.\nExample for SomlLM2.\nLLMs will predict continue predicting words until the next word with the highest score is the EOS token.\nYou can see how the decoding process works here. And a more advanced strategy here.\nWhat is the context window/context length/model max length? It’s the maximum number of tokens the model can process in a single input sequence (including both the prompt and any output generated during a single pass).\nLet’s say we have a context window of 2048, if we have one input that takes 1000, we have 1048 left for the model to reply.\nRemember that LLMs doesn’t have memory, if you’re in a chat and the model is remembering the conversation is because on each iteration the previous context is being sent as well.\nKeep in mind that the greater the context length is, the more memory it takes to run the model.\nThe maximum context window that a model can handle is often found in the tokenizer_config.json and/or in the config.json files for safetensors models.\nYou can look for model_max_length in tokenizer_config.json and if the value doesn’t make sense, look for max_position_embeddings in config.json.\nThe values in these files may be higher than a power-of-2 size, so for example if the setting says 131072, that would be 128k model max length.\nAn alternative is looking for this information in LLM Explorer.\nWhat is the embedding size? The embedding size refers to the number of dimensions used to represent each token as a numerical vector. For example, if a model has an embedding size of 768, each token is represented as a 768-dimensional vector.\nLarger embedding sizes capture more nuanced meanings and relationships between words, improving model performance.\nThe larger the embedding size is, the larger the computational requirements are.\nWhat are weights and activations? Weights are trainable parameters that a model learns during training, defining the strength of connections between neurons across different layers. They control how much influence input data has on each neuron’s output. In LLMs, such as transformer models, weights capture complex language patterns like grammar, context, and semantics by determining relationships between words and distributing attention.\nActivations, on the other hand, are the outputs produced by neurons after applying an activation function (e.g., ReLU, Sigmoid, or Tanh) to the weighted sum of inputs. They introduce non-linearity, enabling the model to learn and represent complex patterns that linear functions cannot capture. In LLMs, activations help the model represent nuanced aspects of language, such as the relative importance of words or phrases in a given context.\nTogether, weights and activations work in tandem: weights adjust the flow and influence of information through the network during training, while activations determine the processed outputs at each neuron, ultimately allowing models to perform tasks like text generation, translation, and summarization effectively.\nSafetensors vs GGUF models Both are common formats for sharing LLM models, we can think of safetensors as the raw files, more convenient for fine-tuning and tinkering, while the gguf can be seen as a binary format more convenient for sharing and consume.\nRequired vRAM for LLM Models A basic formula to estimate vRAM usage is:\nNumber of parameters x (Precision / 8 ) x 1.2\nFor example for a 2B parameter using FP32\nThis calculation assumes a maximum sequence length of 512, a batch size of 8, and the use of a single GPU.\n2000000000 x (32/8) x 1.2 / 1024 / 1024 / 1024 = 8.9\nSome models also publish the amount of memory they take, for example Mistral publishes sizes here.\nThese estimations are for inference, training takes more memory.\nThere are also some tools to know if you can run a given model in your hardware:\nhttps://huggingface.co/spaces/Vokturz/can-it-run-llm https://vram.asmirnov.xyz/ https://rahulschand.github.io/gpu_poor LLM Models Naming There is no naming convention, but the more common way of naming models is like this:\nFor example given the name codellama-70b-python.Q4_K_S.gguf we can tell:\n70B –\u003e 70 billion parameters. Q4 –\u003e Quantization level. The lower the number, the lower the memory required (and also the precision). _0|_1|_K –\u003e Rounding method used for the weights. _S|_M|_L –\u003e Size category from small to large. The lower the letter, the lower the memory required (and also the precision). What does it mean model names like 8x7B It means the model is a mixture of experts of 8 models. Each model with 7B parameters.\nPadding in LLM Models In the context of LLMs, padding refers to the practice of adding dummy tokens (usually a special token) to sequences to make them all the same length. Padding is used in situations where batches of text sequences need to be processed together.\nFor example:\nGiven sentences of varying lengths like [\"Hello\", \"How are you?\", \"Good morning\"], they might be padded to a common length (e.g., 3 tokens) as: [\"Hello\", \"\", \"\"] [\"How\", \"are\", \"you?\"] [\"Good\", \"morning\", \"\"] Padding helps with:\nBatch Processing: Modern LLMs are highly parallelized and operate more efficiently when sequences are grouped and processed together in batches. Padding ensures all sequences in a batch have the same length. Fixed-Length Input: Many models expect fixed-length input tensors (e.g., max_length), so padding helps conform variable-length data to this requirement. Efficient GPU Utilization: Padding aligns data to the tensor shapes required by hardware accelerators (like GPUs/TPUs) to optimize memory and compute usage. What Are Padding-Free LLM Models? Models designed to eliminate the need for padding by processing sequences of varying lengths more dynamically. These models do not require all input sequences to be the same length and instead handle variable-length data directly.\nPadding free helps with:\nEfficiency: Avoiding padding minimizes redundant computations, leading to faster processing. Memory Usage: Padding-free models save memory by not storing unnecessary padding tokens. Scalability: These models are more adaptable to varying sequence lengths, especially for real-time or online processing tasks. Prompts Prompts are the text inputs provided to a large language model (LLM) that guide its responses. They can be simple questions, detailed instructions, or context-setting narratives. The design and clarity of a prompt directly influence the quality and relevance of the generated output. In essence, prompts tell the model what to do, framing the task and setting the context for the answer.\nSystem Prompt The system prompt is an instruction provided by the system to guide the behavior of a language model. It sets the context, defines roles, and establishes rules for how the model should respond during an interaction. This prompt is usually hidden from the end user but is crucial in shaping the overall output and ensuring consistency in the model’s behavior.\nWhen running LLMs in your own infrastructure you can define your own system prompts. While this is often hidden from the end user, there are models such as Claude, publishing their system prompt.\nChain of Thought / Reasoning Models New reasoning models use chain-of-thought techniques, prompting the model to generate intermediate reasoning steps before providing a final answer. Unlike regular models that generate outputs in one go, these models break down the problem into smaller, logical steps. This approach improves accuracy and transparency, especially for complex tasks requiring multi-step reasoning and logical deduction.\nNo CoT vs Zero-shot CoT vs Manual CoT No CoT In this approach, the model is given the question without any guidance on reasoning or intermediate steps. The model is expected to generate an answer directly without an explicit chain of thought.\nZero-shot CoT Here, a simple prompt like “Think step by step” is added to the question. This minimal instruction encourages the model to generate a chain of thought implicitly, but without detailed guidance on how to structure its reasoning.\nThis is also referred as Re-Act (Reasoning Act) approach.\nPrompting the model to think “step by step” encourages the decoding process toward next tokens that generate a plan, rather than a final solution.\nManual CoT This method involves providing detailed, step-by-step instructions or examples that illustrate the desired reasoning process. By showing how to break down and solve similar problems, manual CoT guides the model explicitly on structuring its chain of thought to arrive at an answer.\nLLM Agents LLM agents are programs that combine one or more large language models with various tools—such as APIs, databases, or knowledge sources—to analyze inputs, reason through tasks, and execute actions or automations autonomously. Their primary use case is to streamline and automate complex workflows by integrating natural language understanding with practical tool usage.\nWhen deploying these agents, it’s crucial to implement robust guardrails. These may include action constraints, monitoring mechanisms, fail-safes, and ethical guidelines to prevent unintended behavior and ensure safe, reliable operation.\nWhen building agents there are no-code and pro-code alternatives, for example:\nNo-code: https://n8n.io/ / https://github.com/n8n-io/n8n\nPro-code: https://www.langchain.com/langgraph / https://github.com/langchain-ai/langgraph\nHugging Face course on AI Agents: https://huggingface.co/learn/agents-course/unit0/introduction\nExample use cases An agent that monitors system logs for errors and tries to fix stuff before it breaks. It analyzes the content of the log, checks if it has a solution for that error in its knowledge base and runs the tools for fixing it. If it doesn’t know how to fix it, analyzes the log and transforms it to natural language, after that it opens a support case and routes it to the proper team.\nModel Context Protocol (MCP) The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.\nLearn more in the Anthropic announcement, and official docs. And check available servers upstream.\nLLM Training and Fine-Tuning Concepts This section describes general concepts around LLM Training and Fine-Tuning.\nSynthetic Data Generation (SDG) We use SDG for generating training data using what is called a teacher model. The teacher model generates further training data based on the human-crafted training data.\nIn InstructLab there is a SDG 1.5 Pipeline that adds a Critic model that filters, analyzes, critiques, plans, revises and judges the a first draft of generated data and generates the final training data set.\nYou can read more on how InstructLab does SDG here.\nOverfitting and Underfitting We say a model is overfitted when it becomes too complex and starts to memorize the training data instead of learning the underlying patters. It will perform extremely well on the training data but will struggle with unseen data. On the other hand a model is underfitted when it lacks the capacity to graps intricate patterns. It will poorly perform with the training data as well as with unseen data.\nRequired number of epochs Epochs is the number of runs that are repeated on the same data set. Training with high epochs usually end up with model overfitting.\nThere is no magic number and each model will require different number of epochs, you can use benchmarks to see how the model performs. People tend to say 3 to 6 epochs is enough for fine tuning. For example, instruct-gpt from OpenAI was trained (not fine-tuned) on 16 epochs.\nEffective Batch Size Refers to the total number of training examples processed in one update step of the model’s parameters. It takes into account both the batch size per device (or per GPU/TPU) and the gradient accumulation across multiple forward and backward passes.\n","wordCount":"3336","inLanguage":"en","datePublished":"2025-03-25T00:00:00Z","dateModified":"2025-03-25T00:00:00Z","author":{"@type":"Person","name":"Mario"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://linuxera.org/introduction-to-llm-concepts/"},"publisher":{"@type":"Organization","name":"Linuxera","logo":{"@type":"ImageObject","url":"https://linuxera.org/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://linuxera.org/ accesskey=h title="Linuxera (Alt + H)">Linuxera</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://linuxera.org/ title="🏠 Home"><span>🏠 Home</span></a></li><li><a href=https://linuxera.org/archives/ title="🗄️ Archive"><span>🗄️ Archive</span></a></li><li><a href=https://linuxera.org/search/ title="🔎 Search"><span>🔎 Search</span></a></li><li><a href=https://linuxera.org/tags/ title="🏷️ Tags"><span>🏷️ Tags</span></a></li><li><a href=https://linuxera.org/presentations/ title="🎴 Presentations"><span>🎴 Presentations</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://linuxera.org/>Home</a>&nbsp;»&nbsp;<a href=https://linuxera.org/posts/>Posts</a></div><h1 class=post-title>Introduction to LLM concepts</h1><div class=post-meta><span title='2025-03-25 00:00:00 +0000 UTC'>Published on March 25, 2025</span>&nbsp;·&nbsp;<span title='2025-03-25 00:00:00 +0000 UTC'>Last updated on March 25, 2025</span>&nbsp;·&nbsp;16 min&nbsp;·&nbsp;Mario</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction-to-llm-concepts aria-label="Introduction to LLM concepts">Introduction to LLM concepts</a><ul><li><a href=#general-llm-concepts-and-questions aria-label="General LLM Concepts and Questions">General LLM Concepts and Questions</a><ul><li><a href=#whats-a-large-language-model-llm aria-label="What&amp;rsquo;s a Large Language Model (LLM)?">What&rsquo;s a Large Language Model (LLM)?</a></li><li><a href=#whats-the-difference-between-a-base-and-an-instruct-model aria-label="What&amp;rsquo;s the difference between a Base and an Instruct model?">What&rsquo;s the difference between a <code>Base</code> and an <code>Instruct</code> model?</a></li><li><a href=#what-are-parameters-in-a-llm-model aria-label="What are Parameters in a LLM Model?">What are Parameters in a LLM Model?</a></li><li><a href=#teacher-vs-critic-vs-judge-model aria-label="Teacher vs Critic vs Judge Model">Teacher vs Critic vs Judge Model</a><ul><li><a href=#teacher-model aria-label="Teacher Model">Teacher Model</a></li><li><a href=#critic-model aria-label="Critic Model">Critic Model</a></li><li><a href=#judge-model aria-label="Judge Model">Judge Model</a><ul><li><a href=#llm-as-a-judge aria-label="LLM as a Judge">LLM as a Judge</a></li></ul></li><li><a href=#critic-and-judge-models-collaboration aria-label="Critic and Judge models collaboration">Critic and Judge models collaboration</a></li></ul></li><li><a href=#rag-vs-fine-tuning aria-label="RAG vs Fine Tuning">RAG vs Fine Tuning</a></li><li><a href=#evaluating-models aria-label="Evaluating Models">Evaluating Models</a></li><li><a href=#what-is-quantization aria-label="What is quantization?">What is quantization?</a></li><li><a href=#what-is-a-distilled-model aria-label="What is a distilled model?">What is a distilled model?</a><ul><li><a href=#how-knowledge-distillation-works aria-label="How Knowledge Distillation Works">How Knowledge Distillation Works</a></li></ul></li><li><a href=#what-are-tokens-in-the-context-of-llms aria-label="What are tokens in the context of LLMs?">What are tokens in the context of LLMs?</a></li><li><a href=#what-is-the-context-windowcontext-lengthmodel-max-length aria-label="What is the context window/context length/model max length?">What is the context window/context length/model max length?</a></li><li><a href=#what-is-the-embedding-size aria-label="What is the embedding size?">What is the embedding size?</a></li><li><a href=#what-are-weights-and-activations aria-label="What are weights and activations?">What are weights and activations?</a></li><li><a href=#safetensors-vs-gguf-models aria-label="Safetensors vs GGUF models">Safetensors vs GGUF models</a></li><li><a href=#required-vram-for-llm-models aria-label="Required vRAM for LLM Models">Required vRAM for LLM Models</a></li><li><a href=#llm-models-naming aria-label="LLM Models Naming">LLM Models Naming</a><ul><li><a href=#what-does-it-mean-model-names-like-8x7b aria-label="What does it mean model names like 8x7B">What does it mean model names like 8x7B</a></li></ul></li><li><a href=#padding-in-llm-models aria-label="Padding in LLM Models">Padding in LLM Models</a><ul><li><a href=#what-are-padding-free-llm-models aria-label="What Are Padding-Free LLM Models?">What Are Padding-Free LLM Models?</a></li></ul></li><li><a href=#prompts aria-label=Prompts>Prompts</a><ul><li><a href=#system-prompt aria-label="System Prompt">System Prompt</a></li><li><a href=#chain-of-thought--reasoning-models aria-label="Chain of Thought / Reasoning Models">Chain of Thought / Reasoning Models</a><ul><li><a href=#no-cot-vs-zero-shot-cot-vs-manual-cot aria-label="No CoT vs Zero-shot CoT vs Manual CoT">No CoT vs Zero-shot CoT vs Manual CoT</a><ul><li><a href=#no-cot aria-label="No CoT">No CoT</a></li><li><a href=#zero-shot-cot aria-label="Zero-shot CoT">Zero-shot CoT</a></li><li><a href=#manual-cot aria-label="Manual CoT">Manual CoT</a></li></ul></li></ul></li></ul></li><li><a href=#llm-agents aria-label="LLM Agents">LLM Agents</a><ul><li><a href=#example-use-cases aria-label="Example use cases">Example use cases</a></li></ul></li><li><a href=#model-context-protocol-mcp aria-label="Model Context Protocol (MCP)">Model Context Protocol (MCP)</a></li></ul></li><li><a href=#llm-training-and-fine-tuning-concepts aria-label="LLM Training and Fine-Tuning Concepts">LLM Training and Fine-Tuning Concepts</a><ul><li><a href=#synthetic-data-generation-sdg aria-label="Synthetic Data Generation (SDG)">Synthetic Data Generation (SDG)</a></li><li><a href=#overfitting-and-underfitting aria-label="Overfitting and Underfitting">Overfitting and Underfitting</a></li><li><a href=#required-number-of-epochs aria-label="Required number of epochs">Required number of epochs</a></li><li><a href=#effective-batch-size aria-label="Effective Batch Size">Effective Batch Size</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=introduction-to-llm-concepts>Introduction to LLM concepts<a hidden class=anchor aria-hidden=true href=#introduction-to-llm-concepts>#</a></h1><p>In this post, I&rsquo;ll cover various LLM concepts and the questions I asked myself while diving deep into the world of LLMs. I expect this post to be updated as I continue to learn more things around LLMs.</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>This post is the result of my exploratory work on LLMs. While I’ve made a strong effort to ensure the information is accurate, I’m far from an expert on the topic, and some details may not be entirely correct. If you notice anything missing or inaccurate, please leave a comment!</p></div><h2 id=general-llm-concepts-and-questions>General LLM Concepts and Questions<a hidden class=anchor aria-hidden=true href=#general-llm-concepts-and-questions>#</a></h2><p>This section describes general concepts and questions around Large Language Models.</p><h3 id=whats-a-large-language-model-llm>What&rsquo;s a Large Language Model (LLM)?<a hidden class=anchor aria-hidden=true href=#whats-a-large-language-model-llm>#</a></h3><p>A Large Language Model (LLM) is an AI system trained on vast amounts of text data to understand and generate human-like language. Using deep learning, specifically transformer architectures, LLMs can answer questions, summarize text, generate content, and more. They predict the most likely next word based on context.</p><h3 id=whats-the-difference-between-a-base-and-an-instruct-model>What&rsquo;s the difference between a <code>Base</code> and an <code>Instruct</code> model?<a hidden class=anchor aria-hidden=true href=#whats-the-difference-between-a-base-and-an-instruct-model>#</a></h3><p>A base model predicts the next word, an instruct model has been fine-tuned to predict the next word in a conversation between a user and a helpful assistant and specifically to follow instructions. There are other types like chat models that are fine-tuned to follow a conversation in a chat-like mode with a given format, etc.</p><p>You could make a base model follow instructions but that requires configuring the model via prompts to do so.</p><p>Usually, base models are used for fine-tune your model based on the base one.</p><p>We can make a base model behave like an instruct model by formatting our prompts in a consistent way that the model can understand. That&rsquo;s done via chat templates. If you want to learn more about chat templates, <a href=https://huggingface.co/docs/transformers/en/chat_templating>read this</a>. You can see the chat template for an instruct model in the <code>tokenizer_config.json</code> (<a href=https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/blob/main/tokenizer_config.json#L146>example</a>).</p><table><thead><tr><th><strong>Aspect</strong></th><th><strong>Base Models</strong></th><th><strong>Instruct Models</strong></th></tr></thead><tbody><tr><td><strong>Training Objective</strong></td><td>General language modeling.</td><td>Fine-tuned for instruction following.</td></tr><tr><td><strong>Focus</strong></td><td>Broad and generic text understanding.</td><td>Task-specific, user-centric interactions.</td></tr><tr><td><strong>Performance</strong></td><td>Literal, unrefined responses.</td><td>Context-aware, tailored responses.</td></tr><tr><td><strong>Safety and Alignment</strong></td><td>Lower alignment with human values.</td><td>Higher alignment due to RLHF.</td></tr><tr><td><strong>Examples of Usage</strong></td><td>Pre-training for fine-tuning tasks.</td><td>User-facing applications like chatbots.</td></tr></tbody></table><p>RLHF -> Reinforcement Learning from Human Feedback</p><ul><li><strong>Base Models</strong>: Useful as a starting point for further fine-tuning for specific tasks.</li><li><strong>Instruct Models</strong>: Ideal for deployment in interactive environments where users provide natural-language instructions.</li></ul><p>You may want to use base when you train the model yourself or you want to do purely text/code continuation.</p><h3 id=what-are-parameters-in-a-llm-model>What are Parameters in a LLM Model?<a hidden class=anchor aria-hidden=true href=#what-are-parameters-in-a-llm-model>#</a></h3><p>In a Large Language Model (LLM), parameters refer to the numerical values within the model that determine how it processes and generates text. These parameters are the weights and biases of the neural network, which are adjusted during the training process.</p><h3 id=teacher-vs-critic-vs-judge-model>Teacher vs Critic vs Judge Model<a hidden class=anchor aria-hidden=true href=#teacher-vs-critic-vs-judge-model>#</a></h3><h4 id=teacher-model>Teacher Model<a hidden class=anchor aria-hidden=true href=#teacher-model>#</a></h4><p>A teacher model in many cases is a large, high-performing model whose outputs are used to train a smaller, more efficient student model. This process (called knowledge distillation) typically involves having the teacher model generate &ldquo;soft labels&rdquo; or guidance on either the same dataset or additional unlabeled data, which helps the student learn more nuanced patterns. A teacher model can also be used to generate synthetic data from a curated dataset (i.e: one carefully selected or labeled by humans).</p><h4 id=critic-model>Critic Model<a hidden class=anchor aria-hidden=true href=#critic-model>#</a></h4><p>A critic model is designed to evaluate or critique the output of another model. Its role is typically focused on assessing the quality, relevance, accuracy, or coherence of generated outputs and identifying areas of improvement.</p><p>Critic models are often used during training to evaluate outputs from the teacher model.</p><h4 id=judge-model>Judge Model<a hidden class=anchor aria-hidden=true href=#judge-model>#</a></h4><p>A judge model is designed to evaluate and rank the performance of multiple outputs. It acts more as an arbiter than a feedback provider, making decisions about which output is &ldquo;better&rdquo; according to predefined criteria. Depending on the setup, its role can overlap with or complement that of a critic model.</p><p>Judge models can sometimes be trained on human preference data, which helps them simulate human judgement more closely.</p><h5 id=llm-as-a-judge>LLM as a Judge<a hidden class=anchor aria-hidden=true href=#llm-as-a-judge>#</a></h5><p>Using LLMs as judges is a powerful solution to asses outputs in a human way, without requiring costly human time. The idea is simple: ask an LLM to do the grading for you.</p><p>You can read more about this in <a href=https://huggingface.co/learn/cookbook/en/llm_judge>this hugging face cookbook</a>.</p><h4 id=critic-and-judge-models-collaboration>Critic and Judge models collaboration<a hidden class=anchor aria-hidden=true href=#critic-and-judge-models-collaboration>#</a></h4><p>In advanced LLM setups, critic and judge models often work together:</p><ol><li><strong>Critic</strong> improves candidate outputs by iterating over feedback.</li><li><strong>Judge</strong> evaluates the improved outputs to decide the final response.</li></ol><p>This dynamic can be crucial in applications where <strong>fine-tuned outputs</strong>, such as accurate medical advice, high-quality creative writing, or nuanced technical analysis, are required.</p><h3 id=rag-vs-fine-tuning>RAG vs Fine Tuning<a hidden class=anchor aria-hidden=true href=#rag-vs-fine-tuning>#</a></h3><p>The difference between RAG and fine-tuning is that RAG augments a natural language processing (NLP) model by connecting it to an organization’s proprietary database, while fine-tuning optimizes deep learning models for domain-specific tasks. RAG and fine-tuning have the same intended outcome: enhancing a model’s performance to maximize value for the enterprise that uses it.</p><p>RAG uses an organization’s internal data to augment prompt engineering, while fine-tuning retrains a model on a focused set of external data to improve performance.</p><p>When we talk about fine-tuning most of the time we refer to parameter-efficient fine-tuning (PEFT) which focuses on tuning only the relevant parameters to make the mode more effective in a certain domain, this helps keeping training costs low.</p><table><thead><tr><th><strong>Aspect</strong></th><th><strong>RAG (Retrieval-Augmented Generation)</strong></th><th><strong>Fine-Tuning</strong></th></tr></thead><tbody><tr><td><strong>Knowledge Source</strong></td><td>External, dynamically retrieved.</td><td>Internal, baked into the model’s weights.</td></tr><tr><td><strong>Adaptation Method</strong></td><td>Combines retrieval and generation dynamically.</td><td>Updates model weights with new training data.</td></tr><tr><td><strong>Scalability</strong></td><td>Scales easily with growing knowledge bases.</td><td>Requires retraining for new domains.</td></tr><tr><td><strong>Flexibility</strong></td><td>Can adapt to changing information in real-time.</td><td>Static after training; requires retraining for updates.</td></tr><tr><td><strong>Cost and Time</strong></td><td>Requires infrastructure for retrieval; no retraining cost.</td><td>Computationally expensive and time-consuming.</td></tr><tr><td><strong>Performance</strong></td><td>Dependent on the quality of the retrieval system.</td><td>Highly accurate for specific tasks if trained well.</td></tr><tr><td><strong>Use Cases</strong></td><td>Open-domain, dynamic tasks needing current data.</td><td>Fixed-domain, static tasks requiring precision.</td></tr></tbody></table><h3 id=evaluating-models>Evaluating Models<a hidden class=anchor aria-hidden=true href=#evaluating-models>#</a></h3><p>In order to evaluate a LLM performance there are different benchmarks that can be used, there are several benchmarks available, two of the most common ones are:</p><ul><li>MMLU: Massive Multitask Language Understanding, tests a model’s proficiency across a wide range of subjects, ensuring broad knowledge coverage.</li><li>MATH: Evaluates mathematical problem-solving abilities, ensuring that models can handle numerical and logical challenges</li></ul><h3 id=what-is-quantization>What is quantization?<a hidden class=anchor aria-hidden=true href=#what-is-quantization>#</a></h3><p>Quantization is a compression technique in order to reduce model&rsquo;s size. It maps high precision values to lower precision values for the model&rsquo;s weights and activations. This compression has an impact on the model accuracy, in some cases comparable results can be achieve with lower precision.</p><p>The way it works is by representing a range of full precision float32 numbers (FP32) in half precision float16 numbers (FP16), and sometimes even integer numbers like int 4 bits (INT4). Usually quantization is done from FP32 to INT8.</p><p>For example, if we have a 2B parameter model with half precision FP16 where each parameter takes 16 bits (2bytes), it means the model will weight ~4GB:</p><p><code>(2000000000*2)/1024/1024/1024 = 3.7</code></p><p><img loading=lazy src=./quantization.png alt="Quantization Example"></p><p>More info available <a href=https://symbl.ai/developers/blog/a-guide-to-quantization-in-llms/>here</a>.</p><h3 id=what-is-a-distilled-model>What is a distilled model?<a hidden class=anchor aria-hidden=true href=#what-is-a-distilled-model>#</a></h3><p>A distilled model is a smaller, more efficient version of a larger model, trained using a technique called knowledge distillation. The goal is to retain as much of the original model&rsquo;s performance as possible while reducing its size and computational requirements.</p><h4 id=how-knowledge-distillation-works>How Knowledge Distillation Works<a hidden class=anchor aria-hidden=true href=#how-knowledge-distillation-works>#</a></h4><p>A large model (teacher) generates outputs on a dataset.</p><p>A smaller model (student) is trained to mimic the outputs of the teacher model, often learning from both:</p><ul><li>The original dataset.</li><li>The teacher model’s predictions, probabilities, or reasoning steps.</li></ul><p>The student model achieves similar accuracy with fewer parameters, making it faster and cheaper to use.</p><p>This approach is common in LLMs to create lighter, optimized versions for deployment in real-world applications with limited computing power.</p><h3 id=what-are-tokens-in-the-context-of-llms>What are tokens in the context of LLMs?<a hidden class=anchor aria-hidden=true href=#what-are-tokens-in-the-context-of-llms>#</a></h3><p>We can think of a token as if it was a word, but a token is not always a whole word. For example, English has around ~600k words, while an LLM might have a vocabulary of around ~32k tokens (i.e: llama2).</p><p>Tokenization works on sub-words units that can be combined. For example, the tokens &ldquo;play&rdquo; and &ldquo;ful&rdquo; can be combined to form &ldquo;playful&rdquo;. Or we can append &ldquo;ed&rdquo; to form &ldquo;played&rdquo;.</p><p>You can see a tokenizer in action in this <a href=https://huggingface.co/spaces/Xenova/the-tokenizer-playground>hugging face tokenizer playground</a>.</p><p>LLMs use special tokens specific to the models to open and close the structured components of its generation. The most important special token is the End of sequence token (EOS). For example SmolLM2 from hugging face uses the following EOS token <code>&lt;|im_end|></code>. You can see the special tokens for models in the <code>tokenizer_config.json</code>.</p><p><a href=https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/blob/main/tokenizer_config.json>Example for SomlLM2</a>.</p><p>LLMs will predict continue predicting words until the next word with the highest score is the EOS token.</p><p>You can see how the decoding process works <a href=https://huggingface.co/spaces/agents-course/decoding_visualizer>here</a>. And a more advanced strategy <a href=https://huggingface.co/spaces/agents-course/beam_search_visualizer>here</a>.</p><h3 id=what-is-the-context-windowcontext-lengthmodel-max-length>What is the context window/context length/model max length?<a hidden class=anchor aria-hidden=true href=#what-is-the-context-windowcontext-lengthmodel-max-length>#</a></h3><p>It&rsquo;s the maximum number of tokens the model can process in a single input sequence (including both the prompt and any output generated during a single pass).</p><p>Let&rsquo;s say we have a context window of 2048, if we have one input that takes 1000, we have 1048 left for the model to reply.</p><p>Remember that LLMs doesn&rsquo;t have memory, if you&rsquo;re in a chat and the model is remembering the conversation is because on each iteration the previous context is being sent as well.</p><p>Keep in mind that the greater the context length is, the more memory it takes to run the model.</p><p>The maximum context window that a model can handle is often found in the <code>tokenizer_config.json</code> and/or in the <code>config.json</code> files for safetensors models.</p><p>You can look for <code>model_max_length</code> in <code>tokenizer_config.json</code> and if the value doesn&rsquo;t make sense, look for <code>max_position_embeddings</code> in <code>config.json</code>.</p><p>The values in these files may be higher than a power-of-2 size, so for example if the setting says <code>131072</code>, that would be <code>128k</code> model max length.</p><p>An alternative is looking for this information in <a href=https://llm.extractum.io/>LLM Explorer</a>.</p><h3 id=what-is-the-embedding-size>What is the embedding size?<a hidden class=anchor aria-hidden=true href=#what-is-the-embedding-size>#</a></h3><p>The embedding size refers to the number of dimensions used to represent each token as a numerical vector. For example, if a model has an embedding size of 768, each token is represented as a 768-dimensional vector.</p><p>Larger embedding sizes capture more nuanced meanings and relationships between words, improving model performance.</p><p>The larger the embedding size is, the larger the computational requirements are.</p><h3 id=what-are-weights-and-activations>What are weights and activations?<a hidden class=anchor aria-hidden=true href=#what-are-weights-and-activations>#</a></h3><p>Weights are trainable parameters that a model learns during training, defining the strength of connections between neurons across different layers. They control how much influence input data has on each neuron&rsquo;s output. In LLMs, such as transformer models, weights capture complex language patterns like grammar, context, and semantics by determining relationships between words and distributing attention.</p><p>Activations, on the other hand, are the outputs produced by neurons after applying an activation function (e.g., ReLU, Sigmoid, or Tanh) to the weighted sum of inputs. They introduce non-linearity, enabling the model to learn and represent complex patterns that linear functions cannot capture. In LLMs, activations help the model represent nuanced aspects of language, such as the relative importance of words or phrases in a given context.</p><p>Together, weights and activations work in tandem: weights adjust the flow and influence of information through the network during training, while activations determine the processed outputs at each neuron, ultimately allowing models to perform tasks like text generation, translation, and summarization effectively.</p><h3 id=safetensors-vs-gguf-models>Safetensors vs GGUF models<a hidden class=anchor aria-hidden=true href=#safetensors-vs-gguf-models>#</a></h3><p>Both are common formats for sharing LLM models, we can think of safetensors as the raw files, more convenient for fine-tuning and tinkering, while the gguf can be seen as a binary format more convenient for sharing and consume.</p><h3 id=required-vram-for-llm-models>Required vRAM for LLM Models<a hidden class=anchor aria-hidden=true href=#required-vram-for-llm-models>#</a></h3><p>A basic formula to estimate vRAM usage is:</p><p>Number of parameters x (Precision / 8 ) x 1.2</p><p>For example for a 2B parameter using FP32</p><blockquote><p>This calculation assumes a maximum sequence length of 512, a batch size of 8, and the use of a single GPU.</p></blockquote><p><code>2000000000 x (32/8) x 1.2 / 1024 / 1024 / 1024 = 8.9</code></p><p>Some models also publish the amount of memory they take, for example Mistral publishes sizes <a href=https://docs.mistral.ai/getting-started/models/weights/#sizes>here</a>.</p><p>These estimations are for inference, training takes more memory.</p><p>There are also some tools to know if you can run a given model in your hardware:</p><ul><li><a href=https://huggingface.co/spaces/Vokturz/can-it-run-llm>https://huggingface.co/spaces/Vokturz/can-it-run-llm</a></li><li><a href=https://vram.asmirnov.xyz/>https://vram.asmirnov.xyz/</a></li><li><a href=https://rahulschand.github.io/gpu_poor>https://rahulschand.github.io/gpu_poor</a></li></ul><h3 id=llm-models-naming>LLM Models Naming<a hidden class=anchor aria-hidden=true href=#llm-models-naming>#</a></h3><p>There is no naming convention, but the more common way of naming models is like this:</p><p><img loading=lazy src=./model-names.png alt="Model Names Examples"></p><p>For example given the name <code>codellama-70b-python.Q4_K_S.gguf</code> we can tell:</p><ul><li><code>70B</code> &ndash;> 70 billion parameters.</li><li><code>Q4</code> &ndash;> Quantization level. The lower the number, the lower the memory required (and also the precision).</li><li><code>_0|_1|_K</code> &ndash;> Rounding method used for the weights.</li><li><code>_S|_M|_L</code> &ndash;> Size category from small to large. The lower the letter, the lower the memory required (and also the precision).</li></ul><h4 id=what-does-it-mean-model-names-like-8x7b>What does it mean model names like 8x7B<a hidden class=anchor aria-hidden=true href=#what-does-it-mean-model-names-like-8x7b>#</a></h4><p>It means the model is a mixture of experts of 8 models. Each model with 7B parameters.</p><h3 id=padding-in-llm-models>Padding in LLM Models<a hidden class=anchor aria-hidden=true href=#padding-in-llm-models>#</a></h3><p>In the context of LLMs, padding refers to the practice of adding dummy tokens (usually a special <code>&lt;PAD></code> token) to sequences to make them all the same length. Padding is used in situations where batches of text sequences need to be processed together.</p><p>For example:</p><ul><li>Given sentences of varying lengths like <code>["Hello", "How are you?", "Good morning"]</code>, they might be padded to a common length (e.g., 3 tokens) as:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>[</span><span class=s2>&#34;Hello&#34;</span><span class=p>,</span> <span class=s2>&#34;&lt;PAD&gt;&#34;</span><span class=p>,</span> <span class=s2>&#34;&lt;PAD&gt;&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=s2>&#34;How&#34;</span><span class=p>,</span> <span class=s2>&#34;are&#34;</span><span class=p>,</span> <span class=s2>&#34;you?&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=s2>&#34;Good&#34;</span><span class=p>,</span> <span class=s2>&#34;morning&#34;</span><span class=p>,</span> <span class=s2>&#34;&lt;PAD&gt;&#34;</span><span class=p>]</span>
</span></span></code></pre></div><p>Padding helps with:</p><ol><li><strong>Batch Processing</strong>: Modern LLMs are highly parallelized and operate more efficiently when sequences are grouped and processed together in batches. Padding ensures all sequences in a batch have the same length.</li><li><strong>Fixed-Length Input</strong>: Many models expect fixed-length input tensors (e.g., <code>max_length</code>), so padding helps conform variable-length data to this requirement.</li><li><strong>Efficient GPU Utilization</strong>: Padding aligns data to the tensor shapes required by hardware accelerators (like GPUs/TPUs) to optimize memory and compute usage.</li></ol><h4 id=what-are-padding-free-llm-models>What Are Padding-Free LLM Models?<a hidden class=anchor aria-hidden=true href=#what-are-padding-free-llm-models>#</a></h4><p>Models designed to eliminate the need for padding by processing sequences of varying lengths more dynamically. These models do not require all input sequences to be the same length and instead handle variable-length data directly.</p><p>Padding free helps with:</p><ul><li><strong>Efficiency</strong>: Avoiding padding minimizes redundant computations, leading to faster processing.</li><li><strong>Memory Usage</strong>: Padding-free models save memory by not storing unnecessary padding tokens.</li><li><strong>Scalability</strong>: These models are more adaptable to varying sequence lengths, especially for real-time or online processing tasks.</li></ul><h3 id=prompts>Prompts<a hidden class=anchor aria-hidden=true href=#prompts>#</a></h3><p>Prompts are the text inputs provided to a large language model (LLM) that guide its responses. They can be simple questions, detailed instructions, or context-setting narratives. The design and clarity of a prompt directly influence the quality and relevance of the generated output. In essence, prompts tell the model what to do, framing the task and setting the context for the answer.</p><h4 id=system-prompt>System Prompt<a hidden class=anchor aria-hidden=true href=#system-prompt>#</a></h4><p>The system prompt is an instruction provided by the system to guide the behavior of a language model. It sets the context, defines roles, and establishes rules for how the model should respond during an interaction. This prompt is usually hidden from the end user but is crucial in shaping the overall output and ensuring consistency in the model&rsquo;s behavior.</p><p>When running LLMs in your own infrastructure you can define your own system prompts. While this is often hidden from the end user, there are models such as Claude, <a href=https://docs.anthropic.com/en/release-notes/system-prompts>publishing their system prompt</a>.</p><h4 id=chain-of-thought--reasoning-models>Chain of Thought / Reasoning Models<a hidden class=anchor aria-hidden=true href=#chain-of-thought--reasoning-models>#</a></h4><p>New reasoning models use chain-of-thought techniques, prompting the model to generate intermediate reasoning steps before providing a final answer. Unlike regular models that generate outputs in one go, these models break down the problem into smaller, logical steps. This approach improves accuracy and transparency, especially for complex tasks requiring multi-step reasoning and logical deduction.</p><h5 id=no-cot-vs-zero-shot-cot-vs-manual-cot>No CoT vs Zero-shot CoT vs Manual CoT<a hidden class=anchor aria-hidden=true href=#no-cot-vs-zero-shot-cot-vs-manual-cot>#</a></h5><h6 id=no-cot>No CoT<a hidden class=anchor aria-hidden=true href=#no-cot>#</a></h6><p>In this approach, the model is given the question without any guidance on reasoning or intermediate steps. The model is expected to generate an answer directly without an explicit chain of thought.</p><h6 id=zero-shot-cot>Zero-shot CoT<a hidden class=anchor aria-hidden=true href=#zero-shot-cot>#</a></h6><p>Here, a simple prompt like &ldquo;Think step by step&rdquo; is added to the question. This minimal instruction encourages the model to generate a chain of thought implicitly, but without detailed guidance on how to structure its reasoning.</p><p>This is also referred as Re-Act (Reasoning Act) approach.</p><p>Prompting the model to think &ldquo;step by step&rdquo; encourages the decoding process toward next tokens that generate a plan, rather than a final solution.</p><h6 id=manual-cot>Manual CoT<a hidden class=anchor aria-hidden=true href=#manual-cot>#</a></h6><p>This method involves providing detailed, step-by-step instructions or examples that illustrate the desired reasoning process. By showing how to break down and solve similar problems, manual CoT guides the model explicitly on structuring its chain of thought to arrive at an answer.</p><h3 id=llm-agents>LLM Agents<a hidden class=anchor aria-hidden=true href=#llm-agents>#</a></h3><p>LLM agents are programs that combine one or more large language models with various tools—such as APIs, databases, or knowledge sources—to analyze inputs, reason through tasks, and execute actions or automations autonomously. Their primary use case is to streamline and automate complex workflows by integrating natural language understanding with practical tool usage.</p><p>When deploying these agents, it’s crucial to implement robust guardrails. These may include action constraints, monitoring mechanisms, fail-safes, and ethical guidelines to prevent unintended behavior and ensure safe, reliable operation.</p><p>When building agents there are no-code and pro-code alternatives, for example:</p><p>No-code: <a href=https://n8n.io/>https://n8n.io/</a> / <a href=https://github.com/n8n-io/n8n>https://github.com/n8n-io/n8n</a></p><p>Pro-code: <a href=https://www.langchain.com/langgraph>https://www.langchain.com/langgraph</a> / <a href=https://github.com/langchain-ai/langgraph>https://github.com/langchain-ai/langgraph</a></p><p>Hugging Face course on AI Agents: <a href=https://huggingface.co/learn/agents-course/unit0/introduction>https://huggingface.co/learn/agents-course/unit0/introduction</a></p><h4 id=example-use-cases>Example use cases<a hidden class=anchor aria-hidden=true href=#example-use-cases>#</a></h4><p>An agent that monitors system logs for errors and tries to fix stuff before it breaks. It analyzes the content of the log, checks if it has a solution for that error in its knowledge base and runs the tools for fixing it. If it doesn&rsquo;t know how to fix it, analyzes the log and transforms it to natural language, after that it opens a support case and routes it to the proper team.</p><h3 id=model-context-protocol-mcp>Model Context Protocol (MCP)<a hidden class=anchor aria-hidden=true href=#model-context-protocol-mcp>#</a></h3><p>The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.</p><p>Learn more in the <a href=https://www.anthropic.com/news/model-context-protocol>Anthropic announcement</a>, and <a href=https://modelcontextprotocol.io/introduction>official docs</a>. And check <a href=https://github.com/modelcontextprotocol/servers>available servers upstream</a>.</p><h2 id=llm-training-and-fine-tuning-concepts>LLM Training and Fine-Tuning Concepts<a hidden class=anchor aria-hidden=true href=#llm-training-and-fine-tuning-concepts>#</a></h2><p>This section describes general concepts around LLM Training and Fine-Tuning.</p><h3 id=synthetic-data-generation-sdg>Synthetic Data Generation (SDG)<a hidden class=anchor aria-hidden=true href=#synthetic-data-generation-sdg>#</a></h3><p>We use SDG for generating training data using what is called a teacher model. The teacher model generates further training data based on the human-crafted training data.</p><p>In InstructLab there is a SDG 1.5 Pipeline that adds a Critic model that filters, analyzes, critiques, plans, revises and judges the a first draft of generated data and generates the final training data set.</p><p>You can read more on how InstructLab does SDG <a href=https://www.redhat.com/en/blog/how-instructlabs-synthetic-data-generation-enhances-llms>here</a>.</p><h3 id=overfitting-and-underfitting>Overfitting and Underfitting<a hidden class=anchor aria-hidden=true href=#overfitting-and-underfitting>#</a></h3><p>We say a model is overfitted when it becomes too complex and starts to memorize the training data instead of learning the underlying patters. It will perform extremely well on the training data but will struggle with unseen data. On the other hand a model is underfitted when it lacks the capacity to graps intricate patterns. It will poorly perform with the training data as well as with unseen data.</p><h3 id=required-number-of-epochs>Required number of epochs<a hidden class=anchor aria-hidden=true href=#required-number-of-epochs>#</a></h3><p>Epochs is the number of runs that are repeated on the same data set. Training with high epochs usually end up with model overfitting.</p><p>There is no magic number and each model will require different number of epochs, you can use benchmarks to see how the model performs. People tend to say 3 to 6 epochs is enough for fine tuning. For example, instruct-gpt from OpenAI <a href=https://x.com/abacaj/status/1748812961441886433>was trained (not fine-tuned) on 16 epochs</a>.</p><h3 id=effective-batch-size>Effective Batch Size<a hidden class=anchor aria-hidden=true href=#effective-batch-size>#</a></h3><p>Refers to the total number of training examples processed in one <strong>update step</strong> of the model&rsquo;s parameters. It takes into account both the <strong>batch size per device</strong> (or per GPU/TPU) and the <strong>gradient accumulation</strong> across multiple forward and backward passes.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://linuxera.org/tags/artificialintelligence/>artificialintelligence</a></li><li><a href=https://linuxera.org/tags/ai/>ai</a></li><li><a href=https://linuxera.org/tags/llm/>llm</a></li><li><a href=https://linuxera.org/tags/llms/>llms</a></li><li><a href=https://linuxera.org/tags/openai/>openai</a></li></ul><nav class=paginav><a class=prev href=https://linuxera.org/rag-beginners-guide/><span class=title>« Prev</span><br><span>A Beginner’s Guide to RAG: What I Wish Someone Told Me</span></a>
<a class=next href=https://linuxera.org/signing-verifying-container-images-with-cosign-own-pki/><span class=title>Next »</span><br><span>Signing and verifying container images with Cosign and your own PKI</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Introduction to LLM concepts on twitter" href="https://twitter.com/intent/tweet/?text=Introduction%20to%20LLM%20concepts&url=https%3a%2f%2flinuxera.org%2fintroduction-to-llm-concepts%2f&hashtags=artificialintelligence%2cai%2cllm%2cllms%2copenai"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introduction to LLM concepts on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flinuxera.org%2fintroduction-to-llm-concepts%2f&title=Introduction%20to%20LLM%20concepts&summary=Introduction%20to%20LLM%20concepts&source=https%3a%2f%2flinuxera.org%2fintroduction-to-llm-concepts%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></div></footer><div class=share-buttons><p>If this post has been helpful to you, consider <u><a target=_blank href=https://ko-fi.com/mvazce>supporting the work.</a></u></p></div><script src=https://utteranc.es/client.js repo=mvazquezc/mvazquezc.github.io issue-term=pathname label=blog-comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://linuxera.org/>Linuxera</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
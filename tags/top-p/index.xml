<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>top-p on Linuxera</title>
    <link>https://linuxera.org/tags/top-p/</link>
    <description>Recent content in top-p on Linuxera</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Nov 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://linuxera.org/tags/top-p/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Turning the Knobs of LLM Text Generation</title>
      <link>https://linuxera.org/turning-the-knobs-of-llm-text-generation/</link>
      <pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate>
      
      <guid>https://linuxera.org/turning-the-knobs-of-llm-text-generation/</guid>
      <description>Turning the Knobs of LLM Text Generation Ever wonder how much control you actually have over the text an LLM produces? In this post, we will look at three simple but powerful knobs you can tweak to push a model toward more deterministic output or toward something more creative.
We are talking about top_k, top_p and temperature. But before describing them, we need to understand the two main behaviors we can get from an LLM when it is sampling tokens:</description>
    </item>
    
  </channel>
</rss>

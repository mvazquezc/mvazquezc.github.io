<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CPU and Memory Management on Kubernetes with Cgroupsv2 | Linuxera</title><meta name=keywords content="kubernetes,openshift,cgroups,cgroupsv2"><meta name=description content="CPU and Memory Management on Kubernetes with Cgroupsv2 In this post I&rsquo;ll try to explain how CPU and Memory management works under the hood on Kubernetes. If you ever wondered what happens when you set requests and limits for your pods, keep reading!
Attention
This is the result of my exploratory work around cgroupsv2 and their application to Kubernetes. Even though I tried really hard to make sure the information in this post is accurate, I&rsquo;m far from being an expert on the topic and some information may not be 100% accurate."><meta name=author content="Mario"><link rel=canonical href=https://linuxera.org/cpu-memory-management-kubernetes-cgroupsv2/><link crossorigin=anonymous href=/assets/css/stylesheet.8cc7ef3cdd44c5188f9267864f378d8dd8892d583e0fd07b5e5321e496f1e4d1.css integrity="sha256-jMfvPN1ExRiPkmeGTzeNjdiJLVg+D9B7XlMh5Jbx5NE=" rel="preload stylesheet" as=style><link rel=icon href=https://linuxera.org/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://linuxera.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://linuxera.org/favicon-32x32.png><link rel=apple-touch-icon href=https://linuxera.org/apple-touch-icon.png><link rel=mask-icon href=https://linuxera.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script defer data-domain=linuxera.org src=https://stats.linuxera.org/js/script.file-downloads.hash.outbound-links.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script><meta property="og:title" content="CPU and Memory Management on Kubernetes with Cgroupsv2"><meta property="og:description" content="CPU and Memory Management on Kubernetes with Cgroupsv2 In this post I&rsquo;ll try to explain how CPU and Memory management works under the hood on Kubernetes. If you ever wondered what happens when you set requests and limits for your pods, keep reading!
Attention
This is the result of my exploratory work around cgroupsv2 and their application to Kubernetes. Even though I tried really hard to make sure the information in this post is accurate, I&rsquo;m far from being an expert on the topic and some information may not be 100% accurate."><meta property="og:type" content="article"><meta property="og:url" content="https://linuxera.org/cpu-memory-management-kubernetes-cgroupsv2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-04-06T00:00:00+00:00"><meta property="article:modified_time" content="2023-11-16T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="CPU and Memory Management on Kubernetes with Cgroupsv2"><meta name=twitter:description content="CPU and Memory Management on Kubernetes with Cgroupsv2 In this post I&rsquo;ll try to explain how CPU and Memory management works under the hood on Kubernetes. If you ever wondered what happens when you set requests and limits for your pods, keep reading!
Attention
This is the result of my exploratory work around cgroupsv2 and their application to Kubernetes. Even though I tried really hard to make sure the information in this post is accurate, I&rsquo;m far from being an expert on the topic and some information may not be 100% accurate."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://linuxera.org/posts/"},{"@type":"ListItem","position":2,"name":"CPU and Memory Management on Kubernetes with Cgroupsv2","item":"https://linuxera.org/cpu-memory-management-kubernetes-cgroupsv2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CPU and Memory Management on Kubernetes with Cgroupsv2","name":"CPU and Memory Management on Kubernetes with Cgroupsv2","description":"CPU and Memory Management on Kubernetes with Cgroupsv2 In this post I\u0026rsquo;ll try to explain how CPU and Memory management works under the hood on Kubernetes. If you ever wondered what happens when you set requests and limits for your pods, keep reading!\nAttention\nThis is the result of my exploratory work around cgroupsv2 and their application to Kubernetes. Even though I tried really hard to make sure the information in this post is accurate, I\u0026rsquo;m far from being an expert on the topic and some information may not be 100% accurate.","keywords":["kubernetes","openshift","cgroups","cgroupsv2"],"articleBody":"CPU and Memory Management on Kubernetes with Cgroupsv2 In this post I’ll try to explain how CPU and Memory management works under the hood on Kubernetes. If you ever wondered what happens when you set requests and limits for your pods, keep reading!\nAttention\nThis is the result of my exploratory work around cgroupsv2 and their application to Kubernetes. Even though I tried really hard to make sure the information in this post is accurate, I’m far from being an expert on the topic and some information may not be 100% accurate. If you detect something that is missing / wrong, please comment on the post!\nI’ll be using a Kubernetes v1.26 (latest at the time of this writing) with an operating system with support for cgroupsv2 like Fedora 37. The tool used to create the cluster is kcli and the command used was:\nkcli create kube generic -P ctlplanes=1 -P workers=1 -P ctlplane_memory=4096 -P numcpus=8 -P worker_memory=8192 -P image=fedora37 -P sdn=calico -P version=1.26 -P ingress=true -P ingress_method=nginx -P metallb=true -P domain=linuxera.org resource-mgmt-cluster Introduction to Cgroups As we explained in a previous post, Cgroups can be used to limit what resources are available to processes on the system, since containers are processes this applies to them as well. In Kubernetes it’s not different.\nCgroups version 2 introduces improvements and new features on top of Cgroups version 1, you can read more about what changed in this link.\nIn the next sections we will see how we can limit memory and cpu for processes.\nLimiting memory using Cgroupsv2 An evolved memory controller is available in Cgroupsv2, it allows for better management of memory resources for the processes inside the cgroup. In this section we will cover how to hard limit a process to a given amount of memory, and how to use new controls to make our programs work on memory-restricted environments.\nHard limiting memory Hard limiting memory is pretty straightforward, we just set a memory.max and since the memory is a resource that cannot be compressed, once the process reaches the limit it will be killed.\nWe will be using this python script:\ncat \u003c /opt/dumb.py f = open(\"/dev/urandom\", \"r\", encoding = \"ISO-8859-1\") data = \"\" i=0 while i \u003c 20: data += f.read(10485760) # 10MiB i += 1 print(\"Used %d MiB\" % (i * 10)) EOF Let’s create a new cgroup under the system.slice:\nsudo mkdir -p /sys/fs/cgroup/system.slice/memorytest Set a limit of 200MiB of RAM for this cgroup and disable swap:\necho \"200M\" \u003e /sys/fs/cgroup/system.slice/memorytest/memory.max echo \"0\" \u003e /sys/fs/cgroup/system.slice/memorytest/memory.swap.max Add the current shell process to the cgroup:\necho $$ \u003e /sys/fs/cgroup/system.slice/memorytest/cgroup.procs Run the python script:\npython3 /opt/dumb.py Attention\nEven if the script stopped at 80MB that’s caused because the python interpreter + shared libraries consume also memory. We can check the current memory usage in the cgroup by using the systemd-cgtop system.slice/memorytest command or with something like this MEMORY=$(cat /sys/fs/cgroup/system.slice/memorytest/memory.current);echo $(( $MEMORY / 1024 / 1024 ))MiB\nUsed 10 MiB Used 20 MiB Used 30 MiB Used 40 MiB Used 50 MiB Used 60 MiB Used 70 MiB Used 80 MiB Killed Remove the cgroup:\nWarning\nMake sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.\nsudo rmdir /sys/fs/cgroup/system.slice/memorytest/ Better memory management In the previous section we have seen how to hard-limit our processes to a given amount of memory, in this section we will be making use of new configurations to better allow our program to run under memory-restricted scenarios.\nAs we said, memory cannot be compressed, and as such, when a process reaches the limit set it will be OOMKilled. While this remains true, some memory in use by our program can be reclaimed by the kernel. This will free some memory that is no longer in use.\nIn Cgroupsv2 we can work with the following memory configurations:\nmemory.high: Memory usage throttle limit. If the cgroup goes over this limit, the cgroup processes will be throttled and put under heavy reclaim pressure. memory.max: As we saw earlier, this is the memory usage hard limit. Anything going beyond this number gets OOMKilled. memory.low: Best-effort memory protection. While processes in this cgroup or child cgroups are below this threshold, the cgroup memory won’t be reclaimed unless it cannot be reclaimed from other unprotected cgroups. memory.min: Specifies a minimum amount of memory that the cgroup must always retain and that won’t be reclaimed by the system under any conditions as long as the memory usage is below the threshold defined. memory.swap.high: Same as memory.high but for swap. memory.swap.max: Same as memory.max but for swap. Note\nMemory throttling is a resource control mechanism that limits the amount of memory a process can use, when throttled the kernel will try to reclaim memory. Keep in mind that memory reclaiming is an I/O expensive process.\nIn order to demonstrate how this works, we will be using the same python script we used previously.\nLet’s create a new cgroup under the system.slice:\nsudo mkdir -p /sys/fs/cgroup/system.slice/memorytest2 Set a limit of 200MiB of RAM for this cgroup and disable swap:\necho \"200M\" \u003e /sys/fs/cgroup/system.slice/memorytest2/memory.max echo \"0\" \u003e /sys/fs/cgroup/system.slice/memorytest2/memory.swap.max Set a throttle limit of 150MiB:\necho \"150M\" \u003e /sys/fs/cgroup/system.slice/memorytest2/memory.high Add the current shell process to the cgroup:\necho $$ \u003e /sys/fs/cgroup/system.slice/memorytest2/cgroup.procs Run the python script:\npython3 /opt/dumb.py Used 10 MiB Used 20 MiB Used 30 MiB Used 40 MiB Used 50 MiB Used 60 MiB Used 70 MiB Delete the cgroup:\nWarning\nMake sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.\nsudo rmdir /sys/fs/cgroup/system.slice/memorytest2/ We tried to limit the memory consumption for our process, and we failed. Determining the exact amount of memory required by an application is a difficult and error-prone task. Luckily for us, Facebook folks created senpain. Let’s see how we can use it to better determine the configuration for our process.\nDownload senpai:\ncurl -L https://raw.githubusercontent.com/facebookincubator/senpai/main/senpai.py -o /tmp/senpai.py Create a new cgroup under the system.slice:\nsudo mkdir -p /sys/fs/cgroup/system.slice/memorytest3 Add the current shell process to the cgroup:\necho $$ \u003e /sys/fs/cgroup/system.slice/memorytest3/cgroup.procs Run senpai in a different shell with the following command:\npython3 /tmp/senpay.py Run the python script:\npython3 /opt/dumb.py At this point senpai should’ve set the memory.high restrictions for our cgroups based on the usage of our python script:\ncat /sys/fs/cgroup/system.slice/memorytest3/memory.high 437448704 We can stop senpai. We need around 420MiB memory to run our python script, so a better configuration for it would be:\nAttention\nWe are adding a max swap usage of 50M to ease memory reclaim.\necho \"450M\" \u003e /sys/fs/cgroup/system.slice/memorytest3/memory.max echo \"50M\" \u003e /sys/fs/cgroup/system.slice/memorytest3/memory.swap.max At this point we should be able to run the program with no issues:\npython3 /opt/dumb.py Used 10 MiB Used 20 MiB ... Used 190 MiB Used 200 MiB Delete the cgroup:\nWarning\nMake sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.\nsudo rmdir /sys/fs/cgroup/system.slice/memorytest3/ Now that we have seen how to limit memory, let’s see how to limit CPU.\nLimiting CPU using Cgroupsv2 Limiting CPU is not as straightforward as limiting memory, since CPU can be compressed we can make sure that a process doesn’t use more CPU than allowed without having to kill it.\nWe need to configure the parent cgroup so it has the cpu and cpuset controllers enabled for its children groups. Below example configures the controllers for the system.slice cgroup which is the parent group we will be using. By default, only memory and pids controllers are enabled.\nEnable cpu and cpuset controllers for the /sys/fs/cgroup/ and /sys/fs/cgroup/system.slice children groups:\necho \"+cpu\" \u003e\u003e /sys/fs/cgroup/cgroup.subtree_control echo \"+cpuset\" \u003e\u003e /sys/fs/cgroup/cgroup.subtree_control echo \"+cpu\" \u003e\u003e /sys/fs/cgroup/system.slice/cgroup.subtree_control echo \"+cpuset\" \u003e\u003e /sys/fs/cgroup/system.slice/cgroup.subtree_control Limiting CPU — Pin process to CPU and limit CPU bandwidth Let’s create a new cgroup under the system.slice:\nsudo mkdir -p /sys/fs/cgroup/system.slice/cputest Assign only 1 core to this cgroup\nAttention\nBelow command assigns core 0 to our cgroup.\necho \"0\" \u003e /sys/fs/cgroup/system.slice/cputest/cpuset.cpus Set a limit of half-cpu for this cgroup:\nAttention\nThe value for cpu.max is in units of 1/1000ths of a CPU core, so 50000 represents 50% of a single core.\necho 50000 \u003e /sys/fs/cgroup/system.slice/cputest/cpu.max Add the current shell process to the cgroup:\necho $$ \u003e /sys/fs/cgroup/system.slice/cputest/cgroup.procs Download the cpuload utility:\ncurl -L https://github.com/vikyd/go-cpu-load/releases/download/0.0.1/go-cpu-load-linux-amd64 -o /tmp/cpuload \u0026\u0026 chmod +x /tmp/cpuload Run the cpu load:\nAttention\nWe’re requesting 1 core and 50% of the CPU, this should fit within the cpu.max setting.\n/tmp/cpuload -p 50 -c 1 If we check with systemd-cgtop system.slice/cputest the usage we will see something like this:\nControl Group Tasks %CPU Memory Input/s Output/s system.slice/cputest 6 47.7 856.0K - - Since we’re within the budget, we shouldn’t see any throttling happening:\nNote\nCPU throttling is a resource control mechanism that limits the amount of CPU time a process can use, preventing it from consuming excessive CPU resources and affecting the performance of other processes.\ngrep throttled /sys/fs/cgroup/system.slice/cputest/cpu.stat nr_throttled 0 throttled_usec 0 If we stop the cpuload command and request 100% of 1 core we will see throttling:\n/tmp/cpuload -p 100 -c 1 Control Group Tasks %CPU Memory Input/s Output/s system.slice/cputest 6 50.0 720.0K - - grep throttled /sys/fs/cgroup/system.slice/cputest/cpu.stat nr_throttled 336 throttled_usec 16640745 Remove the cgroup:\nWarning\nMake sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.\nsudo rmdir /sys/fs/cgroup/system.slice/cputest/ This use case is very simple, we pinned our process to 1 core and limited the CPU to half a core. Let’s see what happens when multiple processes compete for the CPU.\nLimiting CPU — Pin processes to CPU and limit CPU bandwidth Let’s create a new cgroup under the system.slice:\nsudo mkdir -p /sys/fs/cgroup/system.slice/compitingcputest Assign only 1 core to this cgroup\nAttention\nBelow command assigns core 0 to our cgroup.\necho \"0\" \u003e /sys/fs/cgroup/system.slice/compitingcputest/cpuset.cpus Set a limit of one cpu for this cgroup:\nAttention\nThe value for cpu.max is in units of 1/1000ths of a CPU core, so 100000 represents 100% of a single core.\necho 100000 \u003e /sys/fs/cgroup/system.slice/compitingcputest/cpu.max Open two shells and attach their process to the cgroup:\necho $$ \u003e /sys/fs/cgroup/system.slice/compitingcputest/cgroup.procs Run the cpu load in one of the shells:\nAttention\nWe’re requesting 1 core and 100% of the CPU, this should fit within the cpu.max setting.\n/tmp/cpuload -p 100 -c 1 If we check for throttling we will see that no throttling is happening.\ngrep throttled /sys/fs/cgroup/system.slice/compitingcputest/cpu.stat nr_throttled 0 throttled_usec 0 Run another instance of cpuload on the other shell:\n/tmp/cpuload -p 100 -c 1 At this point, we shouldn’t see throttling, but the CPU time would be shared by the two processes, in the top output below we can see that each process is consuming half cpu.\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 822742 root 20 0 4104 2004 1680 S 49.8 0.0 0:24.30 cpuload 822717 root 20 0 4104 2008 1680 S 49.5 0.1 6:28.51 cpuload Close the shells and remove the cgroup:\nWarning\nMake sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.\nsudo rmdir /sys/fs/cgroup/system.slice/compitingcputest/ In this use case, we pinned our process to 1 core and limited the CPU to one core. On top of that, we spawned two processes that competed for CPU. Since CPU bandwidth distribution was not set, each process got half cpu. In the next section we will see how to distribute CPU across processes using weights.\nLimiting CPU — Pin processes to CPU, limit and distribute CPU bandwidth Let’s create a new cgroup under the system.slice with two sub-groups (appA and appB):\nsudo mkdir -p /sys/fs/cgroup/system.slice/distributedbandwidthtest/{appA,appB} Enable cpu and cpuset controllers for the /sys/fs/cgroup/system.slice/distributedbandwidthtest children groups:\necho \"+cpu\" \u003e\u003e /sys/fs/cgroup/system.slice/distributedbandwidthtest/cgroup.subtree_control echo \"+cpuset\" \u003e\u003e /sys/fs/cgroup/system.slice/distributedbandwidthtest/cgroup.subtree_control Assign only 1 core to the parent cgroup\nAttention\nBelow command assigns core 0 to our cgroup.\necho \"0\" \u003e /sys/fs/cgroup/system.slice/distributedbandwidthtest/cpuset.cpus Set a limit of one cpu for this cgroup:\nAttention\nThe value for cpu.max is in units of 1/1000ths of a CPU core, so 100000 represents 100% of a single core.\necho 100000 \u003e /sys/fs/cgroup/system.slice/distributedbandwidthtest/cpu.max Open two shells and attach their process to the different child cgroups, then run cpuload:\nShell 1\necho $$ \u003e /sys/fs/cgroup/system.slice/distributedbandwidthtest/appA/cgroup.procs /tmp/cpuload -p 100 -c 1 Shell 2\necho $$ \u003e /sys/fs/cgroup/system.slice/distributedbandwidthtest/appB/cgroup.procs /tmp/cpuload -p 100 -c 1 If you check the top output, you will see that CPU is evenly distributed across both processes, let’s modify weights to give more CPU to appB cgroup.\nIn cgroupvs1 there was cpu shares concept, in cgroupsv2 this changed and now we use cpu weights. All weights are in the range [1, 10000] with the default at 100. This allows symmetric multiplicative biases in both directions at fine enough granularity while staying in the intuitive range. If we wanted to give appA a 30% of the CPU and appB the other 70%, providing that the parent cgroup CPU weight is set to 100 this is the configuration we will apply:\ncat /sys/fs/cgroup/system.slice/distributedbandwidthtest/cpu.weight 100 Assign 30% of the cpu to appA\necho 30 \u003e /sys/fs/cgroup/system.slice/distributedbandwidthtest/appA/cpu.weight Assign 70% of the cpu to appB\necho 70 \u003e /sys/fs/cgroup/system.slice/distributedbandwidthtest/appB/cpu.weight If we look at the top output we will see something like this:\nAttention\nYou can see how one of the cpuload processes is getting 70% of the cpu while the other is getting the other 30%.\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1077 root 20 0 4104 2008 1680 S 70.0 0.1 12:41.27 cpuload 1071 root 20 0 4104 2008 1680 S 30.0 0.1 12:24.14 cpuload Close the shells and remove the cgroups:\nWarning\nMake sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.\nsudo rmdir /sys/fs/cgroup/system.slice/distributedbandwidthtest/appA/ sudo rmdir /sys/fs/cgroup/system.slice/distributedbandwidthtest/appB/ sudo rmdir /sys/fs/cgroup/system.slice/distributedbandwidthtest/ At this point, we should have a clear understanding on how the basics work, next section will introduce these concepts applied to Kubernetes.\nResource Management on Kubernetes We won’t be covering the basics, I recommend reading the official docs. We will be focusing on CPU/Memory requests and limits.\nCgroupsv2 configuration for a Kubernetes node Before describing the cgroupsv2 configuration, we need to understand how Kubelet configurations will impact cgroupsv2 configurations. In our test cluster, we have the following Kubelet settings in order to reserve resources for system daemons:\nsystemReserved: cpu: 500m memory: 500Mi If we describe the node this is what we will see:\noc describe node Attention\nYou can see how half cpu (500m) and 500Mi of memory have been subtracted from the allocatable capacity.\nCapacity: cpu: 4 memory: 6069552Ki Allocatable: cpu: 3500m memory: 5455152Ki Even if we remove resources from the allocatable capacity, depending on the QoS of our pods we would be able to over commit on resources, at that point, eviction may happen and cgroups will make sure that pods with more priority get the required resources they asked for.\nCgroupsv2 configuration on the node In a regular Kubernetes node we will have at least three main parent cgroups:\nkubepods.slice: Parent cgroup used by Kubernetes to place pod processes. It has two child cgroups named after pod QoS inside: kubepods-besteffort.slice and kubepods-burstable.slice. Guaranteed pods get created inside this parent cgroup. system.slice: Parent cgroup used by the O.S to place system processes. Kubelet, sshd, etc. run here. user.slice: Parent cgroup used by the O.S to place user processes. When you run a regular command, it runs here. Note\nIn Systemd a Slice is a concept for hierarchically managing resources of a group of processes. This management is done by creating a cgroup. Scopes manage a set of externally created processes, the main purpose of a scope is grouping worker processes for managing resources.\n/sys/fs/cgroup/ ├── kubepods.slice │ ├── kubepods-besteffort.slice │ │ └── kubepods-besteffort-pod7589d90f_83af_4a05_a4ee_8bb078db72b8.slice │ │ ├── cri-containerd-2be6af51555a1d9ebb8678f3254e81b5f3547dfc230b07a2c1067f5d430b7221.scope │ │ └── cri-containerd-cbce8911226299472976f069f20afe0ba20c80037f9fd8394c0a8f8aaac60bee.scope │ ├── kubepods-burstable.slice │ │ └── kubepods-burstable-pode00fb079_24be_4039_b2cb_f68876881d70.slice │ │ ├── cri-containerd-a0c611e1b04856e9d565dfef25746d7bdcaaf12bb92fff6221aa6b89a12fbb31.scope │ │ └── cri-containerd-ea6361278865134bd9d52e718baa556e7693d766ab38d28d64941a1935fae004.scope │ └── kubepods-podbe70a1c9_81c5_4764_b28f_0965edee08d0.slice │ ├── cri-containerd-208bf4e7ddeef45a3bd3acff96ff0747b35e9204cea418082b586df6adf022ad.scope │ └── cri-containerd-71305184cec893cd21cfef2cbe699453ad89a51e4f60586670f194574f287a53.scope ├── system.slice │ ├── kubelet.service │ └── sshd.service └── user.slice └── user-1000.slice In order to get these cgroups created, Kubelet uses one of the two available drivers: systemd or cgroupsfs. Cgroupsv2 are only supported by systemd driver.\nThe root cgroup kubepods.slice and the QoS cgroups kubepods-besteffort.slice and kubepods-burstable.slice are created by Kubelet when it starts, on top of that Kubelet will create a cgroup (using the driver) as soon as a new Pod gets created. The pod will have from 1 to N containers, the cgroups for these containers will be created by the container runtime by using the driver as well.\nOn the output above you can see different cgroups for pods like kubepods-besteffort-pod7589d90f_83af_4a05_a4ee_8bb078db72b8.slice and one for a container like cri-containerd-2be6af51555a1d9ebb8678f3254e81b5f3547dfc230b07a2c1067f5d430b7221.scope.\nSo far, we have been looking at the configuration of cgroups via the filesystem. Systemd tooling can be used for that as well:\nsystemctl show --no-pager cri-containerd-2be6af51555a1d9ebb8678f3254e81b5f3547dfc230b07a2c1067f5d430b7221.scope CPUWeight=1 MemoryMax=infinity CPU Bandwidth configuration on the node In the previous sections we have talked about how cpu.weight works for distributing CPU bandwidth to processes. The parent cgroups in a Kubernetes node will be configured as follows:\nsystem.slice: A cpu.weight of 100. user.slice: A cpu.weight of 100. In a Kubernetes node, we won’t have much/any user processes running. So at the end, the two cgroups competing for resources will be system.slice and kubepods.slice. But wait, what cpu.weight is configured for kubepods.slice?\nWhen Kubelet starts it detects the number of CPUs available on the node, on top of that it reads the systemReserved.cpu configuration. That will give you a number of milicores available for Kubernetes to use on that node.\nFor example, if I have a 4 CPU node that’s 4000 milicores, if I reserved 500m for the system resources (kubelet, sshd, etc.) that leaves Kubernetes with 3500 milicores that can be assigned to workloads.\nNow, Kubelet knows that 3500 milicores is the amount of CPU that can be assigned to workloads (and assigned means that is more or less assured in case workloads request it). The cgroups cpu.weight needs to be configured so CPU get distributed accordingly, let’s see how that’s done:\nIn the past (cgroupsv1), CPU Shares were used and every CPU was represented by 1024 Shares. Now, we need to translate from shares to weight and the community has a formula for that (more info here). In cgroupsv2 we still use Shares under the hood, but that’s only because the formula created to not having to change the specification requires them. So we have a constant that sets the Shares/CPU to 1024 and a function that translates milicores to shares. Finally, there is a function that translates CPU Shares to CPU Weight using the formula from 1. After we know the weight that needs to be applied to the kubepods.slice, the relevant code that does that is here and here.\nContinuing with the example, the cpu.weight for our 4 CPU node with 500 milicores reserved for system resources would be:\nFormula being used: (((cpuShares - 2) * 9999) / 262142) + 1\ncpuShares = 3.5 Cores * 1024 = 3584\ncpu.weight = (((3584 - 2) * 9999) / 262142) + 1 = 137,62\nIf we check our node:\ncat /sys/fs/cgroup/kubepods.slice/cpu.weight 137 At this point we know how the different cgroups get configured on the node, next let’s see what happens when kubepods.slice and system.slice compete for cpu.\nkubepods.slice and system.slice competing for CPU In the previous section we have seen how the different cgroups get configured on our 4 CPU node, in this section we will see what happens when the two slices compete for CPU.\nLet’s say that we have two processes, the sshd service and a guaranteed pod. Both processes have access to all 4 CPUs and they’re trying to use the 100% of the 4 CPUs.\nTo calculate the percentage of CPU allocated to each process, we can use the following formulas:\nPod Process: (cpu.weight of pod / total cpu.weight) * number of CPUs Ssh Process: (cpu.weight of ssh / total cpu.weight) * number of CPUs In this case, the total cpu.weight is 237 (137 from kubepods.slice + 100 from system.slice), so:\nPod Process: (137 / 237) * 4 = 2.31 CPUs or ~231% Ssh Process: (100 / 237) * 4 = 1.68 CPUs or ~168% So pod process would get around 231% of the available CPU (400% -\u003e 4 Cores x 100) and ssh process would get around 168% of the available CPU.\nDanger\nKeep in mind that these calculations are not 100% accurate, since the CFS will try to assign CPU in the fairest way possible and results may vary depending on the system load and other process running on the system.\nCgroupsv2 configuration for a Pod In the previous sections we have focused on the configuration at the node level, but let’s see what happens when we create a pod on the different QoS.\nCgroup configuration for a BestEffort Pod We will be using this pod definition:\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: cputest name: cputest-besteffort spec: containers: - image: quay.io/mavazque/trbsht:latest name: cputest resources: {} dnsPolicy: ClusterFirst restartPolicy: Always Once created, in the node where the pod gets scheduled we can find the cgroup that was created by using these commands:\nGet container id:\ncrictl ps | grep cputest-besteffort 2be6af51555a1 b67fff43d1e61 4 minutes ago Running cputest 0 cbce891122629 cputest-besteffort Get the cgroups path:\ncrictl inspect 2be6af51555a1 | jq '.info.runtimeSpec.linux.cgroupsPath' \"kubepods-besteffort-pod7589d90f_83af_4a05_a4ee_8bb078db72b8.slice:cri-containerd:2be6af51555a1d9ebb8678f3254e81b5f3547dfc230b07a2c1067f5d430b7221\" With above information, the full path will be /sys/fs/cgroup/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7589d90f_83af_4a05_a4ee_8bb078db72b8.slice\nIf we check the cpu.max, cpu.weight and memory.max configuration, this is what we see:\ncpu.max is set to max 100000. cpu.weight is set to 1. memory.max is set to max. As we can see, the pod is allowed to use as much CPU as it wants, but it has the lowest weight possible which means that it only will get CPU when other processes with higher weight yield some. You can expect a lot of throttling for these pods when the system is under load. On the memory side, it can use as much memory as it wants, but if the cluster requires evicting this pod to reclaim memory in order to schedule more priority pods the container will be OOMKilled. The max from the cpu.max config means that the processes can use all the CPU time available on the system (which varies depending on the speed of your CPU).\nCgroup configuration for a Burstable Pod We will be using this pod definition:\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: cputest name: cputest-burstable spec: containers: - image: quay.io/mavazque/trbsht:latest name: cputest resources: requests: cpu: 2 memory: 100Mi dnsPolicy: ClusterFirst restartPolicy: Always Once created, in the node where go the cgroup configuration by following the steps described previously and this is the configuration we see:\ncpu.max is set to max 100000. cpu.weight is set to 79. memory.max is set to max. The pod will be allowed to use as much CPU as it wants, and the weight has been set to it has certain priority over other processes running on the system. On the memory side it can use as much memory as it wants, but if the cluster requires evicting this pod to reclaim memory in order to schedule more priority pods the container will be OOMKilled. The cpu.weight value 79 comes from the formula we saw earlier ((((cpuShares - 2) * 9999) / 262142) + 1):\ncpuShares = 2 Cores * 1024 = 2048 cpu.weight = (((2048 - 2) * 9999) / 262142) + 1 = 79,04 Cgroup configuration for a Guaranteed Pod We will be using this pod definition:\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: cputest name: cputest-guaranteed spec: containers: - image: quay.io/mavazque/trbsht:latest name: cputest resources: requests: cpu: 2 memory: 100Mi limits: cpu: 2 memory: 100Mi dnsPolicy: ClusterFirst restartPolicy: Always Once created, in the node where go the cgroup configuration by following the steps described previously and this is the configuration we see:\ncpu.max is set to 200000 100000. cpu.weight is set to 79. memory.max is set to 104857600 (100Mi = 104857600 bytes). The cpu.max value is different to what we have seen so far, the first value 200000 is the allowed time quota in microseconds for which the process can run during one period. The second value 100000 specific the length of the period. Once the processes consume the time specified by this quota, they will be throttled for the remained of the period and won’t be allowed to run until the next period. This specific configuration allows our processes to run every 0.2 seconds of every 1 second (1/5th). On the memory side, the container can use up to 100Mi once it reaches this value if kernel will try to reclaim some memory, if it cannot be reclaimed the container will be OOMKilled.\nEven if guaranteed QoS will ensure that your application gets the CPU it wants, sometimes your application may benefit from burstable capabilities since the CPU won’t be throttled during peaks (e.g: more visits to a web server).\nHow Kubepods Cgroups compete for resources In the previous examples we have seen how the different pods get different CPU configurations. But what happens if they compete against them for resources?\nIn order for the guaranteed pods to have more priority than burstable pods, and these to have more priority than besteffort different weights get set for the three slices. In a 4 CPU node these are the settings we get:\nGuaranteed pods will run under kubepods.slice which has a cpu.weight of 137. Burstable pods will run under kubepods.slice/kubepods-burstable.slice which has a cpu.weight of 86. BestEffort pods will run under kubepods.slice/kubepods-besteffort.slice which has a cpu.weight of 1. As we can see from above configuration, the weights define the CPU priority. Keep in mind that pods running inside the same parent slice can compete for resources. In this situation, when they’re competing for resources the total cpu.weight will be the one from summing cpu weights from all cpu hungry processes inside a specific parent cgroup. For example:\nWe have two burstable pods, these are the cpu weights that will be configured (based on the formulas we have seen so far):\nbustable1 requests 2 CPUs and gets a cpu.weight of 79 burstable2 requests 1 CPU and gets a cpu.weight of 39 So this is the CPU each one will get (formula: (cpu.weight of pod / total cpu.weight) * 100 * number of CPUs):\nDanger\nKeep in mind that these calculations are not 100% accurate, since the CFS will try to assign CPU in the fairest way possible and results may vary depending on the system load and other process running on the system. These calculations assume that there are no guaranteed pods demanding CPU. 118 value comes from summing all the CPU hungry processes from the burstable cgroup (in this case only two pods, burstable1 - cpu.weight=79 and burstable2 - cpu.weight=39).\nburstable1: (79/118) * 100 * 4 = ~ 267% (or 2.67 CPU) burstable2: (39/118) * 100 * 4 = ~ 132% (or 1.32 CPU) Closing Thoughts Even if knowing the low-level details about resource management on Kubernetes may not be needed in a day-to-day basis, it’s great knowing how the different pieces are tied together. If you’re working on environments were performance and latencies are critical, like in telco environments, knowing this information can make the difference!\nOn top of that, some of the new features that cgroupsv2 enable are:\nContainer aware OOMKilled: Useful when you have sidecars, this could be used to OOMKill the sidecar container rather than your application container. Running Kubernetes System components root-less: More secure Kubernetes environments. Kubernetes Memory QoS: Better overall control of the memory used by pods. The Kubernetes Memory QoS kind of relates to this post, so I’ll be writing a new post covering that in the future.\nFinally, in the next section I’ll put interesting resources around the topic, some of them were my sources when learning all this stuff.\nUseful Resources KubeCon NA 2022 - Cgroupv2 is coming soon to a cluster near you talk. Slides and Recording. FOSDEM 2023 - 7 years of cgroup v2 talk. Slides and Recording. Lisa 2021 - 5 years of cgroup v2 talk. Slides and Recording. KubeCon EU 2020 - Kubernetes On Cgroup v2. Slides and Recording. cgroups man page and kernel docs. RHEL8 cgroupv2 docs. Martin Heinz blog on kubernetes cgroups. Kubernetes cgroups docs. Kubernetes manage resources for containers docs. Kubernetes reserve compute resources docs. Runc Systemd driver docs. Systemd scope and slice docs. ","wordCount":"4718","inLanguage":"en","datePublished":"2023-04-06T00:00:00Z","dateModified":"2023-11-16T00:00:00Z","author":{"@type":"Person","name":"Mario"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://linuxera.org/cpu-memory-management-kubernetes-cgroupsv2/"},"publisher":{"@type":"Organization","name":"Linuxera","logo":{"@type":"ImageObject","url":"https://linuxera.org/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://linuxera.org/ accesskey=h title="Linuxera (Alt + H)">Linuxera</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://linuxera.org/ title="🏠 Home"><span>🏠 Home</span></a></li><li><a href=https://linuxera.org/archives/ title="🗄️ Archive"><span>🗄️ Archive</span></a></li><li><a href=https://linuxera.org/search/ title="🔎 Search"><span>🔎 Search</span></a></li><li><a href=https://linuxera.org/tags/ title="🏷️ Tags"><span>🏷️ Tags</span></a></li><li><a href=https://linuxera.org/presentations/ title="🎴 Presentations"><span>🎴 Presentations</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://linuxera.org/>Home</a>&nbsp;»&nbsp;<a href=https://linuxera.org/posts/>Posts</a></div><h1 class=post-title>CPU and Memory Management on Kubernetes with Cgroupsv2</h1><div class=post-meta><span title='2023-04-06 00:00:00 +0000 UTC'>Published on April 6, 2023</span>&nbsp;·&nbsp;<span title='2023-11-16 00:00:00 +0000 UTC'>Last updated on November 16, 2023</span>&nbsp;·&nbsp;23 min&nbsp;·&nbsp;Mario</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#cpu-and-memory-management-on-kubernetes-with-cgroupsv2 aria-label="CPU and Memory Management on Kubernetes with Cgroupsv2">CPU and Memory Management on Kubernetes with Cgroupsv2</a><ul><li><a href=#introduction-to-cgroups aria-label="Introduction to Cgroups">Introduction to Cgroups</a><ul><li><a href=#limiting-memory-using-cgroupsv2 aria-label="Limiting memory using Cgroupsv2">Limiting memory using Cgroupsv2</a><ul><li><a href=#hard-limiting-memory aria-label="Hard limiting memory">Hard limiting memory</a></li><li><a href=#better-memory-management aria-label="Better memory management">Better memory management</a></li></ul></li><li><a href=#limiting-cpu-using-cgroupsv2 aria-label="Limiting CPU using Cgroupsv2">Limiting CPU using Cgroupsv2</a><ul><li><a href=#limiting-cpu--pin-process-to-cpu-and-limit-cpu-bandwidth aria-label="Limiting CPU — Pin process to CPU and limit CPU bandwidth">Limiting CPU — Pin process to CPU and limit CPU bandwidth</a></li><li><a href=#limiting-cpu--pin-processes-to-cpu-and-limit-cpu-bandwidth aria-label="Limiting CPU — Pin processes to CPU and limit CPU bandwidth">Limiting CPU — Pin processes to CPU and limit CPU bandwidth</a></li><li><a href=#limiting-cpu--pin-processes-to-cpu-limit-and-distribute-cpu-bandwidth aria-label="Limiting CPU — Pin processes to CPU, limit and distribute CPU bandwidth">Limiting CPU — Pin processes to CPU, limit and distribute CPU bandwidth</a></li></ul></li></ul></li><li><a href=#resource-management-on-kubernetes aria-label="Resource Management on Kubernetes">Resource Management on Kubernetes</a><ul><li><a href=#cgroupsv2-configuration-for-a-kubernetes-node aria-label="Cgroupsv2 configuration for a Kubernetes node">Cgroupsv2 configuration for a Kubernetes node</a><ul><li><a href=#cgroupsv2-configuration-on-the-node aria-label="Cgroupsv2 configuration on the node">Cgroupsv2 configuration on the node</a></li><li><a href=#cpu-bandwidth-configuration-on-the-node aria-label="CPU Bandwidth configuration on the node">CPU Bandwidth configuration on the node</a></li><li><a href=#kubepodsslice-and-systemslice-competing-for-cpu aria-label="kubepods.slice and system.slice competing for CPU"><code>kubepods.slice</code> and <code>system.slice</code> competing for CPU</a></li></ul></li><li><a href=#cgroupsv2-configuration-for-a-pod aria-label="Cgroupsv2 configuration for a Pod">Cgroupsv2 configuration for a Pod</a><ul><li><a href=#cgroup-configuration-for-a-besteffort-pod aria-label="Cgroup configuration for a BestEffort Pod">Cgroup configuration for a BestEffort Pod</a></li><li><a href=#cgroup-configuration-for-a-burstable-pod aria-label="Cgroup configuration for a Burstable Pod">Cgroup configuration for a Burstable Pod</a></li><li><a href=#cgroup-configuration-for-a-guaranteed-pod aria-label="Cgroup configuration for a Guaranteed Pod">Cgroup configuration for a Guaranteed Pod</a></li></ul></li><li><a href=#how-kubepods-cgroups-compete-for-resources aria-label="How Kubepods Cgroups compete for resources">How Kubepods Cgroups compete for resources</a></li></ul></li><li><a href=#closing-thoughts aria-label="Closing Thoughts">Closing Thoughts</a></li><li><a href=#useful-resources aria-label="Useful Resources">Useful Resources</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=cpu-and-memory-management-on-kubernetes-with-cgroupsv2>CPU and Memory Management on Kubernetes with Cgroupsv2<a hidden class=anchor aria-hidden=true href=#cpu-and-memory-management-on-kubernetes-with-cgroupsv2>#</a></h1><p>In this post I&rsquo;ll try to explain how CPU and Memory management works under the hood on Kubernetes. If you ever wondered what happens when you set <code>requests</code> and <code>limits</code> for your pods, keep reading!</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>This is the result of my exploratory work around cgroupsv2 and their application to Kubernetes. Even though I tried really hard to make sure the information in this post is accurate, I&rsquo;m far from being an expert on the topic and some information may not be 100% accurate. If you detect something that is missing / wrong, please comment on the post!</p></div><p>I&rsquo;ll be using a Kubernetes v1.26 (latest at the time of this writing) with an operating system with support for cgroupsv2 like Fedora 37. The tool used to create the cluster is <a href=https://kcli.readthedocs.io/>kcli</a> and the command used was:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>kcli create kube generic -P <span class=nv>ctlplanes</span><span class=o>=</span><span class=m>1</span> -P <span class=nv>workers</span><span class=o>=</span><span class=m>1</span> -P <span class=nv>ctlplane_memory</span><span class=o>=</span><span class=m>4096</span> -P <span class=nv>numcpus</span><span class=o>=</span><span class=m>8</span> -P <span class=nv>worker_memory</span><span class=o>=</span><span class=m>8192</span> -P <span class=nv>image</span><span class=o>=</span>fedora37 -P <span class=nv>sdn</span><span class=o>=</span>calico -P <span class=nv>version</span><span class=o>=</span>1.26 -P <span class=nv>ingress</span><span class=o>=</span><span class=nb>true</span> -P <span class=nv>ingress_method</span><span class=o>=</span>nginx -P <span class=nv>metallb</span><span class=o>=</span><span class=nb>true</span> -P <span class=nv>domain</span><span class=o>=</span>linuxera.org resource-mgmt-cluster
</span></span></code></pre></div><h2 id=introduction-to-cgroups>Introduction to Cgroups<a hidden class=anchor aria-hidden=true href=#introduction-to-cgroups>#</a></h2><p>As we explained in a <a href=https://linuxera.org/containers-under-the-hood/>previous post</a>, Cgroups can be used to limit what resources are available to processes on the system, since containers are processes this applies to them as well. In Kubernetes it&rsquo;s not different.</p><p>Cgroups version 2 introduces improvements and new features on top of Cgroups version 1, you can read more about what changed in <a href=https://man7.org/linux/man-pages/man7/cgroups.7.html#CGROUPS_VERSION_2>this link</a>.</p><p>In the next sections we will see how we can limit memory and cpu for processes.</p><h3 id=limiting-memory-using-cgroupsv2>Limiting memory using Cgroupsv2<a hidden class=anchor aria-hidden=true href=#limiting-memory-using-cgroupsv2>#</a></h3><p>An evolved memory controller is available in Cgroupsv2, it allows for better management of memory resources for the processes inside the cgroup. In this section we will cover how to hard limit a process to a given amount of memory, and how to use new controls to make our programs work on memory-restricted environments.</p><h4 id=hard-limiting-memory>Hard limiting memory<a hidden class=anchor aria-hidden=true href=#hard-limiting-memory>#</a></h4><p>Hard limiting memory is pretty straightforward, we just set a memory.max and since the memory is a resource that cannot be compressed, once the process reaches the limit it will be killed.</p><p>We will be using this python script:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; /opt/dumb.py
</span></span></span><span class=line><span class=cl><span class=s>f = open(&#34;/dev/urandom&#34;, &#34;r&#34;, encoding = &#34;ISO-8859-1&#34;)
</span></span></span><span class=line><span class=cl><span class=s>data = &#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s>i=0
</span></span></span><span class=line><span class=cl><span class=s>while i &lt; 20:
</span></span></span><span class=line><span class=cl><span class=s>    data += f.read(10485760) # 10MiB
</span></span></span><span class=line><span class=cl><span class=s>    i += 1
</span></span></span><span class=line><span class=cl><span class=s>    print(&#34;Used %d MiB&#34; % (i * 10))
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span></code></pre></div><ol><li><p>Let&rsquo;s create a new cgroup under the system.slice:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo mkdir -p /sys/fs/cgroup/system.slice/memorytest
</span></span></code></pre></div></li><li><p>Set a limit of 200MiB of RAM for this cgroup and disable swap:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;200M&#34;</span> &gt; /sys/fs/cgroup/system.slice/memorytest/memory.max
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;0&#34;</span> &gt; /sys/fs/cgroup/system.slice/memorytest/memory.swap.max
</span></span></code></pre></div></li><li><p>Add the current shell process to the cgroup:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=nv>$$</span> &gt; /sys/fs/cgroup/system.slice/memorytest/cgroup.procs
</span></span></code></pre></div></li><li><p>Run the python script:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>python3 /opt/dumb.py
</span></span></code></pre></div><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>Even if the script stopped at 80MB that&rsquo;s caused because the python interpreter + shared libraries consume also memory. We can check the current memory usage in the cgroup by using the <code>systemd-cgtop system.slice/memorytest</code> command or with something like this <code>MEMORY=$(cat /sys/fs/cgroup/system.slice/memorytest/memory.current);echo $(( $MEMORY / 1024 / 1024 ))MiB</code></p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>Used 10 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 20 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 30 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 40 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 50 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 60 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 70 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 80 MiB
</span></span></span><span class=line><span class=cl><span class=go>Killed
</span></span></span></code></pre></div></li><li><p>Remove the cgroup:</p><div class="admonition warning"><p class=admonition-title>Warning</p><p class=admonition>Make sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo rmdir /sys/fs/cgroup/system.slice/memorytest/
</span></span></code></pre></div></li></ol><h4 id=better-memory-management>Better memory management<a hidden class=anchor aria-hidden=true href=#better-memory-management>#</a></h4><p>In the previous section we have seen how to hard-limit our processes to a given amount of memory, in this section we will be making use of new configurations to better allow our program to run under memory-restricted scenarios.</p><p>As we said, memory cannot be compressed, and as such, when a process reaches the limit set it will be OOMKilled. While this remains true, some memory in use by our program can be reclaimed by the kernel. This will free some memory that is no longer in use.</p><p>In Cgroupsv2 we can work with the following memory configurations:</p><ul><li><code>memory.high</code>: Memory usage throttle limit. If the cgroup goes over this limit, the cgroup processes will be throttled and put under heavy reclaim pressure.</li><li><code>memory.max</code>: As we saw earlier, this is the memory usage hard limit. Anything going beyond this number gets OOMKilled.</li><li><code>memory.low</code>: Best-effort memory protection. While processes in this cgroup or child cgroups are below this threshold, the cgroup memory won&rsquo;t be reclaimed unless it cannot be reclaimed from other unprotected cgroups.</li><li><code>memory.min</code>: Specifies a minimum amount of memory that the cgroup must always retain and that won&rsquo;t be reclaimed by the system under any conditions as long as the memory usage is below the threshold defined.</li><li><code>memory.swap.high</code>: Same as <code>memory.high</code> but for swap.</li><li><code>memory.swap.max</code>: Same as <code>memory.max</code> but for swap.</li></ul><div class="admonition tip"><p class=admonition-title>Note</p><p class=admonition>Memory throttling is a resource control mechanism that limits the amount of memory a process can use, when throttled the kernel will try to reclaim memory. Keep in mind that memory reclaiming is an I/O expensive process.</p></div><p>In order to demonstrate how this works, we will be using the same python script we used previously.</p><ol><li><p>Let&rsquo;s create a new cgroup under the system.slice:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo mkdir -p /sys/fs/cgroup/system.slice/memorytest2
</span></span></code></pre></div></li><li><p>Set a limit of 200MiB of RAM for this cgroup and disable swap:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;200M&#34;</span> &gt; /sys/fs/cgroup/system.slice/memorytest2/memory.max
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;0&#34;</span> &gt; /sys/fs/cgroup/system.slice/memorytest2/memory.swap.max
</span></span></code></pre></div></li><li><p>Set a throttle limit of 150MiB:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;150M&#34;</span> &gt; /sys/fs/cgroup/system.slice/memorytest2/memory.high
</span></span></code></pre></div></li><li><p>Add the current shell process to the cgroup:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=nv>$$</span> &gt; /sys/fs/cgroup/system.slice/memorytest2/cgroup.procs
</span></span></code></pre></div></li><li><p>Run the python script:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>python3 /opt/dumb.py
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>Used 10 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 20 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 30 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 40 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 50 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 60 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 70 MiB
</span></span></span><span class=line><span class=cl><span class=go>&lt;Hangs here&gt;
</span></span></span></code></pre></div></li><li><p>Delete the cgroup:</p><div class="admonition warning"><p class=admonition-title>Warning</p><p class=admonition>Make sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo rmdir /sys/fs/cgroup/system.slice/memorytest2/
</span></span></code></pre></div></li></ol><p>We tried to limit the memory consumption for our process, and we failed. Determining the exact amount of memory required by an application is a difficult and error-prone task. Luckily for us, Facebook folks created <a href=https://github.com/facebookincubator/senpai>senpain</a>. Let&rsquo;s see how we can use it to better determine the configuration for our process.</p><ol><li><p>Download senpai:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>curl -L https://raw.githubusercontent.com/facebookincubator/senpai/main/senpai.py -o /tmp/senpai.py
</span></span></code></pre></div></li><li><p>Create a new cgroup under the system.slice:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo mkdir -p /sys/fs/cgroup/system.slice/memorytest3
</span></span></code></pre></div></li><li><p>Add the current shell process to the cgroup:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=nv>$$</span> &gt; /sys/fs/cgroup/system.slice/memorytest3/cgroup.procs
</span></span></code></pre></div></li><li><p>Run senpai in a different shell with the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>python3 /tmp/senpay.py 
</span></span></code></pre></div></li><li><p>Run the python script:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>python3 /opt/dumb.py
</span></span></code></pre></div></li><li><p>At this point <code>senpai</code> should&rsquo;ve set the <code>memory.high</code> restrictions for our cgroups based on the usage of our python script:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>cat /sys/fs/cgroup/system.slice/memorytest3/memory.high
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>437448704
</span></span></span></code></pre></div></li><li><p>We can stop <code>senpai</code>. We need around 420MiB memory to run our python script, so a better configuration for it would be:</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>We are adding a max swap usage of 50M to ease memory reclaim.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;450M&#34;</span> &gt; /sys/fs/cgroup/system.slice/memorytest3/memory.max
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;50M&#34;</span> &gt; /sys/fs/cgroup/system.slice/memorytest3/memory.swap.max
</span></span></code></pre></div></li><li><p>At this point we should be able to run the program with no issues:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>python3 /opt/dumb.py
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>Used 10 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 20 MiB
</span></span></span><span class=line><span class=cl><span class=go>...
</span></span></span><span class=line><span class=cl><span class=go>Used 190 MiB
</span></span></span><span class=line><span class=cl><span class=go>Used 200 MiB
</span></span></span></code></pre></div></li><li><p>Delete the cgroup:</p><div class="admonition warning"><p class=admonition-title>Warning</p><p class=admonition>Make sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo rmdir /sys/fs/cgroup/system.slice/memorytest3/
</span></span></code></pre></div></li></ol><p>Now that we have seen how to limit memory, let&rsquo;s see how to limit CPU.</p><h3 id=limiting-cpu-using-cgroupsv2>Limiting CPU using Cgroupsv2<a hidden class=anchor aria-hidden=true href=#limiting-cpu-using-cgroupsv2>#</a></h3><p>Limiting CPU is not as straightforward as limiting memory, since CPU can be compressed we can make sure that a process doesn&rsquo;t use more CPU than allowed without having to kill it.</p><p>We need to configure the parent cgroup so it has the <code>cpu</code> and <code>cpuset</code> controllers enabled for its children groups. Below example configures the controllers for the <code>system.slice</code> cgroup which is the parent group we will be using. By default, only <code>memory</code> and <code>pids</code> controllers are enabled.</p><p>Enable <code>cpu</code> and <code>cpuset</code> controllers for the <code>/sys/fs/cgroup/</code> and <code>/sys/fs/cgroup/system.slice</code> children groups:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;+cpu&#34;</span> &gt;&gt; /sys/fs/cgroup/cgroup.subtree_control
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;+cpuset&#34;</span> &gt;&gt; /sys/fs/cgroup/cgroup.subtree_control
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;+cpu&#34;</span> &gt;&gt; /sys/fs/cgroup/system.slice/cgroup.subtree_control
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;+cpuset&#34;</span> &gt;&gt; /sys/fs/cgroup/system.slice/cgroup.subtree_control
</span></span></code></pre></div><h4 id=limiting-cpu--pin-process-to-cpu-and-limit-cpu-bandwidth>Limiting CPU — Pin process to CPU and limit CPU bandwidth<a hidden class=anchor aria-hidden=true href=#limiting-cpu--pin-process-to-cpu-and-limit-cpu-bandwidth>#</a></h4><ol><li><p>Let&rsquo;s create a new cgroup under the system.slice:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo mkdir -p /sys/fs/cgroup/system.slice/cputest
</span></span></code></pre></div></li><li><p>Assign only 1 core to this cgroup</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>Below command assigns core 0 to our cgroup.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;0&#34;</span> &gt; /sys/fs/cgroup/system.slice/cputest/cpuset.cpus
</span></span></code></pre></div></li><li><p>Set a limit of half-cpu for this cgroup:</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>The value for cpu.max is in units of 1/1000ths of a CPU core, so 50000 represents 50% of a single core.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=m>50000</span> &gt; /sys/fs/cgroup/system.slice/cputest/cpu.max
</span></span></code></pre></div></li><li><p>Add the current shell process to the cgroup:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=nv>$$</span> &gt; /sys/fs/cgroup/system.slice/cputest/cgroup.procs
</span></span></code></pre></div></li><li><p>Download the cpuload utility:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>curl -L https://github.com/vikyd/go-cpu-load/releases/download/0.0.1/go-cpu-load-linux-amd64 -o /tmp/cpuload <span class=o>&amp;&amp;</span> chmod +x /tmp/cpuload
</span></span></code></pre></div></li><li><p>Run the cpu load:</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>We&rsquo;re requesting 1 core and 50% of the CPU, this should fit within the <code>cpu.max</code> setting.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>/tmp/cpuload -p <span class=m>50</span> -c <span class=m>1</span>
</span></span></code></pre></div></li><li><p>If we check with <code>systemd-cgtop system.slice/cputest</code> the usage we will see something like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>Control Group           Tasks   %CPU   Memory  Input/s Output/s
</span></span></span><span class=line><span class=cl><span class=go>system.slice/cputest        6   47.7   856.0K        -        -
</span></span></span></code></pre></div></li><li><p>Since we&rsquo;re within the budget, we shouldn&rsquo;t see any throttling happening:</p><div class="admonition tip"><p class=admonition-title>Note</p><p class=admonition>CPU throttling is a resource control mechanism that limits the amount of CPU time a process can use, preventing it from consuming excessive CPU resources and affecting the performance of other processes.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>grep throttled /sys/fs/cgroup/system.slice/cputest/cpu.stat
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>nr_throttled 0
</span></span></span><span class=line><span class=cl><span class=go>throttled_usec 0
</span></span></span></code></pre></div></li><li><p>If we stop the cpuload command and request 100% of 1 core we will see throttling:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>/tmp/cpuload -p <span class=m>100</span> -c <span class=m>1</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>Control Group           Tasks   %CPU   Memory  Input/s Output/s
</span></span></span><span class=line><span class=cl><span class=go>system.slice/cputest        6   50.0   720.0K        -        -
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>grep throttled /sys/fs/cgroup/system.slice/cputest/cpu.stat
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>nr_throttled 336
</span></span></span><span class=line><span class=cl><span class=go>throttled_usec 16640745
</span></span></span></code></pre></div></li><li><p>Remove the cgroup:</p><div class="admonition warning"><p class=admonition-title>Warning</p><p class=admonition>Make sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo rmdir /sys/fs/cgroup/system.slice/cputest/
</span></span></code></pre></div></li></ol><p>This use case is very simple, we pinned our process to 1 core and limited the CPU to half a core. Let&rsquo;s see what happens when multiple processes compete for the CPU.</p><h4 id=limiting-cpu--pin-processes-to-cpu-and-limit-cpu-bandwidth>Limiting CPU — Pin processes to CPU and limit CPU bandwidth<a hidden class=anchor aria-hidden=true href=#limiting-cpu--pin-processes-to-cpu-and-limit-cpu-bandwidth>#</a></h4><ol><li><p>Let&rsquo;s create a new cgroup under the system.slice:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo mkdir -p /sys/fs/cgroup/system.slice/compitingcputest
</span></span></code></pre></div></li><li><p>Assign only 1 core to this cgroup</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>Below command assigns core 0 to our cgroup.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;0&#34;</span> &gt; /sys/fs/cgroup/system.slice/compitingcputest/cpuset.cpus
</span></span></code></pre></div></li><li><p>Set a limit of one cpu for this cgroup:</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>The value for cpu.max is in units of 1/1000ths of a CPU core, so 100000 represents 100% of a single core.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=m>100000</span> &gt; /sys/fs/cgroup/system.slice/compitingcputest/cpu.max
</span></span></code></pre></div></li><li><p>Open two shells and attach their process to the cgroup:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=nv>$$</span> &gt; /sys/fs/cgroup/system.slice/compitingcputest/cgroup.procs
</span></span></code></pre></div></li><li><p>Run the cpu load in one of the shells:</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>We&rsquo;re requesting 1 core and 100% of the CPU, this should fit within the <code>cpu.max</code> setting.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>/tmp/cpuload -p <span class=m>100</span> -c <span class=m>1</span>
</span></span></code></pre></div></li><li><p>If we check for throttling we will see that no throttling is happening.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>grep throttled /sys/fs/cgroup/system.slice/compitingcputest/cpu.stat
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>nr_throttled 0
</span></span></span><span class=line><span class=cl><span class=go>throttled_usec 0
</span></span></span></code></pre></div></li><li><p>Run another instance of cpuload on the other shell:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>/tmp/cpuload -p <span class=m>100</span> -c <span class=m>1</span>
</span></span></code></pre></div></li><li><p>At this point, we shouldn&rsquo;t see throttling, but the CPU time would be shared by the two processes, in the <code>top</code> output below we can see that each process is consuming half cpu.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>PID    USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                         
</span></span></span><span class=line><span class=cl><span class=go>822742 root      20   0    4104   2004   1680 S  49.8   0.0   0:24.30 cpuload                                         
</span></span></span><span class=line><span class=cl><span class=go>822717 root      20   0    4104   2008   1680 S  49.5   0.1   6:28.51 cpuload             
</span></span></span></code></pre></div></li><li><p>Close the shells and remove the cgroup:</p><div class="admonition warning"><p class=admonition-title>Warning</p><p class=admonition>Make sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo rmdir /sys/fs/cgroup/system.slice/compitingcputest/
</span></span></code></pre></div></li></ol><p>In this use case, we pinned our process to 1 core and limited the CPU to one core. On top of that, we spawned two processes that competed for CPU. Since CPU bandwidth distribution was not set, each process got half cpu. In the next section we will see how to distribute CPU across processes using weights.</p><h4 id=limiting-cpu--pin-processes-to-cpu-limit-and-distribute-cpu-bandwidth>Limiting CPU — Pin processes to CPU, limit and distribute CPU bandwidth<a hidden class=anchor aria-hidden=true href=#limiting-cpu--pin-processes-to-cpu-limit-and-distribute-cpu-bandwidth>#</a></h4><ol><li><p>Let&rsquo;s create a new cgroup under the system.slice with two sub-groups (appA and appB):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo mkdir -p /sys/fs/cgroup/system.slice/distributedbandwidthtest/<span class=o>{</span>appA,appB<span class=o>}</span>
</span></span></code></pre></div></li><li><p>Enable <code>cpu</code> and <code>cpuset</code> controllers for the <code>/sys/fs/cgroup/system.slice/distributedbandwidthtest</code> children groups:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;+cpu&#34;</span> &gt;&gt; /sys/fs/cgroup/system.slice/distributedbandwidthtest/cgroup.subtree_control
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;+cpuset&#34;</span> &gt;&gt; /sys/fs/cgroup/system.slice/distributedbandwidthtest/cgroup.subtree_control
</span></span></code></pre></div></li><li><p>Assign only 1 core to the parent cgroup</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>Below command assigns core 0 to our cgroup.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;0&#34;</span> &gt; /sys/fs/cgroup/system.slice/distributedbandwidthtest/cpuset.cpus
</span></span></code></pre></div></li><li><p>Set a limit of one cpu for this cgroup:</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>The value for cpu.max is in units of 1/1000ths of a CPU core, so 100000 represents 100% of a single core.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=m>100000</span> &gt; /sys/fs/cgroup/system.slice/distributedbandwidthtest/cpu.max
</span></span></code></pre></div></li><li><p>Open two shells and attach their process to the different child cgroups, then run cpuload:</p><ol><li><p>Shell 1</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=nv>$$</span> &gt; /sys/fs/cgroup/system.slice/distributedbandwidthtest/appA/cgroup.procs
</span></span><span class=line><span class=cl>/tmp/cpuload -p <span class=m>100</span> -c <span class=m>1</span>
</span></span></code></pre></div></li><li><p>Shell 2</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=nv>$$</span> &gt; /sys/fs/cgroup/system.slice/distributedbandwidthtest/appB/cgroup.procs
</span></span><span class=line><span class=cl>/tmp/cpuload -p <span class=m>100</span> -c <span class=m>1</span>
</span></span></code></pre></div></li></ol></li><li><p>If you check the top output, you will see that CPU is evenly distributed across both processes, let&rsquo;s modify weights to give more CPU to appB cgroup.</p></li><li><p>In cgroupvs1 there was <code>cpu shares</code> concept, in cgroupsv2 this changed and now we use <code>cpu weights</code>. All weights are in the range [1, 10000] with the default at 100. This allows symmetric multiplicative biases in both directions at fine enough granularity while staying in the intuitive range. If we wanted to give <code>appA</code> a <code>30%</code> of the CPU and <code>appB</code> the other <code>70%</code>, providing that the parent cgroup CPU weight is set to 100 this is the configuration we will apply:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>cat /sys/fs/cgroup/system.slice/distributedbandwidthtest/cpu.weight
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>100
</span></span></span></code></pre></div><ol><li><p>Assign 30% of the cpu to appA</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=m>30</span> &gt; /sys/fs/cgroup/system.slice/distributedbandwidthtest/appA/cpu.weight
</span></span></code></pre></div></li><li><p>Assign 70% of the cpu to appB</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>echo</span> <span class=m>70</span> &gt; /sys/fs/cgroup/system.slice/distributedbandwidthtest/appB/cpu.weight
</span></span></code></pre></div></li></ol></li><li><p>If we look at the top output we will see something like this:</p><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>You can see how one of the cpuload processes is getting 70% of the cpu while the other is getting the other 30%.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>PID    USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                                                                 
</span></span></span><span class=line><span class=cl><span class=go>1077   root      20   0    4104   2008   1680 S  70.0   0.1  12:41.27 cpuload                                                                                 
</span></span></span><span class=line><span class=cl><span class=go>1071   root      20   0    4104   2008   1680 S  30.0   0.1  12:24.14 cpuload 
</span></span></span></code></pre></div></li><li><p>Close the shells and remove the cgroups:</p><div class="admonition warning"><p class=admonition-title>Warning</p><p class=admonition>Make sure you closed the shell attached to the cgroup before running the command below, otherwise it will fail.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>sudo rmdir /sys/fs/cgroup/system.slice/distributedbandwidthtest/appA/
</span></span><span class=line><span class=cl>sudo rmdir /sys/fs/cgroup/system.slice/distributedbandwidthtest/appB/
</span></span><span class=line><span class=cl>sudo rmdir /sys/fs/cgroup/system.slice/distributedbandwidthtest/
</span></span></code></pre></div></li></ol><p>At this point, we should have a clear understanding on how the basics work, next section will introduce these concepts applied to Kubernetes.</p><h2 id=resource-management-on-kubernetes>Resource Management on Kubernetes<a hidden class=anchor aria-hidden=true href=#resource-management-on-kubernetes>#</a></h2><p>We won&rsquo;t be covering the basics, I recommend reading the <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/>official docs</a>. We will be focusing on CPU/Memory requests and limits.</p><h3 id=cgroupsv2-configuration-for-a-kubernetes-node>Cgroupsv2 configuration for a Kubernetes node<a hidden class=anchor aria-hidden=true href=#cgroupsv2-configuration-for-a-kubernetes-node>#</a></h3><p>Before describing the cgroupsv2 configuration, we need to understand how Kubelet configurations will impact cgroupsv2 configurations. In our test cluster, we have the following Kubelet settings in order to reserve resources for system daemons:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>systemReserved</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=l>500m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class=l>500Mi</span><span class=w>
</span></span></span></code></pre></div><p>If we describe the node this is what we will see:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>oc describe node &lt;compute-node&gt;
</span></span></code></pre></div><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>You can see how half cpu (500m) and 500Mi of memory have been subtracted from the allocatable capacity.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>Capacity:
</span></span></span><span class=line><span class=cl><span class=go>  cpu:                4
</span></span></span><span class=line><span class=cl><span class=go>  &lt;omitted&gt;
</span></span></span><span class=line><span class=cl><span class=go>  memory:             6069552Ki
</span></span></span><span class=line><span class=cl><span class=go>Allocatable:
</span></span></span><span class=line><span class=cl><span class=go>  cpu:                3500m
</span></span></span><span class=line><span class=cl><span class=go>  &lt;omitted&gt;
</span></span></span><span class=line><span class=cl><span class=go>  memory:             5455152Ki
</span></span></span></code></pre></div><p>Even if we remove resources from the allocatable capacity, depending on the QoS of our pods we would be able to over commit on resources, at that point, eviction may happen and cgroups will make sure that pods with more priority get the required resources they asked for.</p><h4 id=cgroupsv2-configuration-on-the-node>Cgroupsv2 configuration on the node<a hidden class=anchor aria-hidden=true href=#cgroupsv2-configuration-on-the-node>#</a></h4><p>In a regular Kubernetes node we will have at least three main parent cgroups:</p><ul><li><code>kubepods.slice</code>: Parent cgroup used by Kubernetes to place pod processes. It has two child cgroups named after pod QoS inside: <code>kubepods-besteffort.slice</code> and <code>kubepods-burstable.slice</code>. Guaranteed pods get created inside this parent cgroup.</li><li><code>system.slice</code>: Parent cgroup used by the O.S to place system processes. Kubelet, sshd, etc. run here.</li><li><code>user.slice</code>: Parent cgroup used by the O.S to place user processes. When you run a regular command, it runs here.</li></ul><div class="admonition tip"><p class=admonition-title>Note</p><p class=admonition>In Systemd a <code>Slice</code> is a concept for hierarchically managing resources of a group of processes. This management is done by creating a cgroup. <code>Scopes</code> manage a set of externally created processes, the main purpose of a <code>scope</code> is grouping worker processes for managing resources.</p></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>/sys/fs/cgroup/
</span></span></span><span class=line><span class=cl><span class=go>├── kubepods.slice
</span></span></span><span class=line><span class=cl><span class=go>│   ├── kubepods-besteffort.slice
</span></span></span><span class=line><span class=cl><span class=go>│   │   └── kubepods-besteffort-pod7589d90f_83af_4a05_a4ee_8bb078db72b8.slice
</span></span></span><span class=line><span class=cl><span class=go>│   │       ├── cri-containerd-2be6af51555a1d9ebb8678f3254e81b5f3547dfc230b07a2c1067f5d430b7221.scope
</span></span></span><span class=line><span class=cl><span class=go>│   │       └── cri-containerd-cbce8911226299472976f069f20afe0ba20c80037f9fd8394c0a8f8aaac60bee.scope
</span></span></span><span class=line><span class=cl><span class=go>│   ├── kubepods-burstable.slice
</span></span></span><span class=line><span class=cl><span class=go>│   │   └── kubepods-burstable-pode00fb079_24be_4039_b2cb_f68876881d70.slice
</span></span></span><span class=line><span class=cl><span class=go>│   │       ├── cri-containerd-a0c611e1b04856e9d565dfef25746d7bdcaaf12bb92fff6221aa6b89a12fbb31.scope
</span></span></span><span class=line><span class=cl><span class=go>│   │       └── cri-containerd-ea6361278865134bd9d52e718baa556e7693d766ab38d28d64941a1935fae004.scope
</span></span></span><span class=line><span class=cl><span class=go>│   └── kubepods-podbe70a1c9_81c5_4764_b28f_0965edee08d0.slice
</span></span></span><span class=line><span class=cl><span class=go>│       ├── cri-containerd-208bf4e7ddeef45a3bd3acff96ff0747b35e9204cea418082b586df6adf022ad.scope
</span></span></span><span class=line><span class=cl><span class=go>│       └── cri-containerd-71305184cec893cd21cfef2cbe699453ad89a51e4f60586670f194574f287a53.scope
</span></span></span><span class=line><span class=cl><span class=go>├── system.slice
</span></span></span><span class=line><span class=cl><span class=go>│   ├── kubelet.service
</span></span></span><span class=line><span class=cl><span class=go>│   └── sshd.service
</span></span></span><span class=line><span class=cl><span class=go>└── user.slice
</span></span></span><span class=line><span class=cl><span class=go>    └── user-1000.slice
</span></span></span></code></pre></div><p>In order to get these cgroups created, Kubelet uses one of the two <a href=https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers>available drivers</a>: <code>systemd</code> or <code>cgroupsfs</code>. Cgroupsv2 are only supported by <code>systemd</code> driver.</p><p>The root cgroup <code>kubepods.slice</code> and the QoS cgroups <code>kubepods-besteffort.slice</code> and <code>kubepods-burstable.slice</code> are created by Kubelet when it starts, on top of that Kubelet will create a cgroup (using the driver) as soon as a new Pod gets created. The pod will have from 1 to N containers, the cgroups for these containers will be created by the container runtime by using the driver as well.</p><p>On the output above you can see different cgroups for pods like <code>kubepods-besteffort-pod7589d90f_83af_4a05_a4ee_8bb078db72b8.slice</code> and one for a container like <code>cri-containerd-2be6af51555a1d9ebb8678f3254e81b5f3547dfc230b07a2c1067f5d430b7221.scope</code>.</p><p>So far, we have been looking at the configuration of cgroups via the filesystem. Systemd tooling can be used for that as well:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>systemctl show --no-pager cri-containerd-2be6af51555a1d9ebb8678f3254e81b5f3547dfc230b07a2c1067f5d430b7221.scope
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>&lt;OMITTED_OUTPUT&gt;
</span></span></span><span class=line><span class=cl><span class=go>CPUWeight=1
</span></span></span><span class=line><span class=cl><span class=go>MemoryMax=infinity
</span></span></span><span class=line><span class=cl><span class=go>&lt;OMITTED_OUTPUT&gt;
</span></span></span></code></pre></div><h4 id=cpu-bandwidth-configuration-on-the-node>CPU Bandwidth configuration on the node<a hidden class=anchor aria-hidden=true href=#cpu-bandwidth-configuration-on-the-node>#</a></h4><p>In the previous sections we have talked about how <em><code>cpu.weight</code></em> works for distributing CPU bandwidth to processes. The parent cgroups in a Kubernetes node will be configured as follows:</p><ul><li><code>system.slice</code>: A <em>cpu.weight</em> of <code>100</code>.</li><li><code>user.slice</code>: A <em>cpu.weight</em> of <code>100</code>.</li></ul><p>In a Kubernetes node, we won&rsquo;t have much/any user processes running. So at the end, the two cgroups competing for resources will be <code>system.slice</code> and <code>kubepods.slice</code>. But wait, what <em>cpu.weight</em> is configured for <code>kubepods.slice</code>?</p><p>When Kubelet starts it detects the number of CPUs available on the node, on top of that it reads the <code>systemReserved.cpu</code> configuration. That will give you a number of milicores available for Kubernetes to use on that node.</p><p>For example, if I have a 4 CPU node that&rsquo;s 4000 milicores, if I reserved 500m for the system resources (kubelet, sshd, etc.) that leaves Kubernetes with 3500 milicores that can be assigned to workloads.</p><p>Now, Kubelet knows that 3500 milicores is the amount of CPU that can be <em>assigned</em> to workloads (and assigned means that is more or less assured in case workloads request it). The cgroups <em>cpu.weight</em> needs to be configured so CPU get distributed accordingly, let&rsquo;s see how that&rsquo;s done:</p><ol><li>In the past (cgroupsv1), CPU Shares were used and every CPU was represented by 1024 Shares. Now, we need to translate from shares to weight and the community has a formula for that (more info <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2254-cgroup-v2#phase-1-convert-from-cgroups-v1-settings-to-v2>here</a>).</li><li>In cgroupsv2 we still use Shares under the hood, but that&rsquo;s only because the formula created to not having to change the specification requires them. So we have a <a href=https://github.com/kubernetes/kubernetes/blob/release-1.27/pkg/kubelet/cm/helpers_linux.go#L45>constant</a> that sets the Shares/CPU to 1024 and a function that <a href=https://github.com/kubernetes/kubernetes/blob/release-1.27/pkg/kubelet/cm/helpers_linux.go#L85>translates milicores to shares</a>.</li><li>Finally, there is a function that <a href=https://github.com/kubernetes/kubernetes/blob/release-1.27/pkg/kubelet/cm/cgroup_manager_linux.go#L566>translates CPU Shares to CPU Weight</a> using the formula from 1.</li></ol><p>After we know the weight that needs to be applied to the <code>kubepods.slice</code>, the relevant code that does that is <a href=https://github.com/kubernetes/kubernetes/blob/release-1.27/pkg/kubelet/cm/node_container_manager_linux.go#L115>here</a> and <a href=https://github.com/kubernetes/kubernetes/blob/release-1.27/pkg/kubelet/cm/cgroup_manager_linux.go#L435>here</a>.</p><p>Continuing with the example, the <em>cpu.weight</em> for our 4 CPU node with 500 milicores reserved for system resources would be:</p><p>Formula being used: (((cpuShares - 2) * 9999) / 262142) + 1</p><p>cpuShares = 3.5 Cores * 1024 = 3584</p><p>cpu.weight = (((3584 - 2) * 9999) / 262142) + 1 = 137,62</p><p>If we check our node:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>cat /sys/fs/cgroup/kubepods.slice/cpu.weight
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>137
</span></span></span></code></pre></div><p>At this point we know how the different cgroups get configured on the node, next let&rsquo;s see what happens when <code>kubepods.slice</code> and <code>system.slice</code> compete for cpu.</p><h4 id=kubepodsslice-and-systemslice-competing-for-cpu><code>kubepods.slice</code> and <code>system.slice</code> competing for CPU<a hidden class=anchor aria-hidden=true href=#kubepodsslice-and-systemslice-competing-for-cpu>#</a></h4><p>In the previous section we have seen how the different cgroups get configured on our 4 CPU node, in this section we will see what happens when the two slices compete for CPU.</p><p>Let&rsquo;s say that we have two processes, the sshd service and a guaranteed pod. Both processes have access to all 4 CPUs and they&rsquo;re trying to use the 100% of the 4 CPUs.</p><p>To calculate the percentage of CPU allocated to each process, we can use the following formulas:</p><ul><li>Pod Process: (cpu.weight of pod / total cpu.weight) * number of CPUs</li><li>Ssh Process: (cpu.weight of ssh / total cpu.weight) * number of CPUs</li></ul><p>In this case, the total cpu.weight is 237 (137 from kubepods.slice + 100 from system.slice), so:</p><ul><li>Pod Process: (137 / 237) * 4 = 2.31 CPUs or ~231%</li><li>Ssh Process: (100 / 237) * 4 = 1.68 CPUs or ~168%</li></ul><p>So pod process would get around 231% of the available CPU (400% -> 4 Cores x 100) and ssh process would get around 168% of the available CPU.</p><div class="admonition danger"><p class=admonition-title>Danger</p><p class=admonition>Keep in mind that these calculations are not 100% accurate, since the CFS will try to assign CPU in the fairest way possible and results may vary depending on the system load and other process running on the system.</p></div><h3 id=cgroupsv2-configuration-for-a-pod>Cgroupsv2 configuration for a Pod<a hidden class=anchor aria-hidden=true href=#cgroupsv2-configuration-for-a-pod>#</a></h3><p>In the previous sections we have focused on the configuration at the node level, but let&rsquo;s see what happens when we create a pod on the different QoS.</p><h4 id=cgroup-configuration-for-a-besteffort-pod>Cgroup configuration for a BestEffort Pod<a hidden class=anchor aria-hidden=true href=#cgroup-configuration-for-a-besteffort-pod>#</a></h4><p>We will be using this pod definition:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>creationTimestamp</span><span class=p>:</span><span class=w> </span><span class=kc>null</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>cputest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cputest-besteffort</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>quay.io/mavazque/trbsht:latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cputest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>resources</span><span class=p>:</span><span class=w> </span>{}<span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>dnsPolicy</span><span class=p>:</span><span class=w> </span><span class=l>ClusterFirst</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>restartPolicy</span><span class=p>:</span><span class=w> </span><span class=l>Always</span><span class=w>
</span></span></span></code></pre></div><p>Once created, in the node where the pod gets scheduled we can find the cgroup that was created by using these commands:</p><ol><li><p>Get container id:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>crictl ps <span class=p>|</span> grep cputest-besteffort
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>2be6af51555a1       b67fff43d1e61       4 minutes ago       Running             cputest                     0                   cbce891122629       cputest-besteffort
</span></span></span></code></pre></div></li><li><p>Get the cgroups path:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>crictl inspect 2be6af51555a1 <span class=p>|</span> jq <span class=s1>&#39;.info.runtimeSpec.linux.cgroupsPath&#39;</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>&#34;kubepods-besteffort-pod7589d90f_83af_4a05_a4ee_8bb078db72b8.slice:cri-containerd:2be6af51555a1d9ebb8678f3254e81b5f3547dfc230b07a2c1067f5d430b7221&#34;
</span></span></span></code></pre></div></li><li><p>With above information, the full path will be <code>/sys/fs/cgroup/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7589d90f_83af_4a05_a4ee_8bb078db72b8.slice</code></p></li></ol><p>If we check the <em>cpu.max</em>, <em>cpu.weight</em> and <em>memory.max</em> configuration, this is what we see:</p><ul><li><em>cpu.max</em> is set to <code>max 100000</code>.</li><li><em>cpu.weight</em> is set to <code>1</code>.</li><li><em>memory.max</em> is set to <code>max</code>.</li></ul><p>As we can see, the pod is allowed to use as much CPU as it wants, but it has the lowest weight possible which means that it only will get CPU when other processes with higher weight yield some. You can expect a lot of throttling for these pods when the system is under load. On the memory side, it can use as much memory as it wants, but if the cluster requires evicting this pod to reclaim memory in order to schedule more priority pods the container will be OOMKilled. The <code>max</code> from the <code>cpu.max</code> config means that the processes can use all the CPU time available on the system (which varies depending on the speed of your CPU).</p><h4 id=cgroup-configuration-for-a-burstable-pod>Cgroup configuration for a Burstable Pod<a hidden class=anchor aria-hidden=true href=#cgroup-configuration-for-a-burstable-pod>#</a></h4><p>We will be using this pod definition:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>creationTimestamp</span><span class=p>:</span><span class=w> </span><span class=kc>null</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>cputest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cputest-burstable</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>quay.io/mavazque/trbsht:latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cputest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>requests</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class=l>100Mi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>dnsPolicy</span><span class=p>:</span><span class=w> </span><span class=l>ClusterFirst</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>restartPolicy</span><span class=p>:</span><span class=w> </span><span class=l>Always</span><span class=w>
</span></span></span></code></pre></div><p>Once created, in the node where go the cgroup configuration by following the steps described previously and this is the configuration we see:</p><ul><li><em>cpu.max</em> is set to <code>max 100000</code>.</li><li><em>cpu.weight</em> is set to <code>79</code>.</li><li><em>memory.max</em> is set to <code>max</code>.</li></ul><p>The pod will be allowed to use as much CPU as it wants, and the weight has been set to it has certain priority over other processes running on the system. On the memory side it can use as much memory as it wants, but if the cluster requires evicting this pod to reclaim memory in order to schedule more priority pods the container will be OOMKilled. The <em>cpu.weight</em> value <code>79</code> comes from the formula we saw earlier (<code>(((cpuShares - 2) * 9999) / 262142) + 1</code>):</p><pre tabindex=0><code class=language-math data-lang=math>cpuShares = 2 Cores * 1024 = 2048
cpu.weight = (((2048 - 2) * 9999) / 262142) + 1 = 79,04
</code></pre><h4 id=cgroup-configuration-for-a-guaranteed-pod>Cgroup configuration for a Guaranteed Pod<a hidden class=anchor aria-hidden=true href=#cgroup-configuration-for-a-guaranteed-pod>#</a></h4><p>We will be using this pod definition:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>creationTimestamp</span><span class=p>:</span><span class=w> </span><span class=kc>null</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>cputest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cputest-guaranteed</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>quay.io/mavazque/trbsht:latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cputest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>requests</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class=l>100Mi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>limits</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class=l>100Mi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>dnsPolicy</span><span class=p>:</span><span class=w> </span><span class=l>ClusterFirst</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>restartPolicy</span><span class=p>:</span><span class=w> </span><span class=l>Always</span><span class=w>
</span></span></span></code></pre></div><p>Once created, in the node where go the cgroup configuration by following the steps described previously and this is the configuration we see:</p><ul><li><em>cpu.max</em> is set to <code>200000 100000</code>.</li><li><em>cpu.weight</em> is set to <code>79</code>.</li><li><em>memory.max</em> is set to <code>104857600</code> (100Mi = 104857600 bytes).</li></ul><p>The <em>cpu.max</em> value is different to what we have seen so far, the first value <code>200000</code> is the allowed time quota in microseconds for which the process can run during one period. The second value <code>100000</code> specific the length of the period. Once the processes consume the time specified by this quota, they will be throttled for the remained of the period and won&rsquo;t be allowed to run until the next period. This specific configuration allows our processes to run every 0.2 seconds of every 1 second (1/5th). On the memory side, the container can use up to 100Mi once it reaches this value if kernel will try to reclaim some memory, if it cannot be reclaimed the container will be OOMKilled.</p><p>Even if guaranteed QoS will <em>ensure</em> that your application gets the CPU it wants, sometimes your application may benefit from burstable capabilities since the CPU won&rsquo;t be throttled during peaks (e.g: more visits to a web server).</p><h3 id=how-kubepods-cgroups-compete-for-resources>How Kubepods Cgroups compete for resources<a hidden class=anchor aria-hidden=true href=#how-kubepods-cgroups-compete-for-resources>#</a></h3><p>In the previous examples we have seen how the different pods get different CPU configurations. But what happens if they compete against them for resources?</p><p>In order for the <code>guaranteed</code> pods to have more priority than <code>burstable</code> pods, and these to have more priority than <code>besteffort</code> different weights get set for the three slices. In a 4 CPU node these are the settings we get:</p><ul><li>Guaranteed pods will run under <code>kubepods.slice</code> which has a <code>cpu.weight</code> of <code>137</code>.</li><li>Burstable pods will run under <code>kubepods.slice/kubepods-burstable.slice</code> which has a <code>cpu.weight</code> of <code>86</code>.</li><li>BestEffort pods will run under <code>kubepods.slice/kubepods-besteffort.slice</code> which has a <code>cpu.weight</code> of <code>1</code>.</li></ul><p>As we can see from above configuration, the weights define the CPU priority. Keep in mind that pods running inside the same parent slice can compete for resources. In this situation, when they&rsquo;re competing for resources the <code>total cpu.weight</code> will be the one from summing cpu weights from all cpu hungry processes inside a specific parent cgroup. For example:</p><p>We have two burstable pods, these are the cpu weights that will be configured (based on the formulas we have seen so far):</p><ul><li><code>bustable1</code> requests 2 CPUs and gets a <em>cpu.weight</em> of <code>79</code></li><li><code>burstable2</code> requests 1 CPU and gets a <em>cpu.weight</em> of <code>39</code></li></ul><p>So this is the CPU each one will get (formula: <code>(cpu.weight of pod / total cpu.weight) * 100 * number of CPUs</code>):</p><div class="admonition danger"><p class=admonition-title>Danger</p><p class=admonition>Keep in mind that these calculations are not 100% accurate, since the CFS will try to assign CPU in the fairest way possible and results may vary depending on the system load and other process running on the system. These calculations assume that there are no guaranteed pods demanding CPU. <code>118</code> value comes from summing all the CPU hungry processes from the burstable cgroup (in this case only two pods, <code>burstable1</code> - cpu.weight=79 and <code>burstable2</code> - cpu.weight=39).</p></div><ul><li><code>burstable1</code>: (79/118) * 100 * 4 = ~ 267% (or 2.67 CPU)</li><li><code>burstable2</code>: (39/118) * 100 * 4 = ~ 132% (or 1.32 CPU)</li></ul><h2 id=closing-thoughts>Closing Thoughts<a hidden class=anchor aria-hidden=true href=#closing-thoughts>#</a></h2><p>Even if knowing the low-level details about resource management on Kubernetes may not be needed in a day-to-day basis, it&rsquo;s great knowing how the different pieces are tied together. If you&rsquo;re working on environments were performance and latencies are critical, like in telco environments, knowing this information can make the difference!</p><p>On top of that, some of the new features that cgroupsv2 enable are:</p><ul><li><a href=https://www.scrivano.org/posts/2020-08-14-oom-group/>Container aware OOMKilled</a>: Useful when you have sidecars, this could be used to OOMKill the sidecar container rather than your application container.</li><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/>Running Kubernetes System components root-less</a>: More secure Kubernetes environments.</li><li><a href=https://kubernetes.io/blog/2021/11/26/qos-memory-resources/>Kubernetes Memory QoS</a>: Better overall control of the memory used by pods.</li></ul><p>The Kubernetes Memory QoS kind of relates to this post, so I&rsquo;ll be writing a new post covering that in the future.</p><p>Finally, in the next section I&rsquo;ll put interesting resources around the topic, some of them were my sources when learning all this stuff.</p><h2 id=useful-resources>Useful Resources<a hidden class=anchor aria-hidden=true href=#useful-resources>#</a></h2><ul><li>KubeCon NA 2022 - Cgroupv2 is coming soon to a cluster near you talk. <a href=https://static.sched.com/hosted_files/kccncna2022/69/cgroupv2-is-coming-soon-to-a-cluster-near-you-kubecon-na-2022.pdf>Slides</a> and <a href="https://www.youtube.com/watch?v=sgyFCp1CRhA">Recording</a>.</li><li>FOSDEM 2023 - 7 years of cgroup v2 talk. <a href=https://chrisdown.name/talks/cg2023/cg2023-fosdem.pdf>Slides</a> and <a href="https://www.youtube.com/watch?v=LX6fMlIYZcg">Recording</a>.</li><li>Lisa 2021 - 5 years of cgroup v2 talk. <a href=https://www.usenix.org/system/files/lisa21_slides_down.pdf>Slides</a> and <a href="https://www.youtube.com/watch?v=kPMZYoRxtmg">Recording</a>.</li><li>KubeCon EU 2020 - Kubernetes On Cgroup v2. <a href=https://static.sched.com/hosted_files/kccnceu20/b8/kubernetes_on_cgroup_v2.pdf>Slides</a> and <a href="https://www.youtube.com/watch?v=u8h0e84HxcE">Recording</a>.</li><li>cgroups <a href=https://man7.org/linux/man-pages/man7/cgroups.7.html>man page</a> and kernel <a href=https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html>docs</a>.</li><li><a href=https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/using-cgroups-v2-to-control-distribution-of-cpu-time-for-applications_managing-monitoring-and-updating-the-kernel>RHEL8 cgroupv2 docs</a>.</li><li>Martin Heinz <a href=https://martinheinz.dev/blog/91>blog on kubernetes cgroups</a>.</li><li>Kubernetes cgroups <a href=https://kubernetes.io/docs/concepts/architecture/cgroups/>docs</a>.</li><li>Kubernetes manage resources for containers <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/>docs</a>.</li><li>Kubernetes reserve compute resources <a href=https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/>docs</a>.</li><li><a href=https://github.com/opencontainers/runc/blob/main/docs/systemd.md>Runc Systemd driver docs</a>.</li><li>Systemd <a href=https://www.freedesktop.org/software/systemd/man/systemd.scope.html>scope</a> and <a href=https://www.freedesktop.org/software/systemd/man/systemd.slice.html>slice</a> docs.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://linuxera.org/tags/kubernetes/>kubernetes</a></li><li><a href=https://linuxera.org/tags/openshift/>openshift</a></li><li><a href=https://linuxera.org/tags/cgroups/>cgroups</a></li><li><a href=https://linuxera.org/tags/cgroupsv2/>cgroupsv2</a></li></ul><nav class=paginav><a class=prev href=https://linuxera.org/gateway-api-kubernetes/><span class=title>« Prev</span><br><span>Gateway API for Kubernetes</span></a>
<a class=next href=https://linuxera.org/exposing-multiple-kubernetes-clusters-single-lb-and-ip/><span class=title>Next »</span><br><span>Exposing multiple Kubernetes clusters with a single load balancer and a single public IP</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share CPU and Memory Management on Kubernetes with Cgroupsv2 on twitter" href="https://twitter.com/intent/tweet/?text=CPU%20and%20Memory%20Management%20on%20Kubernetes%20with%20Cgroupsv2&url=https%3a%2f%2flinuxera.org%2fcpu-memory-management-kubernetes-cgroupsv2%2f&hashtags=kubernetes%2copenshift%2ccgroups%2ccgroupsv2"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share CPU and Memory Management on Kubernetes with Cgroupsv2 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flinuxera.org%2fcpu-memory-management-kubernetes-cgroupsv2%2f&title=CPU%20and%20Memory%20Management%20on%20Kubernetes%20with%20Cgroupsv2&summary=CPU%20and%20Memory%20Management%20on%20Kubernetes%20with%20Cgroupsv2&source=https%3a%2f%2flinuxera.org%2fcpu-memory-management-kubernetes-cgroupsv2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></div></footer><div class=share-buttons><p>If this post has been helpful to you, consider <u><a target=_blank href=https://ko-fi.com/mvazce>supporting the work.</a></u></p></div><script src=https://utteranc.es/client.js repo=mvazquezc/mvazquezc.github.io issue-term=pathname label=blog-comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://linuxera.org/>Linuxera</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>